<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Regression | Mark Goldberg</title>
    <link>/tags/regression/</link>
      <atom:link href="/tags/regression/index.xml" rel="self" type="application/rss+xml" />
    <description>Regression</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019</copyright><lastBuildDate>Sun, 04 Aug 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/logo.png</url>
      <title>Regression</title>
      <link>/tags/regression/</link>
    </image>
    
    <item>
      <title>Logistic regression</title>
      <link>/post/statistics/logistic_regression/logistic_regression/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/logistic_regression/logistic_regression/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#background&#34;&gt;Background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#binomial-logistic-regression&#34;&gt;Binomial logistic regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression&#34;&gt;Logistic regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-discriminant-analysis-lda&#34;&gt;Linear Discriminant Analysis (LDA)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quadratic-discriminant-analysis-qda&#34;&gt;Quadratic Discriminant Analysis (QDA)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#roc-curve-for-lda&#34;&gt;ROC-curve for LDA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tasks&#34;&gt;Tasks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Logistic regression builds model for binary dependent variables (0/1, True/False).&lt;/p&gt;
&lt;p&gt;Logistic function: &lt;span class=&#34;math display&#34;&gt;\[Y = \frac{1}{1+e^l} = \frac{e^l}{e^l+1}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is a linear combination of all observations (log-odds): &lt;span class=&#34;math inline&#34;&gt;\(l = \beta_0 + \beta_{1}x_{1} + \beta_{2}x_{2} + ... + \beta_{p}x_{p} + \epsilon\)&lt;/span&gt;&lt;br /&gt;
See also: &lt;a href=&#34;https://en.wikipedia.org/wiki/Sigmoid_function&#34;&gt;Sigmoid functions&lt;/a&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;binomial-logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Binomial logistic regression&lt;/h2&gt;
&lt;p&gt;Probability of passing an exam versus hours of study.&lt;br /&gt;
Data from &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34;&gt;wiki&lt;/a&gt; describe if students &lt;strong&gt;pass&lt;/strong&gt; exam depending of how many &lt;strong&gt;hours&lt;/strong&gt; they studied.&lt;br /&gt;
We build &lt;strong&gt;logistic regression&lt;/strong&gt; model to predict if ‘pass’ depending on learning ‘hours’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# put data into dataframe
hours=c(0.50,.75,1,1.25,1.5,1.75,1.75,2,2.25,2.5,2.75,3,3.25,3.50,4,4.25,4.5,4.75,5,5.5)
pass=c(0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1)
df = data.frame(hours, pass)

# Logistic Regression model
model.logit &amp;lt;- glm(pass ~ hours, data = df, family = &amp;#39;binomial&amp;#39;)

summary(model.logit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = pass ~ hours, family = &amp;quot;binomial&amp;quot;, data = df)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.70557  -0.57357  -0.04654   0.45470   1.82008  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)  
## (Intercept)  -4.0777     1.7610  -2.316   0.0206 *
## hours         1.5046     0.6287   2.393   0.0167 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 27.726  on 19  degrees of freedom
## Residual deviance: 16.060  on 18  degrees of freedom
## AIC: 20.06
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Coefficients Intercept = -4.0777 and Hours = 1.5046 are entered in the &lt;strong&gt;logistic regression equation&lt;/strong&gt; to estimate the odds (probability) of passing the exam: &lt;span class=&#34;math inline&#34;&gt;\(1/(1+e^{-(-4.0777+1.5046\cdot hours)})\)&lt;/span&gt; Calculate the probability to pass exam if studied 4 hours:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1/(1+exp(-(-4.0777+1.5046*4)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.874429&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s find a critical point where probability is 0.5:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;crit = -coef(model.logit)[1]/coef(model.logit)[2]
crit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept) 
##    2.710083&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# predict &amp;#39;pass&amp;#39; for given data
df$predic.prob &amp;lt;- predict(model.logit, df, type=&amp;quot;response&amp;quot;)
df$predic.pass &amp;lt;-  ifelse(df$predic.prob &amp;gt; 0.5, 1, 0)
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    hours pass predic.prob predic.pass
## 1   0.50    0  0.03471034           0
## 2   0.75    0  0.04977295           0
## 3   1.00    0  0.07089196           0
## 4   1.25    0  0.10002862           0
## 5   1.50    0  0.13934447           0
## 6   1.75    0  0.19083650           0
## 7   1.75    1  0.19083650           0
## 8   2.00    0  0.25570318           0
## 9   2.25    1  0.33353024           0
## 10  2.50    0  0.42162653           0
## 11  2.75    1  0.51501086           1
## 12  3.00    0  0.60735865           1
## 13  3.25    1  0.69261733           1
## 14  3.50    0  0.76648084           1
## 15  4.00    1  0.87444750           1
## 16  4.25    1  0.91027764           1
## 17  4.50    1  0.93662366           1
## 18  4.75    1  0.95561071           1
## 19  5.00    1  0.96909707           1
## 20  5.50    1  0.98519444           1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot data
plot(df$hours, df$pass, pch=19, col=&amp;#39;black&amp;#39;,
     main=&amp;#39;Probability of Passing Exam vs Hours Studying&amp;#39;,
     ylab=&amp;#39;Probability of Passing Exam&amp;#39;,
     xlab=&amp;#39;Hours Studying&amp;#39;)

# data frame to build a logistic function curve &amp;#39;hours~pass&amp;#39;
df2 &amp;lt;- data.frame(hours=seq(min(df$hours),max(df$hours),0.1), pass=NA)

# predict &amp;#39;pass&amp;#39; from our model
df2$pass &amp;lt;- predict(model.logit, df2, type=&amp;quot;response&amp;quot;)
# draw logistic function for our data sets
lines(df2$pass~df2$hours, lwd=2)
# critical point
abline(h=0.5, col=&amp;#39;green&amp;#39;) # 
abline(v=crit, col=&amp;#39;red&amp;#39;) # 

# draw predicted points (-0.02 to avoid overlapping with actual data)
points(df$hours, df$predic.pass-0.03, pch=19, col=&amp;#39;red&amp;#39;)

legend(&amp;#39;bottomright&amp;#39;, lty=c(1,1,1,1),
       col = c(&amp;#39;black&amp;#39;, &amp;#39;red&amp;#39;, &amp;#39;black&amp;#39;, &amp;#39;green&amp;#39;, &amp;#39;red&amp;#39;),
       legend = c(&amp;#39;actual data&amp;#39;, &amp;#39;predicted&amp;#39;, &amp;#39;Logistic function&amp;#39;, &amp;#39;Decision p&amp;#39;, &amp;#39;Decision bound&amp;#39;),
       lwd=c(NA,NA,1,1,1), pch=c(19,19,NA,NA,NA), bty = &amp;#39;n&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/logistic_regression/logistic_regression_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic regression&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Data&lt;/em&gt;: generated credit card balance &lt;code&gt;Default{ISLR}&lt;/code&gt;. 10000 observations for 4 variables:&lt;br /&gt;
&lt;strong&gt;default&lt;/strong&gt; – binary variable: Yes, if credit card holder did not return debt;&lt;br /&gt;
&lt;strong&gt;student&lt;/strong&gt; – binary variable: Yes, if credit card holder is a student;&lt;br /&gt;
&lt;strong&gt;balance&lt;/strong&gt; – average month balance on the bank account;&lt;br /&gt;
&lt;strong&gt;income&lt;/strong&gt; – income of credit card holder.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;#39;ISLR&amp;#39;)
head(Default)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   default student   balance    income
## 1      No      No  729.5265 44361.625
## 2      No     Yes  817.1804 12106.135
## 3      No      No 1073.5492 31767.139
## 4      No      No  529.2506 35704.494
## 5      No      No  785.6559 38463.496
## 6      No     Yes  919.5885  7491.559&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
# train subset rate is 0.85
inTrain &amp;lt;- sample(seq_along(Default$default), nrow(Default)*0.85)
df &amp;lt;- Default[inTrain, ]

# logistic regression model &amp;#39;default ~ f(balance)&amp;#39;
model.logit &amp;lt;- glm(default ~ balance, data = df, family = &amp;#39;binomial&amp;#39;)
summary(model.logit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = default ~ balance, family = &amp;quot;binomial&amp;quot;, data = df)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3282  -0.1420  -0.0553  -0.0201   3.7934  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -1.088e+01  4.022e-01  -27.07   &amp;lt;2e-16 ***
## balance      5.657e-03  2.448e-04   23.11   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2509.1  on 8499  degrees of freedom
## Residual deviance: 1336.5  on 8498  degrees of freedom
## AIC: 1340.5
## 
## Number of Fisher Scoring iterations: 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Predict &amp;#39;default&amp;#39; by &amp;#39;balance&amp;#39;
p.logit &amp;lt;- predict(model.logit, df, type = &amp;#39;response&amp;#39;)
predicted &amp;lt;- factor(ifelse(p.logit &amp;gt; 0.5, 2, 1),
                  levels = c(1, 2),
                  labels = c(&amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39;))

# true values for &amp;#39;default&amp;#39; in train data
actual &amp;lt;- df$default

# confusion matrix
conf.m &amp;lt;- table(actual, predicted)
conf.m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       predicted
## actual   No  Yes
##    No  8172   41
##    Yes  194   93&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sensitivity
conf.m[2, 2] / sum(conf.m[2, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3240418&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# specificity
conf.m[1, 1] / sum(conf.m[1, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9950079&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# probability
sum(diag(conf.m)) / sum(conf.m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9723529&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-discriminant-analysis-lda&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Discriminant Analysis (LDA)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#library(&amp;#39;GGally&amp;#39;)
library(&amp;#39;MASS&amp;#39;)
model.lda &amp;lt;- lda(default ~ balance, data = Default[inTrain, ])
model.lda&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## lda(default ~ balance, data = Default[inTrain, ])
## 
## Prior probabilities of groups:
##         No        Yes 
## 0.96623529 0.03376471 
## 
## Group means:
##       balance
## No   801.1297
## Yes 1757.2025
## 
## Coefficients of linear discriminants:
##                LD1
## balance 0.00220817&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Predict
p.lda &amp;lt;- predict(model.lda, df, type = &amp;#39;response&amp;#39;)
actual &amp;lt;- factor(ifelse(p.lda$posterior[, &amp;#39;Yes&amp;#39;] &amp;gt; 0.5, 
                         2, 1),
                  levels = c(1, 2),
                  labels = c(&amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39;))

# confusion matrix
conf.m &amp;lt;- table(actual, predicted)
conf.m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       predicted
## actual   No  Yes
##    No  8366   42
##    Yes    0   92&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sensitivity
conf.m[2, 2] / sum(conf.m[2, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# specificity
conf.m[1, 1] / sum(conf.m[1, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9950048&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# true
sum(diag(conf.m)) / sum(conf.m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9950588&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;quadratic-discriminant-analysis-qda&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quadratic Discriminant Analysis (QDA)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.qda &amp;lt;- qda(default ~ balance, data = Default[inTrain, ])
model.qda&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## qda(default ~ balance, data = Default[inTrain, ])
## 
## Prior probabilities of groups:
##         No        Yes 
## 0.96623529 0.03376471 
## 
## Group means:
##       balance
## No   801.1297
## Yes 1757.2025&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# predict
p.qda &amp;lt;- predict(model.qda, df, type = &amp;#39;response&amp;#39;)
predict &amp;lt;- factor(ifelse(p.qda$posterior[, &amp;#39;Yes&amp;#39;] &amp;gt; 0.5, 
                         2, 1),
                  levels = c(1, 2),
                  labels = c(&amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39;))

# confusion matrix
conf.m &amp;lt;- table(actual, predict)
conf.m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       predict
## actual   No  Yes
##    No  8390   18
##    Yes    0   92&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sensitivity
conf.m[2, 2] / sum(conf.m[2, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# specificity
conf.m[1, 1] / sum(conf.m[1, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9978592&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# true
sum(diag(conf.m)) / sum(conf.m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9978824&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;roc-curve-for-lda&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ROC-curve for LDA&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# считаем 1-SPC и TPR для всех вариантов границы отсечения
x &amp;lt;- NULL    # для (1 - SPC)
y &amp;lt;- NULL    # для TPR

# confusion matrix
tbl &amp;lt;- as.data.frame(matrix(rep(0, 4), 2, 2))
rownames(tbl) &amp;lt;- c(&amp;#39;fact.No&amp;#39;, &amp;#39;fact.Yes&amp;#39;)
colnames(tbl) &amp;lt;- c(&amp;#39;predict.No&amp;#39;, &amp;#39;predict.Yes&amp;#39;)

# probability vector
p.vector &amp;lt;- seq(0, 1, length = 501)

# цикл по вероятностям отсечения
for (p in p.vector){
    # prediction
    prediction &amp;lt;- factor(ifelse(p.lda$posterior[, &amp;#39;Yes&amp;#39;] &amp;gt; p, 
                             2, 1),
                      levels = c(1, 2),
                      labels = c(&amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39;))
    
    # data frame to compare data with prediction
    df.compare &amp;lt;- data.frame(actual = actual, prediction = prediction)
    
    # fill confusion matrix
    tbl[1, 1] &amp;lt;- nrow(df.compare[df.compare$Факт == &amp;#39;No&amp;#39; &amp;amp; df.compare$Прогноз == &amp;#39;No&amp;#39;, ])
    tbl[2, 2] &amp;lt;- nrow(df.compare[df.compare$Факт == &amp;#39;Yes&amp;#39; &amp;amp; df.compare$Прогноз == &amp;#39;Yes&amp;#39;, ])
    tbl[1, 2] &amp;lt;- nrow(df.compare[df.compare$Факт == &amp;#39;No&amp;#39; &amp;amp; df.compare$Прогноз == &amp;#39;Yes&amp;#39;, ])
    tbl[2, 1] &amp;lt;- nrow(df.compare[df.compare$Факт == &amp;#39;Yes&amp;#39; &amp;amp; df.compare$Прогноз == &amp;#39;No&amp;#39;, ])
    
    # calculate metrix
    TPR &amp;lt;- tbl[2, 2] / sum(tbl[2, 2] + tbl[2, 1])
    y &amp;lt;- c(y, TPR)
    SPC &amp;lt;- tbl[1, 1] / sum(tbl[1, 1] + tbl[1, 2])
    x &amp;lt;- c(x, 1 - SPC)
}

# ROC-curve
par(mar = c(5, 5, 1, 1))
# curve
plot(x, y, type = &amp;#39;l&amp;#39;, col = &amp;#39;blue&amp;#39;, lwd = 3,
     xlab = &amp;#39;(1 - SPC)&amp;#39;, ylab = &amp;#39;TPR&amp;#39;, 
     xlim = c(0, 1), ylim = c(0, 1))
# line of random classifier
abline(a = 0, b = 1, lty = 3, lwd = 2)

# oint for probability 0.5
points(x[p.vector == 0.5], y[p.vector == 0.5], pch = 16)
text(x[p.vector == 0.5], y[p.vector == 0.5], &amp;#39;p = 0.5&amp;#39;, pos = 4)
# point for probability 0.2
points(x[p.vector == 0.2], y[p.vector == 0.2], pch = 16)
text(x[p.vector == 0.2], y[p.vector == 0.2], &amp;#39;p = 0.2&amp;#39;, pos = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/logistic_regression/logistic_regression_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict &amp;lt;- factor(ifelse(p.lda$posterior[, &amp;#39;Yes&amp;#39;] &amp;gt; 0.2, 2, 1),
                      levels = c(1, 2),
                      labels = c(&amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39;))

conf.m &amp;lt;- table(actual, predict)
conf.m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       predict
## actual   No  Yes
##    No  8124  284
##    Yes    0   92&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sensitivity
conf.m[2, 2] / sum(conf.m[2, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# specificity
conf.m[1, 1] / sum(conf.m[1, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9662226&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# true
sum(diag(conf.m)) / sum(conf.m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9665882&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tasks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tasks&lt;/h2&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Spline model</title>
      <link>/post/statistics/splines/splines/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/splines/splines/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#generate-dataset-from-a-given-function&#34;&gt;Generate dataset from a given function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#split-data-for-train-and-test&#34;&gt;Split data for train and test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#diagram-of-the-given-function-and-generated-datasets&#34;&gt;Diagram of the given function and generated datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#build-a-model-using-splines&#34;&gt;Build a model using splines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#diagram-of-mse-for-train-and-test-data&#34;&gt;Diagram of MSE for train and test data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#build-optimal-model-and-plot-for-the-model&#34;&gt;Build optimal model and plot for the model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bibliograpy&#34;&gt;Bibliograpy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;In this example we will generate data from a given function and then build a model using splines and estimate quality of the model.&lt;/p&gt;
&lt;div id=&#34;generate-dataset-from-a-given-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generate dataset from a given function&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# parameters to generate a dataset
n.all &amp;lt;- 100             # number of observations
train.percent &amp;lt;- 0.85    # portion of the data for training
res.sd &amp;lt;- 1              # standard deviation of noise
x.min &amp;lt;- 5               # min limit of the data
x.max &amp;lt;- 105             # max limit of the data

# generate x
set.seed(1)       # to get reproducible results by randomizer
x &amp;lt;- runif(x.min, x.max, n = n.all)

# noise from normal destibution
set.seed(1)
res &amp;lt;- rnorm(mean = 0, sd = res.sd, n = n.all)

# generate y using a given function
y.func &amp;lt;- function(x) {4 - 2e-02*x + 5.5e-03*x^2 - 4.9e-05*x^3}

# add noise
y &amp;lt;- y.func(x) + res&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;split-data-for-train-and-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Split data for train and test&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# split dataset for training and test
set.seed(1)
# generate vector of chosen x for train data
inTrain &amp;lt;- sample(seq_along(x), size = train.percent*n.all)

# train data set
x.train &amp;lt;- x[inTrain]
y.train &amp;lt;- y[inTrain]

# test data set
x.test &amp;lt;- x[-inTrain]
y.test &amp;lt;- y[-inTrain]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;diagram-of-the-given-function-and-generated-datasets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diagram of the given function and generated datasets&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# lines of generated data for plot
x.line &amp;lt;- seq(x.min, x.max, length = n.all)
y.line &amp;lt;- y.func(x.line)

# PLOT
# generate plot by train data
par(mar = c(4, 4, 1, 1)) # reduce margins (optional)
plot(x.train, y.train,
     main = &amp;#39;Generated data and original function&amp;#39;,
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = &amp;#39;X&amp;#39;, ylab = &amp;#39;Y&amp;#39;, 
     xlim = c(x.min, x.max),
     ylim = c(min(y), max(y)), 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# add points of test data
points(x.test, y.test, col = &amp;#39;red&amp;#39;, bg = &amp;#39;red&amp;#39;, pch = 21)

# add the given function
lines(x.line, y.line, lwd = 2, lty = 2)

# add legend
legend(&amp;#39;topleft&amp;#39;, legend = c(&amp;#39;train&amp;#39;, &amp;#39;test&amp;#39;, &amp;#39;f(X)&amp;#39;),
       pch = c(16, 16, NA), 
       col = c(grey(0.2), &amp;#39;red&amp;#39;, &amp;#39;black&amp;#39;),  
       lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/splines/splines_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build-a-model-using-splines&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build a model using splines&lt;/h2&gt;
&lt;p&gt;We will compair sevaral models with degree of freedoms (df) from 2 to 40, where 2 correspond to a linear model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max.df &amp;lt;- 40                       # max degree of freedom (df)
# 
tbl &amp;lt;- data.frame(df = 2:max.df)   # data frame for writing errors
tbl$MSE.train &amp;lt;- 0                 # column 1: errors of train data
tbl$MSE.test &amp;lt;- 0                  # сcolumn 2: errors of test data

# generate models using for cycle
for (i in 2:max.df) {
    mod &amp;lt;- smooth.spline(x = x.train, y = y.train, df = i)
    
    # predicted values for train and test data using built model
    y.model.train &amp;lt;- predict(mod, data.frame(x = x.train))$y[, 1]
    y.model.test &amp;lt;- predict(mod, data.frame(x = x.test))$y[, 1]
    
    # MSE errors for train and test data
    MSE &amp;lt;- c(sum((y.train - y.model.train)^2) / length(x.train),
             sum((y.test - y.model.test)^2) / length(x.test))
    
    # write errors to the previously created data frame
    tbl[tbl$df == i, c(&amp;#39;MSE.train&amp;#39;, &amp;#39;MSE.test&amp;#39;)] &amp;lt;- MSE
}

# view first rows of the table
head(tbl, 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   df MSE.train MSE.test
## 1  2 3.7188566 2.885166
## 2  3 1.4463925 1.635813
## 3  4 0.8938817 1.239533
## 4  5 0.7668250 1.038918&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;diagram-of-mse-for-train-and-test-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diagram of MSE for train and test data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot MSE from our table
plot(x = tbl$df, y = tbl$MSE.test,
     main = &amp;quot;Changes of MSE from degrees of freedom&amp;quot;,
     type = &amp;#39;l&amp;#39;, col = &amp;#39;red&amp;#39;, lwd = 2,
     xlab = &amp;#39;spline degree of freedom&amp;#39;, ylab = &amp;#39;MSE&amp;#39;,
     ylim = c(min(tbl$MSE.train, tbl$MSE.test), 
              max(tbl$MSE.train, tbl$MSE.test)),
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# add 
points(x = tbl$df, y = tbl$MSE.test,
       pch = 21, col = &amp;#39;red&amp;#39;, bg = &amp;#39;red&amp;#39;)
lines(x = tbl$df, y = tbl$MSE.train, col = grey(0.3), lwd = 2)
# minimal MSE
abline(h = res.sd, lty = 2, col = grey(0.4), lwd = 2)

# add legend
legend(&amp;#39;topright&amp;#39;, legend = c(&amp;#39;train&amp;#39;, &amp;#39;test&amp;#39;),
       pch = c(NA, 16), 
       col = c(grey(0.2), &amp;#39;red&amp;#39;),  
       lty = c(1, 1), lwd = c(2, 2), cex = 1.2)

# df of minimal MSE for test data
min.MSE.test &amp;lt;- min(tbl$MSE.test)
df.min.MSE.test &amp;lt;- tbl[tbl$MSE.test == min.MSE.test, &amp;#39;df&amp;#39;]

# optimal df for precise model and maximal simplicity
df.my.MSE.test &amp;lt;- 6
my.MSE.test &amp;lt;- tbl[tbl$df == df.my.MSE.test, &amp;#39;MSE.test&amp;#39;]

# show the optimal solution
abline(v = df.my.MSE.test, 
       lty = 2, lwd = 2)
points(x = df.my.MSE.test, y = my.MSE.test, 
       pch = 15, col = &amp;#39;blue&amp;#39;)
mtext(df.my.MSE.test, 
      side = 1, line = -1, at = df.my.MSE.test, col = &amp;#39;blue&amp;#39;, cex = 1.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/splines/splines_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build-optimal-model-and-plot-for-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build optimal model and plot for the model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.MSE.test &amp;lt;- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test)

# predict data for 250 x&amp;#39;s to get smoothed curve
x.model.plot &amp;lt;- seq(x.min, x.max, length = 250)
y.model.plot &amp;lt;- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1]

# plot train data
par(mar = c(4, 4, 1, 1))
plot(x.train, y.train,
     main = &amp;quot;Initial data and the best fit model&amp;quot;,
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = &amp;#39;X&amp;#39;, ylab = &amp;#39;Y&amp;#39;, 
     xlim = c(x.min, x.max),
     ylim = c(min(y), max(y)), 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# add test data
points(x.test, y.test, col = &amp;#39;red&amp;#39;, bg = &amp;#39;red&amp;#39;, pch = 21)

# function
lines(x.line, y.line,lwd = 2, lty = 2)

# add model
lines(x.model.plot, y.model.plot, lwd = 2, col = &amp;#39;blue&amp;#39;)

# legend
legend(&amp;#39;topleft&amp;#39;, legend = c(&amp;#39;train&amp;#39;, &amp;#39;test&amp;#39;, &amp;#39;f(X)&amp;#39;, &amp;#39;model&amp;#39;),
       pch = c(16, 16, NA, NA), 
       col = c(grey(0.2), &amp;#39;red&amp;#39;, &amp;#39;black&amp;#39;, &amp;#39;blue&amp;#39;),  
       lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/splines/splines_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliograpy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bibliograpy&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://faculty.marshall.usc.edu/gareth-james/&#34;&gt;An Introduction to Statistical Learning by Gareth James&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
