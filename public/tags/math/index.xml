<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Math | Mark Goldberg</title>
    <link>https://suvar.netlify.com/tags/math/</link>
      <atom:link href="https://suvar.netlify.com/tags/math/index.xml" rel="self" type="application/rss+xml" />
    <description>Math</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019</copyright><lastBuildDate>Wed, 14 Aug 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://suvar.netlify.com/img/logo.png</url>
      <title>Math</title>
      <link>https://suvar.netlify.com/tags/math/</link>
    </image>
    
    <item>
      <title>Linear Regression (Math)</title>
      <link>https://suvar.netlify.com/post/statistics/linear_regression_math/linear_regression_math/</link>
      <pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/linear_regression_math/linear_regression_math/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#simple-linear-regression&#34;&gt;Simple linear regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-find-coefficients-0-and-1&#34;&gt;How to find coefficients β0 and β1?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix-form-for-multiple-regression&#34;&gt;Matrix form for multiple regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;simple-linear-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simple linear regression&lt;/h2&gt;
&lt;p&gt;Ordinary Least Squares (OLS) function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(minimize\left\{SSE = sum_{i=1}^{n} (y_i - \hat y_i)^2 \right\}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Assumtions of OLS regression:&lt;br /&gt;
* Linear relationship&lt;br /&gt;
* Multivariathe normality&lt;br /&gt;
* No autocorrelation&lt;br /&gt;
* Homoscedastic (constant variance in residuals)&lt;br /&gt;
* There are more observations (n) than features (p) (n &amp;gt; p)&lt;br /&gt;
* No or little multicollinearity&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat y = \beta_0 + \beta_1 x\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; - expectd value of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, the same as &lt;span class=&#34;math inline&#34;&gt;\(E(Y|x)\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; - intercept&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; - slope&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat \beta_0 = \bar y - \hat\beta_1 \bar x\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_1 = \frac{\displaystyle\sum_{i=1}^{n} (x_i - \bar x)(y_i-\bar y)}{\displaystyle\sum_{i=1}^{n} (x_i - \bar x)^2}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-find-coefficients-0-and-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to find coefficients β0 and β1?&lt;/h2&gt;
&lt;p&gt;There are several ways to extimate coefficients of linear regression. Here we discuss the least squares approach. Other aproaches include &lt;strong&gt;maximul likelihood estimation&lt;/strong&gt;.&lt;br /&gt;
To estimate &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; we should find &lt;strong&gt;minimum&lt;/strong&gt; of &lt;strong&gt;sum of squared residuals&lt;/strong&gt; (&lt;span class=&#34;math inline&#34;&gt;\(SSR\)&lt;/span&gt;)&lt;br /&gt;
To find this minimum we should calculate derivatives of &lt;span class=&#34;math inline&#34;&gt;\(SSR\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and set them to 0:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle\min_{\beta_0,\beta1} : SSR \implies \frac{\partial SSR}{\partial \beta_0} = \frac{\partial SSR}{\partial \beta_1} = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here is the solution:&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(SSR = \displaystyle\sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1x_i))^2 = \displaystyle\sum_{i=1}^{n} (y_i^2 - 2y_i \beta_0 - 2y_i \beta_1 x_i + \beta_0^2 + 2\beta_0\beta_1x_i + \beta_1^2x_i^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial SSR}{\partial\beta_0} = \displaystyle\sum_{i=1}^{n}(-2y_i + 2\beta_0 + 2\beta_1x_i)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle\sum_{i=1}^{n} (-y_i + \hat\beta_0 + \hat\beta_1 x_i) = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar y = \frac{1}{n} \displaystyle\sum_{i=1}^{n} y_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar x = \frac{1}{n} \displaystyle\sum_{i=1}^{n} x_i \implies -n\bar y + n \hat\beta_0 + \hat\beta_1 n \hat x = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat \beta_0 = \bar y - \hat\beta_1 \bar x\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial SSR}{\partial \beta_1} = \displaystyle\sum_{i=1}^{n} (-2x_i y_i + 2\beta_0 x_i + 2\beta_1 x_i^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(-\displaystyle\sum_{i=1}^{n} x_i y_i + \hat\beta_0 \displaystyle\sum_{i=1}^{n} x_i + \hat\beta_1\displaystyle\sum_{i=1}^{n} x_i^2 = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(-\displaystyle\sum_{i=1}^{n} x_i y_i + (\bar y - \hat\beta_1 \bar x) \displaystyle\sum_{i=1}^{n} x_i + \hat\beta_1\displaystyle\sum_{i=1}^{n} x_i^2 = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_1 = \frac{\displaystyle\sum_{i=1}^{n} x_i(y_i-\bar y)}{\displaystyle\sum_{i=1}^{n} x_i (x_i - \bar x)} = \frac{\displaystyle\sum_{i=1}^{n} (x_i - \bar x)(y_i-\bar y)}{\displaystyle\sum_{i=1}^{n} (x_i - \bar x)^2} = \frac{Cov(x,y)}{Var(x)} = r_{xy} \frac{s_y}{s_x}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; - estimated &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt; - averages of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(r_{xy}\)&lt;/span&gt; - sample correlation coefficient between &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(s_x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s_y\)&lt;/span&gt; - uncorrected sample standard deviations of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(Var\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Cov\)&lt;/span&gt; - sample variance and sample covariance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-form-for-multiple-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Matrix form for multiple regression&lt;/h2&gt;
&lt;p&gt;We can write &lt;span class=&#34;math inline&#34;&gt;\(\hat y = \beta_0 + \beta_1 x\)&lt;/span&gt; in a matrix form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y = \begin{bmatrix}y_1\\y_2\\\vdots\\y_n\end{bmatrix}\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(b = \begin{bmatrix}\beta_1\\\beta_2\\\vdots\\\beta_n\end{bmatrix}\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(X = \begin{bmatrix}1 &amp;amp; x_{1,1} &amp;amp; x_{1,2} \dots &amp;amp; x_{1,k}\\1 &amp;amp; x_{2,1} &amp;amp; x_{2,2} \dots &amp;amp; x_{2,k}\\\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\\1 &amp;amp; x_{n,1} &amp;amp; x_{n,2} \dots &amp;amp; x_{n,k}\end{bmatrix}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y = Xb\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X&amp;#39;Y = X&amp;#39;Xb\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\((X&amp;#39;X)^{-1}X&amp;#39;Xb = (X&amp;#39;X)^{-1}X&amp;#39;Y\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(b = (X&amp;#39;X)^{-1}X&amp;#39;Y\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\((X&amp;#39;X)^{-1}X&amp;#39;X = I\)&lt;/span&gt; - identity matrix&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
