<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Clustering | Mark Goldberg</title>
    <link>/tags/clustering/</link>
      <atom:link href="/tags/clustering/index.xml" rel="self" type="application/rss+xml" />
    <description>Clustering</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019</copyright><lastBuildDate>Fri, 09 Aug 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/logo.png</url>
      <title>Clustering</title>
      <link>/tags/clustering/</link>
    </image>
    
    <item>
      <title>Determining the optimal number of clusters</title>
      <link>/post/statistics/determin_number_clusters/determin_number_clusters/</link>
      <pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/determin_number_clusters/determin_number_clusters/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#elbow-method&#34;&gt;Elbow method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#average-silhouette-method&#34;&gt;Average silhouette method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gap-statistic-method&#34;&gt;Gap statistic method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-nbclust&#34;&gt;Using NbCLust&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sources&#34;&gt;Sources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;elbow-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Elbow method&lt;/h2&gt;
&lt;p&gt;The basic idea is to find &lt;strong&gt;minimal the total intra-cluster variation&lt;/strong&gt; or &lt;strong&gt;total Within-cluster Sum ofSquares&lt;/strong&gt; (WSS).&lt;br /&gt;
Plot number of clusters ~ &lt;em&gt;WSS&lt;/em&gt; show how WSS is reduced with increase of number of clusters. The optimal number of clusters is when adding another cluster doesn’t improve much better the total WSS.&lt;br /&gt;
The optimal number of clusters can be defined as follow:&lt;br /&gt;
1. Compute clustering algorithm (e.g k-means) for different number of clusters (e.g k is from 1 to 10)&lt;br /&gt;
2. For each k calculate the total within-cluster sum of squares (wss).&lt;br /&gt;
3. Plot k ~ wss 4. The location of a bend (knee) in the plot is generally considered as an indicator of the optimal number of clusters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;average-silhouette-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Average silhouette method&lt;/h2&gt;
&lt;p&gt;**Average silhouette method determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering.&lt;br /&gt;
The optimal number of clusters is detected by maximal the average slhouette.&lt;/p&gt;
&lt;p&gt;The optimal number of clusters can be defined as follow:&lt;br /&gt;
1. Compute clustering algorithm (e.g k-means) for different number of clusters (e.g k is from 1 to 10)&lt;br /&gt;
2. For each k, calculate the average silhouette of observations (aso).&lt;br /&gt;
3. Plot k ~ aso&lt;br /&gt;
4. The location of the maximum is considered as the optimal number of clusters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gap-statistic-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gap statistic method&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;gap statistic&lt;/strong&gt; compares the &lt;strong&gt;total within intra-cluster variation&lt;/strong&gt; for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e, that yields the largest gap statistic).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(factoextra)

# normalized data
df &amp;lt;- scale(iris[, -5])
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Sepal.Length Sepal.Width Petal.Length Petal.Width
## [1,]   -0.8976739  1.01560199    -1.335752   -1.311052
## [2,]   -1.1392005 -0.13153881    -1.335752   -1.311052
## [3,]   -1.3807271  0.32731751    -1.392399   -1.311052
## [4,]   -1.5014904  0.09788935    -1.279104   -1.311052
## [5,]   -1.0184372  1.24503015    -1.335752   -1.311052
## [6,]   -0.5353840  1.93331463    -1.165809   -1.048667&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Elbow method
fviz_nbclust(df, kmeans, method = &amp;quot;wss&amp;quot;) +
    labs(subtitle = &amp;quot;Elbow method&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/determin_number_clusters/determin_number_clusters_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Silhouette method
fviz_nbclust(df, kmeans, method = &amp;quot;silhouette&amp;quot;)+
    labs(subtitle = &amp;quot;Silhouette method&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/determin_number_clusters/determin_number_clusters_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Gap statistic
set.seed(123)
fviz_nbclust(df, kmeans, nstart = 25,  method = &amp;quot;gap_stat&amp;quot;, nboot = 50)+
  labs(subtitle = &amp;quot;Gap statistic method&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/determin_number_clusters/determin_number_clusters_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt; As we can see from these examples, the &lt;strong&gt;Elbow method&lt;/strong&gt; predicts 2-3 clusters, &lt;strong&gt;Silhouette method&lt;/strong&gt; predicts 2 and the &lt;strong&gt;Gap statistic method&lt;/strong&gt; predicts 2 clusters. As we know, &lt;code&gt;iris&lt;/code&gt; data contains 3 clusters of different &lt;code&gt;Species&lt;/code&gt; and two of them is generally dificult to distinguish. This situation we can see from the shown diagrams. Unfortunately there is no method to exactly predict number of clusters. As researchers we should try to find if number of clusters have natural sense.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-nbclust&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using NbCLust&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(NbClust)
res &amp;lt;- NbClust(data = df,                 # matrix or data frame
               distance = &amp;quot;euclidean&amp;quot;,    # distance
               method = &amp;quot;kmeans&amp;quot;,        # method
               min.nc = 2, max.nc = 10)   # min and max number of clusters to test&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/determin_number_clusters/determin_number_clusters_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## *** : The Hubert index is a graphical method of determining the number of clusters.
##                 In the plot of Hubert index, we seek a significant knee that corresponds to a 
##                 significant increase of the value of the measure i.e the significant peak in Hubert
##                 index second differences plot. 
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/determin_number_clusters/determin_number_clusters_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## *** : The D index is a graphical method of determining the number of clusters. 
##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex
##                 second differences plot) that corresponds to a significant increase of the value of
##                 the measure. 
##  
## ******************************************************************* 
## * Among all indices:                                                
## * 11 proposed 2 as the best number of clusters 
## * 10 proposed 3 as the best number of clusters 
## * 1 proposed 4 as the best number of clusters 
## * 1 proposed 6 as the best number of clusters 
## * 1 proposed 10 as the best number of clusters 
## 
##                    ***** Conclusion *****                            
##  
## * According to the majority rule, the best number of clusters is  2 
##  
##  
## *******************************************************************&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res$Best.nc[,1:5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     KL       CH Hartigan    CCC    Scott
## Number_clusters 3.0000   2.0000   3.0000 3.0000   3.0000
## Value_Index     5.1669 251.3493  54.2213 5.1886 131.6411&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Example with iris data
data &amp;lt;- iris[, -5]
# distance matrix
diss_matrix &amp;lt;-  dist(data, method = &amp;quot;euclidean&amp;quot;) # here method is the distance method
res &amp;lt;- NbClust(data,                    # matrix or data frame
               diss = diss_matrix,      # distance matrix
               distance = NULL,         # if set, diss musnt be NULL
               min.nc = 2, max.nc = 10, # min and max number of clusters to test
               method = &amp;quot;complete&amp;quot;,     # clustering method
               index = &amp;quot;alllong&amp;quot;)       # all indexes; can be selected&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/determin_number_clusters/determin_number_clusters_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## *** : The Hubert index is a graphical method of determining the number of clusters.
##                 In the plot of Hubert index, we seek a significant knee that corresponds to a 
##                 significant increase of the value of the measure i.e the significant peak in Hubert
##                 index second differences plot. 
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/determin_number_clusters/determin_number_clusters_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## *** : The D index is a graphical method of determining the number of clusters. 
##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex
##                 second differences plot) that corresponds to a significant increase of the value of
##                 the measure. 
##  
## ******************************************************************* 
## * Among all indices:                                                
## * 2 proposed 2 as the best number of clusters 
## * 15 proposed 3 as the best number of clusters 
## * 6 proposed 4 as the best number of clusters 
## * 1 proposed 6 as the best number of clusters 
## * 3 proposed 10 as the best number of clusters 
## 
##                    ***** Conclusion *****                            
##  
## * According to the majority rule, the best number of clusters is  3 
##  
##  
## *******************************************************************&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# indeces first 5 methods
head(res$All.index[,1:7])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        KL       CH Hartigan     CCC     Scott  Marriot    TrCovW
## 2  1.9652 280.8392 240.7478 30.4441  933.9084 977604.0 6868.5401
## 3  5.3598 485.9050  68.8363 35.8668 1210.7629 347351.8  304.1791
## 4 54.0377 495.1816  16.4167 35.6036 1346.7582 249402.3  135.7432
## 5  0.0263 414.3925  51.1371 33.0698 1387.9419 296129.2  121.5044
## 6  7.1653 455.4931  16.8076 33.9870 1506.5585 193380.9   96.9908
## 7  0.5308 423.7198  20.2960 32.9063 1560.0089 184311.4   93.2005&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optimal number of clusters
best.result &amp;lt;- t(res$Best.nc)
best.result&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Number_clusters Value_Index
## KL                       4     54.0377
## CH                       4    495.1816
## Hartigan                 3    171.9115
## CCC                      3     35.8668
## Scott                    3    276.8545
## Marriot                  3 532302.6940
## TrCovW                   3   6564.3610
## TraceW                   3    117.0760
## Friedman                 4    151.3607
## Rubin                    6    -33.9014
## Cindex                   3      0.3163
## DB                       3      0.7025
## Silhouette               2      0.5160
## Duda                     4      0.5932
## PseudoT2                 4     32.9134
## Beale                    3      1.8840
## Ratkowsky                3      0.4922
## Ball                     3     87.7349
## PtBiserial               3      0.7203
## Gap                      3      0.1343
## Frey                     1          NA
## McClain                  2      0.4228
## Gamma                    4      0.9261
## Gplus                   10     52.7862
## Tau                      3   2649.8405
## Dunn                    10      0.1543
## Hubert                   0      0.0000
## SDindex                  3      1.6226
## Dindex                   0      0.0000
## SDbw                    10      0.0303&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# density plot of all best results
library(magrittr)
best.result %&amp;gt;%
  .[,1] %&amp;gt;%
  na.omit() %&amp;gt;%
  as.vector() %&amp;gt;%
  density(.) %&amp;gt;% 
  plot(., xlab=&amp;#39;k, klusters&amp;#39;, main=&amp;#39;Optimal number of clusters&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/determin_number_clusters/determin_number_clusters_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Es we can see the most of methods predict k=3 for clustering &lt;code&gt;iris&lt;/code&gt; data.&lt;br /&gt;
If you try k-means method, you will get optimal between 2 and 3.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/&#34;&gt;Cluster Validataion Essentials by Alboukadel Kassambara&lt;/a&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://web.stanford.edu/~hastie/Papers/gap.pdf&#34;&gt;Tibshirani R at al. Estimating the number of clusters in a data set via the gap statistic. J.R.Statist.Soc.B, 2001&lt;/a&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;[NbClust: An R Package for Determining the Relevant Number of Clusters in a Data] Set(&lt;a href=&#34;https://cran.r-project.org/web/packages/NbClust/NbClust.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/NbClust/NbClust.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>k-means</title>
      <link>/post/statistics/k_means/k_means/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/k_means/k_means/</guid>
      <description>


&lt;div id=&#34;k-means-cluster-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;K-means cluster analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(datasets)
df &amp;lt;- datasets::iris
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 3 species:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique(df$Species)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] setosa     versicolor virginica 
## Levels: setosa versicolor virginica&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We try to predict them by &lt;em&gt;Petal.Lenght&lt;/em&gt;and &lt;em&gt;Petal.Width&lt;/em&gt; variables using k-means clustering.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot Petal.Length ~ Petal.Width data
plot(df$Petal.Length ~ df$Petal.Width)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/k_means/k_means_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Find number of clusters using wss
wss &amp;lt;- (nrow(df[, 3:4])-1)*sum(apply(df[, 3:4],2,var))
for (i in 2:15) wss[i] &amp;lt;- sum(kmeans(df[, 3:4], i)$withinss)
plot(1:15, wss, type=&amp;quot;b&amp;quot;, xlab=&amp;quot;Number of Clusters&amp;quot;, ylab=&amp;quot;Within groups sum of squares&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/k_means/k_means_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt; More than 3 clusters give no obvious advantages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Make k-means with 3 clasters
ncl &amp;lt;- 3
cl &amp;lt;- stats::kmeans(df[, 3:4], ncl, nstart = 20)
cl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## K-means clustering with 3 clusters of sizes 48, 50, 52
## 
## Cluster means:
##   Petal.Length Petal.Width
## 1     5.595833    2.037500
## 2     1.462000    0.246000
## 3     4.269231    1.342308
## 
## Clustering vector:
##   [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
##  [36] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [71] 3 3 3 3 3 3 3 1 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1
## [106] 1 3 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 3 1
## [141] 1 1 1 1 1 1 1 1 1 1
## 
## Within cluster sum of squares by cluster:
## [1] 16.29167  2.02200 13.05769
##  (between_SS / total_SS =  94.3 %)
## 
## Available components:
## 
## [1] &amp;quot;cluster&amp;quot;      &amp;quot;centers&amp;quot;      &amp;quot;totss&amp;quot;        &amp;quot;withinss&amp;quot;    
## [5] &amp;quot;tot.withinss&amp;quot; &amp;quot;betweenss&amp;quot;    &amp;quot;size&amp;quot;         &amp;quot;iter&amp;quot;        
## [9] &amp;quot;ifault&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compair result of clustering with real data (3 species of iris are in analysis)
table(cl$cluster, df$Species)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    
##     setosa versicolor virginica
##   1      0          2        46
##   2     50          0         0
##   3      0         48         4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot data
clusters &amp;lt;- split.data.frame(df, cl$cluster)
xlim &amp;lt;- c(min(df$Petal.Width), max(df$Petal.Width))
ylim &amp;lt;- c(min(df$Petal.Length), max(df$Petal.Length))
col &amp;lt;- c(&amp;#39;red&amp;#39;, &amp;#39;green&amp;#39;, &amp;#39;blue&amp;#39;)
plot(0, xlab=&amp;#39;Petal width&amp;#39;, ylab=&amp;#39;Petal length&amp;#39;, xlim=xlim, ylim=ylim)
for ( i in 1:ncl ) {
  points(clusters[[i]]$Petal.Length ~ clusters[[i]]$Petal.Width, col=col[i], xlim=xlim, ylim=ylim)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/k_means/k_means_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hierarchical cluster analysis</title>
      <link>/post/statistics/hierarchical_cluster/hierarchical_cluser/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/hierarchical_cluster/hierarchical_cluser/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#hierarchical-cluster-analysis&#34;&gt;Hierarchical cluster analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#choosing-optimal-number-of-clusters&#34;&gt;Choosing optimal number of clusters&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#elbow-method&#34;&gt;Elbow method&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#choosing-the-best-clustering-model&#34;&gt;Choosing the best clustering model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hierarchical-cluster-analysis-1&#34;&gt;Hierarchical cluster analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bibliography&#34;&gt;Bibliography&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;hierarchical-cluster-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical cluster analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare data for analysis &lt;code&gt;datasets::USArrests&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(datasets)
df &amp;lt;- datasets::USArrests
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Murder Assault UrbanPop Rape
## Alabama      13.2     236       58 21.2
## Alaska       10.0     263       48 44.5
## Arizona       8.1     294       80 31.0
## Arkansas      8.8     190       50 19.5
## California    9.0     276       91 40.6
## Colorado      7.9     204       78 38.7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# check if &amp;#39;NA&amp;#39; values present in data
any(is.na(df))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# remove &amp;#39;NA&amp;#39; if necessary
df &amp;lt;- na.omit(df)
# normalize
df &amp;lt;- scale(df)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-optimal-number-of-clusters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing optimal number of clusters&lt;/h2&gt;
&lt;div id=&#34;elbow-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Elbow method&lt;/h3&gt;
&lt;p&gt;plot &lt;strong&gt;k&lt;/strong&gt; ~ &lt;strong&gt;wss&lt;/strong&gt;, where &lt;strong&gt;k&lt;/strong&gt; - is a cluseter number and &lt;strong&gt;wss&lt;/strong&gt; is a total within-cluster sum of square.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wss &amp;lt;- (nrow(df)-1)*sum(apply(df,2,var))
for (i in 2:15) wss[i] &amp;lt;- sum(kmeans(df, centers=i)$withinss)
plot(1:15, wss, type=&amp;quot;b&amp;quot;,
     xlab=&amp;quot;Number of Clusters&amp;quot;,
     ylab=&amp;quot;Within groups sum of squares&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/hierarchical_cluster/hierarchical_cluser_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt; This diagram shows that 4 number of clusers is optimal for this dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-the-best-clustering-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing the best clustering model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc2 &amp;lt;- cluster::agnes(df, method=&amp;#39;complete&amp;#39;)
hc2$ac&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8531583&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This metric allows to estimate the quality of cluster. Closer to 1 is better.&lt;br /&gt;
Using this metric we can try several models and choose the best one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- c( &amp;quot;average&amp;quot;, &amp;quot;single&amp;quot;, &amp;quot;complete&amp;quot;, &amp;quot;ward&amp;quot;)
names(m) &amp;lt;- c( &amp;quot;average&amp;quot;, &amp;quot;single&amp;quot;, &amp;quot;complete&amp;quot;, &amp;quot;ward&amp;quot;)
# function to compute coefficient
ac &amp;lt;- function(x) { cluster::agnes(df, method = x)$ac }

purrr::map_dbl(m, ac)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   average    single  complete      ward 
## 0.7379371 0.6276128 0.8531583 0.9346210&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see the &lt;strong&gt;ward method&lt;/strong&gt; gives the best clustering.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-cluster-analysis-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical cluster analysis&lt;/h2&gt;
&lt;p&gt;Let’s first split data into 4 groups using &lt;code&gt;clust::hclust&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calculate distances
d &amp;lt;- dist(df, method=&amp;#39;euclidean&amp;#39;)

# hierarchical cluster analysis
# &amp;#39;ward.D2&amp;#39; method is equivalent of agnes &amp;#39;ward&amp;#39;
hc1 &amp;lt;- hclust(d, method=&amp;#39;ward.D2&amp;#39;)

# Plot the obtained dendrogram
plot(hc1, hang = -1, cex = 0.6)

# show 4 clusers
rect.hclust(hc1, k=4, border=&amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/hierarchical_cluster/hierarchical_cluser_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# group data by clusters
groups &amp;lt;- cutree(hc1, k=3)
names(groups[groups == 1])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Alabama&amp;quot;        &amp;quot;Alaska&amp;quot;         &amp;quot;Arizona&amp;quot;        &amp;quot;California&amp;quot;    
##  [5] &amp;quot;Colorado&amp;quot;       &amp;quot;Florida&amp;quot;        &amp;quot;Georgia&amp;quot;        &amp;quot;Illinois&amp;quot;      
##  [9] &amp;quot;Louisiana&amp;quot;      &amp;quot;Maryland&amp;quot;       &amp;quot;Michigan&amp;quot;       &amp;quot;Mississippi&amp;quot;   
## [13] &amp;quot;Nevada&amp;quot;         &amp;quot;New Mexico&amp;quot;     &amp;quot;New York&amp;quot;       &amp;quot;North Carolina&amp;quot;
## [17] &amp;quot;South Carolina&amp;quot; &amp;quot;Tennessee&amp;quot;      &amp;quot;Texas&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# check for cluster metrics
names(hc1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;merge&amp;quot;       &amp;quot;height&amp;quot;      &amp;quot;order&amp;quot;       &amp;quot;labels&amp;quot;      &amp;quot;method&amp;quot;     
## [6] &amp;quot;call&amp;quot;        &amp;quot;dist.method&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can split data into 4 groups using &lt;code&gt;cluster::agnes&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# using &amp;#39;agnes&amp;#39; for hierarhical clustering
hc3 &amp;lt;- cluster::agnes(df, method=&amp;#39;ward&amp;#39;)
# plot slaster
cluster::pltree(hc3, hang = -1, cex = 0.6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/hierarchical_cluster/hierarchical_cluser_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# split into groups
groups &amp;lt;- cutree(as.hclust(hc3), k = 4)
groups[groups==1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        Alabama        Georgia      Louisiana    Mississippi North Carolina 
##              1              1              1              1              1 
## South Carolina      Tennessee 
##              1              1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://uc-r.github.io/hc_clustering&#34;&gt;UC Business Analytics R Programming Guide&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
