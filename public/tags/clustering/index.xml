<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Clustering | Mark Goldberg</title>
    <link>/tags/clustering/</link>
      <atom:link href="/tags/clustering/index.xml" rel="self" type="application/rss+xml" />
    <description>Clustering</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019</copyright><lastBuildDate>Fri, 09 Aug 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/logo.png</url>
      <title>Clustering</title>
      <link>/tags/clustering/</link>
    </image>
    
    <item>
      <title>Determining the optimal number of clusters</title>
      <link>/post/statistics/determin_number_clusters/determin_number_clusters/</link>
      <pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/determin_number_clusters/determin_number_clusters/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#elbow-method&#34;&gt;Elbow method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#average-silhouette-method&#34;&gt;Average silhouette method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gap-statistic-method&#34;&gt;Gap statistic method&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-nbclust&#34;&gt;Using NbCLust&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sources&#34;&gt;Sources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;elbow-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Elbow method&lt;/h2&gt;
&lt;p&gt;The basic idea is to find &lt;strong&gt;minimal the total intra-cluster variation&lt;/strong&gt; or &lt;strong&gt;total Within-cluster Sum ofSquares&lt;/strong&gt; (WSS).&lt;br /&gt;
Plot number of clusters ~ &lt;em&gt;WSS&lt;/em&gt; show how WSS is reduced with increase of number of clusters. The optimal number of clusters is when adding another cluster doesn’t improve much better the total WSS.&lt;br /&gt;
The optimal number of clusters can be defined as follow:&lt;br /&gt;
1. Compute clustering algorithm (e.g k-means) for different number of clusters (e.g k is from 1 to 10)&lt;br /&gt;
2. For each k calculate the total within-cluster sum of squares (wss).&lt;br /&gt;
3. Plot k ~ wss 4. The location of a bend (knee) in the plot is generally considered as an indicator of the optimal number of clusters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;average-silhouette-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Average silhouette method&lt;/h2&gt;
&lt;p&gt;**Average silhouette method determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering.&lt;br /&gt;
The optimal number of clusters is detected by maximal the average slhouette.&lt;/p&gt;
&lt;p&gt;The optimal number of clusters can be defined as follow:&lt;br /&gt;
1. Compute clustering algorithm (e.g k-means) for different number of clusters (e.g k is from 1 to 10)&lt;br /&gt;
2. For each k, calculate the average silhouette of observations (aso).&lt;br /&gt;
3. Plot k ~ aso&lt;br /&gt;
4. The location of the maximum is considered as the optimal number of clusters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gap-statistic-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gap statistic method&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;gap statistic&lt;/strong&gt; compares the &lt;strong&gt;total within intra-cluster variation&lt;/strong&gt; for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e, that yields the largest gap statistic).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(factoextra)

# normalized data
df &amp;lt;- scale(iris[, -5])
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Sepal.Length Sepal.Width Petal.Length Petal.Width
## [1,]   -0.8976739  1.01560199    -1.335752   -1.311052
## [2,]   -1.1392005 -0.13153881    -1.335752   -1.311052
## [3,]   -1.3807271  0.32731751    -1.392399   -1.311052
## [4,]   -1.5014904  0.09788935    -1.279104   -1.311052
## [5,]   -1.0184372  1.24503015    -1.335752   -1.311052
## [6,]   -0.5353840  1.93331463    -1.165809   -1.048667&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Elbow method
fviz_nbclust(df, kmeans, method = &amp;quot;wss&amp;quot;) +
    labs(subtitle = &amp;quot;Elbow method&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/determin_number_clusters/determin_number_clusters_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Silhouette method
fviz_nbclust(df, kmeans, method = &amp;quot;silhouette&amp;quot;)+
    labs(subtitle = &amp;quot;Silhouette method&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/determin_number_clusters/determin_number_clusters_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Gap statistic
set.seed(123)
fviz_nbclust(df, kmeans, nstart = 25,  method = &amp;quot;gap_stat&amp;quot;, nboot = 50)+
  labs(subtitle = &amp;quot;Gap statistic method&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/determin_number_clusters/determin_number_clusters_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt; As we can see from these examples, the &lt;strong&gt;Elbow method&lt;/strong&gt; predicts 2-3 clusters, &lt;strong&gt;Silhouette method&lt;/strong&gt; predicts 2 and the &lt;strong&gt;Gap statistic method&lt;/strong&gt; predicts 2 clusters. As we know, &lt;code&gt;iris&lt;/code&gt; data contains 3 clusters of different &lt;code&gt;Species&lt;/code&gt; and two of them is generally dificult to distinguish. This situation we can see from the shown diagrams. Unfortunately there is no method to exactly predict number of clusters. As researchers we should try to find if number of clusters have natural sense.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-nbclust&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using NbCLust&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(NbClust)
res &amp;lt;- NbClust(data = df,                 # matrix or data frame
               distance = &amp;quot;euclidean&amp;quot;,    # distance
               method = &amp;quot;kmeans&amp;quot;,        # method
               min.nc = 2, max.nc = 10)   # min and max number of clusters to test&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/determin_number_clusters/determin_number_clusters_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## *** : The Hubert index is a graphical method of determining the number of clusters.
##                 In the plot of Hubert index, we seek a significant knee that corresponds to a 
##                 significant increase of the value of the measure i.e the significant peak in Hubert
##                 index second differences plot. 
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/determin_number_clusters/determin_number_clusters_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## *** : The D index is a graphical method of determining the number of clusters. 
##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex
##                 second differences plot) that corresponds to a significant increase of the value of
##                 the measure. 
##  
## ******************************************************************* 
## * Among all indices:                                                
## * 11 proposed 2 as the best number of clusters 
## * 10 proposed 3 as the best number of clusters 
## * 1 proposed 4 as the best number of clusters 
## * 1 proposed 6 as the best number of clusters 
## * 1 proposed 10 as the best number of clusters 
## 
##                    ***** Conclusion *****                            
##  
## * According to the majority rule, the best number of clusters is  2 
##  
##  
## *******************************************************************&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res$Best.nc[,1:5]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     KL       CH Hartigan    CCC    Scott
## Number_clusters 3.0000   2.0000   3.0000 3.0000   3.0000
## Value_Index     5.1669 251.3493  54.2213 5.1886 131.6411&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Example with iris data
data &amp;lt;- iris[, -5]
# distance matrix
diss_matrix &amp;lt;-  dist(data, method = &amp;quot;euclidean&amp;quot;) # here method is the distance method
res &amp;lt;- NbClust(data,                    # matrix or data frame
               diss = diss_matrix,      # distance matrix
               distance = NULL,         # if set, diss musnt be NULL
               min.nc = 2, max.nc = 10, # min and max number of clusters to test
               method = &amp;quot;complete&amp;quot;,     # clustering method
               index = &amp;quot;alllong&amp;quot;)       # all indexes; can be selected&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/determin_number_clusters/determin_number_clusters_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## *** : The Hubert index is a graphical method of determining the number of clusters.
##                 In the plot of Hubert index, we seek a significant knee that corresponds to a 
##                 significant increase of the value of the measure i.e the significant peak in Hubert
##                 index second differences plot. 
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/determin_number_clusters/determin_number_clusters_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## *** : The D index is a graphical method of determining the number of clusters. 
##                 In the plot of D index, we seek a significant knee (the significant peak in Dindex
##                 second differences plot) that corresponds to a significant increase of the value of
##                 the measure. 
##  
## ******************************************************************* 
## * Among all indices:                                                
## * 2 proposed 2 as the best number of clusters 
## * 15 proposed 3 as the best number of clusters 
## * 6 proposed 4 as the best number of clusters 
## * 1 proposed 6 as the best number of clusters 
## * 3 proposed 10 as the best number of clusters 
## 
##                    ***** Conclusion *****                            
##  
## * According to the majority rule, the best number of clusters is  3 
##  
##  
## *******************************************************************&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# indeces first 5 methods
head(res$All.index[,1:7])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        KL       CH Hartigan     CCC     Scott  Marriot    TrCovW
## 2  1.9652 280.8392 240.7478 30.4441  933.9084 977604.0 6868.5401
## 3  5.3598 485.9050  68.8363 35.8668 1210.7629 347351.8  304.1791
## 4 54.0377 495.1816  16.4167 35.6036 1346.7582 249402.3  135.7432
## 5  0.0263 414.3925  51.1371 33.0698 1387.9419 296129.2  121.5044
## 6  7.1653 455.4931  16.8076 33.9870 1506.5585 193380.9   96.9908
## 7  0.5308 423.7198  20.2960 32.9063 1560.0089 184311.4   93.2005&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# optimal number of clusters
best.result &amp;lt;- t(res$Best.nc)
best.result&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Number_clusters Value_Index
## KL                       4     54.0377
## CH                       4    495.1816
## Hartigan                 3    171.9115
## CCC                      3     35.8668
## Scott                    3    276.8545
## Marriot                  3 532302.6940
## TrCovW                   3   6564.3610
## TraceW                   3    117.0760
## Friedman                 4    151.3607
## Rubin                    6    -33.9014
## Cindex                   3      0.3163
## DB                       3      0.7025
## Silhouette               2      0.5160
## Duda                     4      0.5932
## PseudoT2                 4     32.9134
## Beale                    3      1.8840
## Ratkowsky                3      0.4922
## Ball                     3     87.7349
## PtBiserial               3      0.7203
## Gap                      3      0.1343
## Frey                     1          NA
## McClain                  2      0.4228
## Gamma                    4      0.9261
## Gplus                   10     52.7862
## Tau                      3   2649.8405
## Dunn                    10      0.1543
## Hubert                   0      0.0000
## SDindex                  3      1.6226
## Dindex                   0      0.0000
## SDbw                    10      0.0303&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# density plot of all best results
library(magrittr)
best.result %&amp;gt;%
  .[,1] %&amp;gt;%
  na.omit() %&amp;gt;%
  as.vector() %&amp;gt;%
  density(.) %&amp;gt;% 
  plot(., xlab=&amp;#39;k, klusters&amp;#39;, main=&amp;#39;Optimal number of clusters&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/determin_number_clusters/determin_number_clusters_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Es we can see the most of methods predict k=3 for clustering &lt;code&gt;iris&lt;/code&gt; data.&lt;br /&gt;
If you try k-means method, you will get optimal between 2 and 3.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/&#34;&gt;Cluster Validataion Essentials by Alboukadel Kassambara&lt;/a&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://web.stanford.edu/~hastie/Papers/gap.pdf&#34;&gt;Tibshirani R at al. Estimating the number of clusters in a data set via the gap statistic. J.R.Statist.Soc.B, 2001&lt;/a&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;[NbClust: An R Package for Determining the Relevant Number of Clusters in a Data] Set(&lt;a href=&#34;https://cran.r-project.org/web/packages/NbClust/NbClust.pdf&#34; class=&#34;uri&#34;&gt;https://cran.r-project.org/web/packages/NbClust/NbClust.pdf&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Differential expression analysis</title>
      <link>/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis/</link>
      <pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#gene-expression-analysis-of-histone-deacetylase-1-hdac1-knockout-mouse.&#34;&gt;Gene expression analysis of histone deacetylase 1 (HDAC1) knockout mouse.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sources&#34;&gt;Sources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;gene-expression-analysis-of-histone-deacetylase-1-hdac1-knockout-mouse.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gene expression analysis of histone deacetylase 1 (HDAC1) knockout mouse.&lt;/h2&gt;
&lt;p&gt;This short tutorial should help to understand the basic principal of gene expression analysis using simple dataset and nearly basic R.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Affymetrix microarray&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Dataset&lt;/em&gt;: &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE5583&#34;&gt;GSE5583&lt;/a&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Paper&lt;/em&gt;: Mol Cell Biol 2006 Nov;26(21):7913-28. &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pubmed/16940178&#34;&gt;PMID: 16940178&lt;/a&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;R code&lt;/em&gt;: Ahmed Moustafa&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Read the data into R
library (RCurl)
url = getURL (&amp;quot;http://bit.ly/GSE5583_data&amp;quot;, followlocation = TRUE)
data = as.matrix(read.table (text = url, row.names = 1, header = T))

# Check the loaded dataset
dim(data) # Dimension of the dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12488     6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data shows gene experssion levels in 6 samples:
# rows correspond to samples (3 wild type WT and 3 knock-out KO)
# columns correspond to genes ids
head(data) # First few rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           WT.GSM130365 WT.GSM130366 WT.GSM130367 KO.GSM130368 KO.GSM130369
## 100001_at         11.5          5.6         69.1         15.7         36.0
## 100002_at         20.5         32.4         93.3         31.8         14.4
## 100003_at         72.4         89.0         79.2         80.5        130.1
## 100004_at        261.0        226.2        365.1        432.0        447.3
## 100005_at       1086.2       1555.6       1487.1       1062.2       1365.9
## 100006_at         49.7         52.9         15.0         25.8         48.8
##           KO.GSM130370
## 100001_at         42.0
## 100002_at         22.9
## 100003_at         86.7
## 100004_at        288.1
## 100005_at       1436.2
## 100006_at         54.8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;###################
# Exploratory plots
###################

# Check the behavior of the data
hist(data, col = &amp;quot;gray&amp;quot;, main=&amp;quot;GSE5583 - Histogram&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Log2 transformation (why?)
data2 = log2(data)

# Check the behavior of the data after log-transformation
hist(data2, col = &amp;quot;gray&amp;quot;, main=&amp;quot;GSE5583 (log2) - Histogram&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Boxplot
boxplot(data2, col=c(&amp;quot;darkgreen&amp;quot;, &amp;quot;darkgreen&amp;quot;, &amp;quot;darkgreen&amp;quot;,
                     &amp;quot;darkred&amp;quot;, &amp;quot;darkred&amp;quot;, &amp;quot;darkred&amp;quot;),
        main=&amp;quot;GSE5583 - boxplots&amp;quot;, las=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Hierarchical clustering of the &amp;quot;samples&amp;quot; based on
# the correlation coefficients of the expression values
hc = hclust(as.dist(1-cor(data2)))
plot(hc, main=&amp;quot;GSE5583 - Hierarchical Clustering&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#######################################
# Differential expression (DE) analysis
#######################################

# Separate the two conditions into two smaller data frames
wt = data2[,1:3]
ko = data2[,4:6]

# Compute the means of the samples of each condition
wt.mean = apply(wt, 1, mean)
ko.mean = apply(ko, 1, mean)

head(wt.mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 100001_at 100002_at 100003_at 100004_at 100005_at 100006_at 
##  4.039868  5.306426  6.320360  8.120503 10.408872  5.089087&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(ko.mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 100001_at 100002_at 100003_at 100004_at 100005_at 100006_at 
##  4.844978  4.452076  6.597451  8.576804 10.318839  5.358071&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Just get the maximum of all the means
limit = max(wt.mean, ko.mean)

# Scatter plot
plot(ko.mean ~ wt.mean, xlab = &amp;quot;WT&amp;quot;, ylab = &amp;quot;KO&amp;quot;,
     main = &amp;quot;GSE5583 - Scatter&amp;quot;, xlim = c(0, limit), ylim = c(0, limit))
# Diagonal line
abline(0, 1, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute fold-change (biological significance)
# Difference between the means of the conditions
fold = wt.mean - ko.mean

# Histogram of the fold differences
hist(fold, col = &amp;quot;gray&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute statistical significance (using t-test)
pvalue = NULL # Empty list for the p-values
tstat = NULL # Empty list of the t test statistics

for(i in 1 : nrow(data)) { # For each gene : 
  x = wt[i,] # WT of gene number i
  y = ko[i,] # KO of gene number i
  
  # Compute t-test between the two conditions
  t = t.test(x, y)
  
  # Put the current p-value in the pvalues list
  pvalue[i] = t$p.value
  # Put the current t-statistic in the tstats list
  tstat[i] = t$statistic
}

head(pvalue)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5449730 0.3253745 0.3287830 0.1892376 0.6928410 0.7180077&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Histogram of p-values (-log10)
hist(-log10(pvalue), col = &amp;quot;gray&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-7.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Volcano: put the biological significance (fold-change)
# and statistical significance (p-value) in one plot
plot(fold, -log10(pvalue), main = &amp;quot;GSE5583 - Volcano&amp;quot;)

fold_cutoff = 2
pvalue_cutoff = 0.01
abline(v = fold_cutoff, col = &amp;quot;blue&amp;quot;, lwd = 3)
abline(v = -fold_cutoff, col = &amp;quot;red&amp;quot;, lwd = 3)
abline(h = -log10(pvalue_cutoff), col = &amp;quot;green&amp;quot;, lwd = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-8.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Screen for the genes that satisfy the filtering criteria

# Fold-change filter for &amp;quot;biological&amp;quot; significance
filter_by_fold = abs(fold) &amp;gt;= fold_cutoff
dim(data2[filter_by_fold, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 210   6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# P-value filter for &amp;quot;statistical&amp;quot; significance
filter_by_pvalue = pvalue &amp;lt;= pvalue_cutoff
dim(data2[filter_by_pvalue, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 429   6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Combined filter (both biological and statistical)
filter_combined = filter_by_fold &amp;amp; filter_by_pvalue

filtered = data2[filter_combined,]
dim(filtered)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 42  6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(filtered)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             WT.GSM130365 WT.GSM130366 WT.GSM130367 KO.GSM130368
## 100716_at       4.852998     4.906891     5.626439     7.572890
## 100914_at      10.340852     9.917074    10.250062    12.248787
## 101368_at       9.937227    10.204693    10.385215    12.270354
## 101550_at       5.526695     5.439623     6.221104     2.137504
## 101635_f_at     7.105385     6.722466     6.943687     5.266787
## 101883_s_at     5.768184     6.127221     5.133399    11.564292
##             KO.GSM130369 KO.GSM130370
## 100716_at       7.791163     7.299208
## 100914_at      12.185526    12.127124
## 101368_at      12.213499    12.078184
## 101550_at       2.906891     2.035624
## 101635_f_at     4.842979     4.643856
## 101883_s_at    11.679568    11.663514&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Let&amp;#39;s generate the volcano plot again,
# highlighting the significantly differential expressed genes
plot(fold, -log10(pvalue), main = &amp;quot;GSE5583 - Volcano #2&amp;quot;)
points (fold[filter_combined], -log10(pvalue[filter_combined]),
        pch = 16, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-9.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Highlighting up-regulated in red and down-regulated in blue
plot(fold, -log10(pvalue), main = &amp;quot;GSE5583 - Volcano #3&amp;quot;)
points (fold[filter_combined &amp;amp; fold &amp;lt; 0],
        -log10(pvalue[filter_combined &amp;amp; fold &amp;lt; 0]),
        pch = 16, col = &amp;quot;red&amp;quot;)
points (fold[filter_combined &amp;amp; fold &amp;gt; 0],
        -log10(pvalue[filter_combined &amp;amp; fold &amp;gt; 0]),
        pch = 16, col = &amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-10.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Cluster the rows (genes) &amp;amp; columns (samples) by correlation
rowv = as.dendrogram(hclust(as.dist(1-cor(t(filtered)))))
colv = as.dendrogram(hclust(as.dist(1-cor(filtered))))

# Generate a heatmap
heatmap(filtered, Rowv=rowv, Colv=colv, cexCol=0.7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-11.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gplots)

# Enhanced heatmap
heatmap.2(filtered, Rowv=rowv, Colv=colv, cexCol=0.7,
          col = rev(redblue(256)), scale = &amp;quot;row&amp;quot;,
          trace=&amp;quot;none&amp;quot;, density.info=&amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Save the heatmap to a PDF file
pdf (&amp;quot;GSE5583_DE_Heatmap.pdf&amp;quot;)
heatmap.2(filtered, Rowv=rowv, Colv=colv, cexCol=0.7,
          col = rev(redblue(256)), scale = &amp;quot;row&amp;quot;)
dev.off()

# Save the DE genes to a text file
write.table (filtered, &amp;quot;GSE5583_DE.txt&amp;quot;, sep = &amp;quot;\t&amp;quot;,
             quote = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n = nrow(filtered)

cor.table = NULL
x = NULL
y = NULL
cor.val = NULL
cor.sig = NULL

for (i in 1 : (n-1)) {
  x_name = rownames(filtered)[i]
  x_exps = filtered[i, ]    
  
  for (j in (i+1) : n) {
    y_name = rownames(filtered)[j]
    y_exps = filtered[j, ]
    
    output = cor.test(x_exps,y_exps)
    
    x = c(x, x_name)
    y = c(y, y_name)
    cor.val = c(cor.val, output$estimate)
    cor.sig = c(cor.sig, output$p.value)
  }
}

cor.table = data.frame (x, y, cor.val, cor.sig)

dim(cor.table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 861   4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(cor.table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           x           y    cor.val      cor.sig
## 1 100716_at   100914_at  0.9732295 0.0010653980
## 2 100716_at   101368_at  0.9897688 0.0001564799
## 3 100716_at   101550_at -0.9060431 0.0128271221
## 4 100716_at 101635_f_at -0.9433403 0.0047245418
## 5 100716_at 101883_s_at  0.9508680 0.0035616301
## 6 100716_at   102712_at  0.9676037 0.0015572795&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sig_cutoff = 0.001

cor.filtered = subset (cor.table, cor.sig &amp;lt; sig_cutoff)

dim(cor.filtered)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 314   4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(cor.filtered)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            x         y    cor.val      cor.sig
## 2  100716_at 101368_at  0.9897688 1.564799e-04
## 8  100716_at 103088_at -0.9761495 8.464861e-04
## 10 100716_at 103299_at -0.9991089 1.190632e-06
## 14 100716_at 104700_at -0.9792543 6.411095e-04
## 15 100716_at 160172_at  0.9833552 4.132702e-04
## 16 100716_at 160943_at  0.9814703 5.118449e-04&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sources&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://gist.github.com/ahmedmoustafa&#34;&gt;Ahmed Moustafa githab&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>k-means</title>
      <link>/post/statistics/k_means/k_means/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/k_means/k_means/</guid>
      <description>


&lt;div id=&#34;k-means-cluster-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;K-means cluster analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(datasets)
df &amp;lt;- datasets::iris
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 3 species:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique(df$Species)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] setosa     versicolor virginica 
## Levels: setosa versicolor virginica&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We try to predict them by &lt;em&gt;Petal.Lenght&lt;/em&gt;and &lt;em&gt;Petal.Width&lt;/em&gt; variables using k-means clustering.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot Petal.Length ~ Petal.Width data
plot(df$Petal.Length ~ df$Petal.Width)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/k_means/k_means_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Find number of clusters using wss
wss &amp;lt;- (nrow(df[, 3:4])-1)*sum(apply(df[, 3:4],2,var))
for (i in 2:15) wss[i] &amp;lt;- sum(kmeans(df[, 3:4], i)$withinss)
plot(1:15, wss, type=&amp;quot;b&amp;quot;, xlab=&amp;quot;Number of Clusters&amp;quot;, ylab=&amp;quot;Within groups sum of squares&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/k_means/k_means_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt; More than 3 clusters give no obvious advantages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Make k-means with 3 clasters
ncl &amp;lt;- 3
cl &amp;lt;- stats::kmeans(df[, 3:4], ncl, nstart = 20)
cl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## K-means clustering with 3 clusters of sizes 48, 50, 52
## 
## Cluster means:
##   Petal.Length Petal.Width
## 1     5.595833    2.037500
## 2     1.462000    0.246000
## 3     4.269231    1.342308
## 
## Clustering vector:
##   [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
##  [36] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [71] 3 3 3 3 3 3 3 1 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1
## [106] 1 3 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 3 1
## [141] 1 1 1 1 1 1 1 1 1 1
## 
## Within cluster sum of squares by cluster:
## [1] 16.29167  2.02200 13.05769
##  (between_SS / total_SS =  94.3 %)
## 
## Available components:
## 
## [1] &amp;quot;cluster&amp;quot;      &amp;quot;centers&amp;quot;      &amp;quot;totss&amp;quot;        &amp;quot;withinss&amp;quot;    
## [5] &amp;quot;tot.withinss&amp;quot; &amp;quot;betweenss&amp;quot;    &amp;quot;size&amp;quot;         &amp;quot;iter&amp;quot;        
## [9] &amp;quot;ifault&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compair result of clustering with real data (3 species of iris are in analysis)
table(cl$cluster, df$Species)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    
##     setosa versicolor virginica
##   1      0          2        46
##   2     50          0         0
##   3      0         48         4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot data
clusters &amp;lt;- split.data.frame(df, cl$cluster)
xlim &amp;lt;- c(min(df$Petal.Width), max(df$Petal.Width))
ylim &amp;lt;- c(min(df$Petal.Length), max(df$Petal.Length))
col &amp;lt;- c(&amp;#39;red&amp;#39;, &amp;#39;green&amp;#39;, &amp;#39;blue&amp;#39;)
plot(0, xlab=&amp;#39;Petal width&amp;#39;, ylab=&amp;#39;Petal length&amp;#39;, xlim=xlim, ylim=ylim)
for ( i in 1:ncl ) {
  points(clusters[[i]]$Petal.Length ~ clusters[[i]]$Petal.Width, col=col[i], xlim=xlim, ylim=ylim)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/k_means/k_means_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hierarchical cluster analysis</title>
      <link>/post/statistics/hierarchical_cluster/hierarchical_cluser/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/hierarchical_cluster/hierarchical_cluser/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#hierarchical-cluster-analysis&#34;&gt;Hierarchical cluster analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#choosing-optimal-number-of-clusters&#34;&gt;Choosing optimal number of clusters&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#elbow-method&#34;&gt;Elbow method&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#choosing-the-best-clustering-model&#34;&gt;Choosing the best clustering model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hierarchical-cluster-analysis-1&#34;&gt;Hierarchical cluster analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bibliography&#34;&gt;Bibliography&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;hierarchical-cluster-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical cluster analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare data for analysis &lt;code&gt;datasets::USArrests&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(datasets)
df &amp;lt;- datasets::USArrests
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Murder Assault UrbanPop Rape
## Alabama      13.2     236       58 21.2
## Alaska       10.0     263       48 44.5
## Arizona       8.1     294       80 31.0
## Arkansas      8.8     190       50 19.5
## California    9.0     276       91 40.6
## Colorado      7.9     204       78 38.7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# check if &amp;#39;NA&amp;#39; values present in data
any(is.na(df))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# remove &amp;#39;NA&amp;#39; if necessary
df &amp;lt;- na.omit(df)
# normalize
df &amp;lt;- scale(df)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-optimal-number-of-clusters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing optimal number of clusters&lt;/h2&gt;
&lt;div id=&#34;elbow-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Elbow method&lt;/h3&gt;
&lt;p&gt;plot &lt;strong&gt;k&lt;/strong&gt; ~ &lt;strong&gt;wss&lt;/strong&gt;, where &lt;strong&gt;k&lt;/strong&gt; - is a cluseter number and &lt;strong&gt;wss&lt;/strong&gt; is a total within-cluster sum of square.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wss &amp;lt;- (nrow(df)-1)*sum(apply(df,2,var))
for (i in 2:15) wss[i] &amp;lt;- sum(kmeans(df, centers=i)$withinss)
plot(1:15, wss, type=&amp;quot;b&amp;quot;,
     xlab=&amp;quot;Number of Clusters&amp;quot;,
     ylab=&amp;quot;Within groups sum of squares&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/hierarchical_cluster/hierarchical_cluser_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt; This diagram shows that 4 number of clusers is optimal for this dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-the-best-clustering-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing the best clustering model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc2 &amp;lt;- cluster::agnes(df, method=&amp;#39;complete&amp;#39;)
hc2$ac&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8531583&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This metric allows to estimate the quality of cluster. Closer to 1 is better.&lt;br /&gt;
Using this metric we can try several models and choose the best one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- c( &amp;quot;average&amp;quot;, &amp;quot;single&amp;quot;, &amp;quot;complete&amp;quot;, &amp;quot;ward&amp;quot;)
names(m) &amp;lt;- c( &amp;quot;average&amp;quot;, &amp;quot;single&amp;quot;, &amp;quot;complete&amp;quot;, &amp;quot;ward&amp;quot;)
# function to compute coefficient
ac &amp;lt;- function(x) { cluster::agnes(df, method = x)$ac }

purrr::map_dbl(m, ac)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   average    single  complete      ward 
## 0.7379371 0.6276128 0.8531583 0.9346210&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see the &lt;strong&gt;ward method&lt;/strong&gt; gives the best clustering.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-cluster-analysis-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical cluster analysis&lt;/h2&gt;
&lt;p&gt;Let’s first split data into 4 groups using &lt;code&gt;clust::hclust&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calculate distances
d &amp;lt;- dist(df, method=&amp;#39;euclidean&amp;#39;)

# hierarchical cluster analysis
# &amp;#39;ward.D2&amp;#39; method is equivalent of agnes &amp;#39;ward&amp;#39;
hc1 &amp;lt;- hclust(d, method=&amp;#39;ward.D2&amp;#39;)

# Plot the obtained dendrogram
plot(hc1, hang = -1, cex = 0.6)

# show 4 clusers
rect.hclust(hc1, k=4, border=&amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/hierarchical_cluster/hierarchical_cluser_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# group data by clusters
groups &amp;lt;- cutree(hc1, k=3)
names(groups[groups == 1])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Alabama&amp;quot;        &amp;quot;Alaska&amp;quot;         &amp;quot;Arizona&amp;quot;        &amp;quot;California&amp;quot;    
##  [5] &amp;quot;Colorado&amp;quot;       &amp;quot;Florida&amp;quot;        &amp;quot;Georgia&amp;quot;        &amp;quot;Illinois&amp;quot;      
##  [9] &amp;quot;Louisiana&amp;quot;      &amp;quot;Maryland&amp;quot;       &amp;quot;Michigan&amp;quot;       &amp;quot;Mississippi&amp;quot;   
## [13] &amp;quot;Nevada&amp;quot;         &amp;quot;New Mexico&amp;quot;     &amp;quot;New York&amp;quot;       &amp;quot;North Carolina&amp;quot;
## [17] &amp;quot;South Carolina&amp;quot; &amp;quot;Tennessee&amp;quot;      &amp;quot;Texas&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# check for cluster metrics
names(hc1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;merge&amp;quot;       &amp;quot;height&amp;quot;      &amp;quot;order&amp;quot;       &amp;quot;labels&amp;quot;      &amp;quot;method&amp;quot;     
## [6] &amp;quot;call&amp;quot;        &amp;quot;dist.method&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can split data into 4 groups using &lt;code&gt;cluster::agnes&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# using &amp;#39;agnes&amp;#39; for hierarhical clustering
hc3 &amp;lt;- cluster::agnes(df, method=&amp;#39;ward&amp;#39;)
# plot slaster
cluster::pltree(hc3, hang = -1, cex = 0.6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/hierarchical_cluster/hierarchical_cluser_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# split into groups
groups &amp;lt;- cutree(as.hclust(hc3), k = 4)
groups[groups==1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        Alabama        Georgia      Louisiana    Mississippi North Carolina 
##              1              1              1              1              1 
## South Carolina      Tennessee 
##              1              1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://uc-r.github.io/hc_clustering&#34;&gt;UC Business Analytics R Programming Guide&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
