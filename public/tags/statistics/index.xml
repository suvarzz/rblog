<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics | Mark Goldberg</title>
    <link>https://suvar.netlify.com/tags/statistics/</link>
      <atom:link href="https://suvar.netlify.com/tags/statistics/index.xml" rel="self" type="application/rss+xml" />
    <description>Statistics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019</copyright><lastBuildDate>Wed, 14 Aug 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://suvar.netlify.com/img/logo.png</url>
      <title>Statistics</title>
      <link>https://suvar.netlify.com/tags/statistics/</link>
    </image>
    
    <item>
      <title>Statistical learning - topics</title>
      <link>https://suvar.netlify.com/post/statistics/statistical_learning_topics/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/statistical_learning_topics/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#statistics&#34;&gt;Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variation-analysis&#34;&gt;Variation analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#unsupervised-learning&#34;&gt;Unsupervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#supervised-regression&#34;&gt;Supervised Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#supervised-classification&#34;&gt;Supervised Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimation-of-model-parameters&#34;&gt;Estimation of model parameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#time-series&#34;&gt;Time Series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deep-learning&#34;&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;statistics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Statistics&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Correlation&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://suvar.netlify.com/post/statistics/hypothesis_testing/hypothesis_testing/&#34;&gt;Hypothesis testing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;variation-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Variation analysis&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;unsupervised-learning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Unsupervised Learning&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://suvar.netlify.com/post/statistics/hierarchical_cluster/hierarchical_cluser/&#34;&gt;Hierarchical Cluster Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://suvar.netlify.com/post/statistics/k_means/k_means/&#34;&gt;K-means Cluster Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Principal Component Analysis&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://suvar.netlify.com/post/statistics/umap/umap/&#34;&gt;Uniform Manifold Approximation and Projection (UMAP)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://suvar.netlify.com/post/statistics/tsne/tsne/&#34;&gt;t-Distributed Stochastic Neighbor Embedding (tSNE)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;supervised-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Supervised Regression&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Linear Regression&lt;/li&gt;
&lt;li&gt;Multiple regression&lt;/li&gt;
&lt;li&gt;Linear Model Selection&lt;/li&gt;
&lt;li&gt;Polynomial Regression&lt;/li&gt;
&lt;li&gt;Stepwise Regression&lt;/li&gt;
&lt;li&gt;Regularized Regression&lt;/li&gt;
&lt;li&gt;Regression Trees &amp;amp; Bagging&lt;/li&gt;
&lt;li&gt;Random Forests&lt;/li&gt;
&lt;li&gt;Imprecise Regression&lt;/li&gt;
&lt;li&gt;Lasso regression&lt;/li&gt;
&lt;li&gt;Ridge regression&lt;/li&gt;
&lt;li&gt;ElasticNet regression&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;supervised-classification&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Supervised Classification&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://suvar.netlify.com/post/statistics/naive_bayes/naive_bayes&#34;&gt;Naïve Bayes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://suvar.netlify.com/post/statistics/logistic_regression/logistic_regression&#34;&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Multinomial logistic regression&lt;/li&gt;
&lt;li&gt;Ordinal logistic regression&lt;/li&gt;
&lt;li&gt;Linear &amp;amp; Quadratic Discriminant Analysis&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://suvar.netlify.com/post/statistics/svm/svm&#34;&gt;Support Vector Machines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Random Forests and Boosting&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation-of-model-parameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimation of model parameters&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://suvar.netlify.com/post/statistics/model_evaluation_metrics/model_evaluation_metrics&#34;&gt;Model evaluation metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://suvar.netlify.com/post/statistics/resampling_methods/resampling_methods&#34;&gt;Resampling Methods&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;time-series&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Time Series&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Exploring &amp;amp; Visualizing Times Series&lt;/li&gt;
&lt;li&gt;Benchmark Methods &amp;amp; Forecast Accuracy&lt;/li&gt;
&lt;li&gt;Moving Averages&lt;/li&gt;
&lt;li&gt;Exponential Smoothing&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;deep-learning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Deep Learning&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Neural Network for Regression&lt;/li&gt;
&lt;li&gt;Neural Network for Classification&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://suvar.netlify.com/post/statistics/lvq/lvq&#34;&gt;Learning Vector Quantization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Feedforward Deep Learning with Keras &amp;amp; Tensorflow&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://suvar.netlify.com/post/statistics/hopfield_network/hopfield_network&#34;&gt;Hopfield Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>tSNE</title>
      <link>https://suvar.netlify.com/post/statistics/tsne/tsne/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/tsne/tsne/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tsne&#34;&gt;tSNE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#barnes-hut-t-distributed-stochastic-neighbor-embedding&#34;&gt;Barnes-Hut t-Distributed Stochastic Neighbor Embedding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compare-with-principal-component-analysis-pca&#34;&gt;Compare with Principal Component Analysis (PCA)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bibliography&#34;&gt;Bibliography&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;tsne&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tSNE&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tsne)
# tSNE
set.seed(5)
tsne_iris = tsne(iris[,1:4], k=2, perplexity=30, max_iter=1000)

# set colors
colors = rainbow(length(unique(iris$Species)))
names(colors) = unique(iris$Species)

# plot tSNE
plot(tsne_iris, t=&amp;#39;n&amp;#39;)
points(tsne_iris, col=colors[iris$Species])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/tsne/tsne_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;barnes-hut-t-distributed-stochastic-neighbor-embedding&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Barnes-Hut t-Distributed Stochastic Neighbor Embedding&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Rtsne&lt;/code&gt; package. It is faster than &lt;code&gt;tsne&lt;/code&gt; and better separate elements in groups.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rtsne)
# remove duplicates
iris_unique &amp;lt;- unique(iris)
mx &amp;lt;- as.matrix(iris_unique[,1:4])
normalize_input(mx)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(3)
rtsne &amp;lt;- Rtsne(mx, dims=2, perplexity=30, theta=0.5)
names(rtsne)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;N&amp;quot;                   &amp;quot;Y&amp;quot;                   &amp;quot;costs&amp;quot;              
##  [4] &amp;quot;itercosts&amp;quot;           &amp;quot;origD&amp;quot;               &amp;quot;perplexity&amp;quot;         
##  [7] &amp;quot;theta&amp;quot;               &amp;quot;max_iter&amp;quot;            &amp;quot;stop_lying_iter&amp;quot;    
## [10] &amp;quot;mom_switch_iter&amp;quot;     &amp;quot;momentum&amp;quot;            &amp;quot;final_momentum&amp;quot;     
## [13] &amp;quot;eta&amp;quot;                 &amp;quot;exaggeration_factor&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot tsne
plot(rtsne[[&amp;#39;Y&amp;#39;]], t=&amp;#39;n&amp;#39;)
points(rtsne[[&amp;#39;Y&amp;#39;]], col=colors[iris_unique$Species])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/tsne/tsne_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-with-principal-component-analysis-pca&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compare with Principal Component Analysis (PCA)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# compare to PCA
pca_iris = princomp(iris[,1:4])$scores[,1:2]

# plot PCA
plot(pca_iris, t=&amp;#39;n&amp;#39;)
points(pca_iris, col=colors[iris$Species])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/tsne/tsne_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression (Math)</title>
      <link>https://suvar.netlify.com/post/statistics/linear_regression_math/linear_regression_math/</link>
      <pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/linear_regression_math/linear_regression_math/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#simple-linear-regression&#34;&gt;Simple linear regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-find-coefficients-0-and-1&#34;&gt;How to find coefficients β0 and β1?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix-form-for-multiple-regression&#34;&gt;Matrix form for multiple regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;simple-linear-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simple linear regression&lt;/h2&gt;
&lt;p&gt;Ordinary Least Squares (OLS) function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(minimize\left\{SSE = sum_{i=1}^{n} (y_i - \hat y_i)^2 \right\}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Assumtions of OLS regression:&lt;br /&gt;
* Linear relationship&lt;br /&gt;
* Multivariathe normality&lt;br /&gt;
* No autocorrelation&lt;br /&gt;
* Homoscedastic (constant variance in residuals)&lt;br /&gt;
* There are more observations (n) than features (p) (n &amp;gt; p)&lt;br /&gt;
* No or little multicollinearity&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat y = \beta_0 + \beta_1 x\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; - expectd value of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, the same as &lt;span class=&#34;math inline&#34;&gt;\(E(Y|x)\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; - intercept&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; - slope&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat \beta_0 = \bar y - \hat\beta_1 \bar x\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_1 = \frac{\displaystyle\sum_{i=1}^{n} (x_i - \bar x)(y_i-\bar y)}{\displaystyle\sum_{i=1}^{n} (x_i - \bar x)^2}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-find-coefficients-0-and-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to find coefficients β0 and β1?&lt;/h2&gt;
&lt;p&gt;There are several ways to extimate coefficients of linear regression. Here we discuss the least squares approach. Other aproaches include &lt;strong&gt;maximul likelihood estimation&lt;/strong&gt;.&lt;br /&gt;
To estimate &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; we should find &lt;strong&gt;minimum&lt;/strong&gt; of &lt;strong&gt;sum of squared residuals&lt;/strong&gt; (&lt;span class=&#34;math inline&#34;&gt;\(SSR\)&lt;/span&gt;)&lt;br /&gt;
To find this minimum we should calculate derivatives of &lt;span class=&#34;math inline&#34;&gt;\(SSR\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and set them to 0:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle\min_{\beta_0,\beta1} : SSR \implies \frac{\partial SSR}{\partial \beta_0} = \frac{\partial SSR}{\partial \beta_1} = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here is the solution:&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(SSR = \displaystyle\sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1x_i))^2 = \displaystyle\sum_{i=1}^{n} (y_i^2 - 2y_i \beta_0 - 2y_i \beta_1 x_i + \beta_0^2 + 2\beta_0\beta_1x_i + \beta_1^2x_i^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial SSR}{\partial\beta_0} = \displaystyle\sum_{i=1}^{n}(-2y_i + 2\beta_0 + 2\beta_1x_i)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle\sum_{i=1}^{n} (-y_i + \hat\beta_0 + \hat\beta_1 x_i) = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar y = \frac{1}{n} \displaystyle\sum_{i=1}^{n} y_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar x = \frac{1}{n} \displaystyle\sum_{i=1}^{n} x_i \implies -n\bar y + n \hat\beta_0 + \hat\beta_1 n \hat x = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat \beta_0 = \bar y - \hat\beta_1 \bar x\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial SSR}{\partial \beta_1} = \displaystyle\sum_{i=1}^{n} (-2x_i y_i + 2\beta_0 x_i + 2\beta_1 x_i^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(-\displaystyle\sum_{i=1}^{n} x_i y_i + \hat\beta_0 \displaystyle\sum_{i=1}^{n} x_i + \hat\beta_1\displaystyle\sum_{i=1}^{n} x_i^2 = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(-\displaystyle\sum_{i=1}^{n} x_i y_i + (\bar y - \hat\beta_1 \bar x) \displaystyle\sum_{i=1}^{n} x_i + \hat\beta_1\displaystyle\sum_{i=1}^{n} x_i^2 = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_1 = \frac{\displaystyle\sum_{i=1}^{n} x_i(y_i-\bar y)}{\displaystyle\sum_{i=1}^{n} x_i (x_i - \bar x)} = \frac{\displaystyle\sum_{i=1}^{n} (x_i - \bar x)(y_i-\bar y)}{\displaystyle\sum_{i=1}^{n} (x_i - \bar x)^2} = \frac{Cov(x,y)}{Var(x)} = r_{xy} \frac{s_y}{s_x}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; - estimated &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt; - averages of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(r_{xy}\)&lt;/span&gt; - sample correlation coefficient between &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(s_x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s_y\)&lt;/span&gt; - uncorrected sample standard deviations of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(Var\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Cov\)&lt;/span&gt; - sample variance and sample covariance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-form-for-multiple-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Matrix form for multiple regression&lt;/h2&gt;
&lt;p&gt;We can write &lt;span class=&#34;math inline&#34;&gt;\(\hat y = \beta_0 + \beta_1 x\)&lt;/span&gt; in a matrix form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y = \begin{bmatrix}y_1\\y_2\\\vdots\\y_n\end{bmatrix}\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(b = \begin{bmatrix}\beta_1\\\beta_2\\\vdots\\\beta_n\end{bmatrix}\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(X = \begin{bmatrix}1 &amp;amp; x_{1,1} &amp;amp; x_{1,2} \dots &amp;amp; x_{1,k}\\1 &amp;amp; x_{2,1} &amp;amp; x_{2,2} \dots &amp;amp; x_{2,k}\\\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\\1 &amp;amp; x_{n,1} &amp;amp; x_{n,2} \dots &amp;amp; x_{n,k}\end{bmatrix}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y = Xb\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X&amp;#39;Y = X&amp;#39;Xb\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\((X&amp;#39;X)^{-1}X&amp;#39;Xb = (X&amp;#39;X)^{-1}X&amp;#39;Y\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(b = (X&amp;#39;X)^{-1}X&amp;#39;Y\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\((X&amp;#39;X)^{-1}X&amp;#39;X = I\)&lt;/span&gt; - identity matrix&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Principal component analysis</title>
      <link>https://suvar.netlify.com/post/statistics/pca/pca/</link>
      <pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/pca/pca/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#principal-component-analysis-of-iris-dataset&#34;&gt;Principal component analysis of iris dataset&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#split-data&#34;&gt;Split data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#principal-component-analysis&#34;&gt;Principal component analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#prediction&#34;&gt;Prediction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-calculate-coordinates-for-test-data&#34;&gt;How to calculate coordinates for test data?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sources&#34;&gt;Sources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;principal-component-analysis-of-iris-dataset&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Principal component analysis of iris dataset&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(factoextra)
df &amp;lt;- iris
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels &amp;quot;setosa&amp;quot;,&amp;quot;versicolor&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)
trainIndex &amp;lt;- caret::createDataPartition(df[,5], p=.8, list = FALSE, times = 1)
# split data
train &amp;lt;- df[trainIndex, ]
test &amp;lt;- df[-trainIndex, ]

# PCA
pca &amp;lt;- prcomp(train[,-5], scale=TRUE)

# eigenvalues on &amp;#39;scree plot&amp;#39;
# percentage of variances explained by each principal component.
factoextra::fviz_eig(pca, addlabels = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/pca/pca_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Graph of individuals
factoextra::fviz_pca_ind(pca, geom = &amp;#39;point&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/pca/pca_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;groups &amp;lt;- as.factor(train[,5])
fviz_pca_ind(pca,
             col.ind = groups, # color by groups
             palette = c(&amp;#39;red&amp;#39;, &amp;#39;green&amp;#39;, &amp;#39;blue&amp;#39;),
             addEllipses = TRUE, # Concentration ellipses
             ellipse.type = &amp;quot;confidence&amp;quot;,
             legend.title = &amp;quot;Groups&amp;quot;,
             repel = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/pca/pca_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Graph of variables
fviz_pca_var(pca,
             col.var = &amp;#39;contrib&amp;#39;,    # color by contributions to the PC 
             gradient.cols = c(&amp;#39;#00AFBB&amp;#39;, &amp;#39;#E7B800&amp;#39;, &amp;#39;#FC4E07&amp;#39;),
             repel = T)          # avoid text overlapping&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/pca/pca_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Graph of individuals and variables
fviz_pca_biplot(pca, repel = TRUE,
                geom = &amp;#39;point&amp;#39;,
                col.var = &amp;#39;red&amp;#39;, # Variables color
                col.ind = &amp;#39;grey&amp;#39;  # Individuals color
                )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/pca/pca_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(factoextra)
library(mlbench) # dataset

data(PimaIndiansDiabetes)
df &amp;lt;- PimaIndiansDiabetes
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   pregnant glucose pressure triceps insulin mass pedigree age diabetes
## 1        6     148       72      35       0 33.6    0.627  50      pos
## 2        1      85       66      29       0 26.6    0.351  31      neg
## 3        8     183       64       0       0 23.3    0.672  32      pos
## 4        1      89       66      23      94 28.1    0.167  21      neg
## 5        0     137       40      35     168 43.1    2.288  33      pos
## 6        5     116       74       0       0 25.6    0.201  30      neg&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    768 obs. of  9 variables:
##  $ pregnant: num  6 1 8 1 0 5 3 10 2 8 ...
##  $ glucose : num  148 85 183 89 137 116 78 115 197 125 ...
##  $ pressure: num  72 66 64 66 40 74 50 0 70 96 ...
##  $ triceps : num  35 29 0 23 35 0 32 0 45 0 ...
##  $ insulin : num  0 0 0 94 168 0 88 0 543 0 ...
##  $ mass    : num  33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ...
##  $ pedigree: num  0.627 0.351 0.672 0.167 2.288 ...
##  $ age     : num  50 31 32 21 33 30 26 29 53 54 ...
##  $ diabetes: Factor w/ 2 levels &amp;quot;neg&amp;quot;,&amp;quot;pos&amp;quot;: 2 1 2 1 2 1 2 1 2 2 ...&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;split-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Split data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trainIndex &amp;lt;- sample(nrow(df), nrow(df)*0.8)
# split data
train &amp;lt;- df[trainIndex, -ncol(df)]
test &amp;lt;- df[-trainIndex, -ncol(df)]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;principal-component-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Principal component analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca &amp;lt;- prcomp(train, scale=T)

# eigenvalues on &amp;#39;scree plot&amp;#39;
# percentage of variances explained by each principal component.
fviz_eig(pca, addlabels = T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/pca/pca_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Graph of individuals
fviz_pca_ind(pca,
             geom = &amp;#39;point&amp;#39;,
             col.ind = &amp;quot;cos2&amp;quot;, # Color by the quality of representation
             gradient.cols = c(&amp;quot;#00AFBB&amp;quot;, &amp;quot;#E7B800&amp;quot;, &amp;quot;#FC4E07&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/pca/pca_files/figure-html/unnamed-chunk-5-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Graph of variables
fviz_pca_var(pca,
             col.var = &amp;#39;contrib&amp;#39;,    # color by contributions to the PC 
             gradient.cols = c(&amp;#39;#00AFBB&amp;#39;, &amp;#39;#E7B800&amp;#39;, &amp;#39;#FC4E07&amp;#39;),
             repel = T)          # avoid text overlapping&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/pca/pca_files/figure-html/unnamed-chunk-5-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Graph of individuals and variables
fviz_pca_biplot(pca, repel = TRUE,
                geom = &amp;#39;point&amp;#39;,
                col.var = &amp;#39;red&amp;#39;, # Variables color
                col.ind = &amp;#39;grey&amp;#39;  # Individuals color
                )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/pca/pca_files/figure-html/unnamed-chunk-5-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Eigenvalues
get_eigenvalue(pca)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       eigenvalue variance.percent cumulative.variance.percent
## Dim.1  2.1484632        26.855791                    26.85579
## Dim.2  1.7117918        21.397397                    48.25319
## Dim.3  1.0560307        13.200383                    61.45357
## Dim.4  0.8577135        10.721419                    72.17499
## Dim.5  0.7827308         9.784135                    81.95912
## Dim.6  0.6490472         8.113090                    90.07222
## Dim.7  0.4246541         5.308177                    95.38039
## Dim.8  0.3695686         4.619608                   100.00000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Results for Variables
pca.var &amp;lt;- get_pca_var(pca)
pca.var$coord          # Coordinates&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               Dim.1      Dim.2       Dim.3        Dim.4       Dim.5
## pregnant -0.2613362  0.7555770 -0.03411457  0.164119419  0.38787318
## glucose  -0.5550955  0.1915545  0.53183488 -0.415232060 -0.33421050
## pressure -0.5730576  0.1955417 -0.49970031  0.001917723 -0.21716148
## triceps  -0.6082808 -0.4930349 -0.28918361  0.106775060  0.36003451
## insulin  -0.5953969 -0.3664227  0.32796053 -0.298678115  0.43293986
## mass     -0.6682581 -0.1438707 -0.33060384 -0.014819512 -0.35047480
## pedigree -0.3747603 -0.2310328  0.46492858  0.741684330 -0.16588066
## age      -0.3634878  0.7839270  0.07533503  0.086210125  0.07758022
##                Dim.6       Dim.7       Dim.8
## pregnant  0.17425048 -0.36905378  0.12529249
## glucose   0.01137266 -0.20644065 -0.21322789
## pressure -0.57136625 -0.07659026  0.06465123
## triceps   0.09518077 -0.04786257 -0.38846870
## insulin  -0.14977159  0.13459416  0.29408539
## mass      0.50046583  0.06735177  0.21302317
## pedigree -0.09760457 -0.02509684  0.04744906
## age       0.02473193  0.46303034 -0.13856443&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca.var$contrib        # Contributions to the PCs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Dim.1     Dim.2      Dim.3        Dim.4      Dim.5
## pregnant  3.178860 33.350823  0.1102055  3.140347407 19.2206059
## glucose  14.341925  2.143551 26.7841028 20.102011066 14.2701239
## pressure 15.285112  2.233715 23.6451844  0.000428775  6.0249461
## triceps  17.221867 14.200527  7.9190087  1.329221633 16.5605910
## insulin  16.500048  7.843569 10.1851313 10.400747539 23.9465371
## mass     20.785505  1.209188 10.3499741  0.025605047 15.6928259
## pedigree  6.537012  3.118145 20.4689687 64.135127098  3.5154352
## age       6.149670 35.900481  0.5374244  0.866511436  0.7689349
##                Dim.6      Dim.7      Dim.8
## pregnant  4.67812337 32.0733237  4.2477111
## glucose   0.01992727 10.0358717 12.3024868
## pressure 50.29824867  1.3813755  1.1309892
## triceps   1.39579650  0.5394567 40.8335307
## insulin   3.45607012  4.2659628 23.4019348
## mass     38.58980271  1.0682250 12.2788742
## pedigree  1.46779037  0.1483210  0.6092002
## age       0.09424099 50.4874634  5.1952730&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca.var$cos2           # Quality of representation &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               Dim.1      Dim.2       Dim.3        Dim.4      Dim.5
## pregnant 0.06829663 0.57089665 0.001163804 2.693518e-02 0.15044560
## glucose  0.30813100 0.03669313 0.282848341 1.724177e-01 0.11169666
## pressure 0.32839502 0.03823655 0.249700400 3.677661e-06 0.04715911
## triceps  0.37000549 0.24308346 0.083627161 1.140091e-02 0.12962485
## insulin  0.35449746 0.13426556 0.107558111 8.920862e-02 0.18743692
## mass     0.44656894 0.02069878 0.109298901 2.196179e-04 0.12283258
## pedigree 0.14044530 0.05337616 0.216158588 5.500956e-01 0.02751639
## age      0.13212341 0.61454148 0.005675366 7.432186e-03 0.00601869
##                 Dim.6        Dim.7       Dim.8
## pregnant 0.0303632306 0.1362006927 0.015698209
## glucose  0.0001293374 0.0426177435 0.045466134
## pressure 0.3264593946 0.0058660682 0.004179781
## triceps  0.0090593787 0.0022908253 0.150907928
## insulin  0.0224315277 0.0181155871 0.086486214
## mass     0.2504660493 0.0045362616 0.045378869
## pedigree 0.0095266529 0.0006298513 0.002251413
## age      0.0006116685 0.2143970968 0.019200100&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Results for individuals
pca.ind &amp;lt;- get_pca_ind(pca)
pca.ind$coord %&amp;gt;% head()        # Coordinates&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Dim.1       Dim.2      Dim.3       Dim.4      Dim.5        Dim.6
## 72  -0.3669946 -0.56840272  0.3011660 -0.47509324  0.7579541  0.007199364
## 752 -0.6216923 -1.11858347 -1.1355107 -0.57113207 -0.3016049  0.252958078
## 551  0.9874862 -0.93348528 -0.6800219 -0.54448283 -0.1871258 -0.330097027
## 531  0.4210481 -1.06864263  0.6917994  0.24507935 -0.2124609 -0.145927479
## 170  0.2644303 -0.02563326 -0.3747252  0.03149040 -0.3520414 -1.156386621
## 764 -1.7515825  1.74941478 -1.0179324  0.02774283  2.3889854  0.431141870
##          Dim.7      Dim.8
## 72  -0.7799567 -0.5294728
## 752  0.1103204 -0.5529107
## 551 -0.4434670 -0.7824037
## 531 -0.3403009  0.2020358
## 170 -0.1597602  0.4188356
## 764  1.0485653 -0.7088242&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca.ind$contrib %&amp;gt;% head()       # Contributions to the PCs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Dim.1        Dim.2      Dim.3        Dim.4       Dim.5
## 72  0.010209939 3.073923e-02 0.01398836 0.0428594975 0.119537749
## 752 0.029299124 1.190467e-01 0.19885548 0.0619387561 0.018927615
## 551 0.073920645 8.290785e-02 0.07131829 0.0562934359 0.007285954
## 531 0.013439008 1.086540e-01 0.07381004 0.0114051979 0.009392415
## 170 0.005300614 6.251561e-05 0.02165614 0.0001882980 0.025787350
## 764 0.232575992 2.911832e-01 0.15980597 0.0001461473 1.187534409
##           Dim.6       Dim.7      Dim.8
## 72  0.000013006 0.233312081 0.12354459
## 752 0.016056557 0.004667754 0.13472450
## 551 0.027342521 0.075425638 0.26977286
## 531 0.005343545 0.044414265 0.01798843
## 170 0.335553241 0.009788896 0.07730788
## 764 0.046643993 0.421684107 0.22141831&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca.ind$cos2 %&amp;gt;% head()           # Quality of representation &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Dim.1        Dim.2      Dim.3        Dim.4       Dim.5
## 72  0.06019710 0.1444004010 0.04053852 1.008820e-01 0.256768635
## 752 0.10372696 0.3357975532 0.34603753 8.754147e-02 0.024412791
## 551 0.27405014 0.2448966943 0.12996122 8.331752e-02 0.009840908
## 531 0.08519110 0.5487767729 0.22998053 2.886318e-02 0.021691462
## 170 0.03731036 0.0003506023 0.07492606 5.291311e-04 0.066129394
## 764 0.20927254 0.2087548895 0.07067877 5.249919e-05 0.389294326
##            Dim.6       Dim.7      Dim.8
## 72  2.316564e-05 0.271892389 0.12529777
## 752 1.717267e-02 0.003266272 0.08204475
## 551 3.062324e-02 0.055270156 0.17204011
## 531 1.023305e-02 0.055648944 0.01961496
## 170 7.135315e-01 0.013618977 0.09360397
## 764 1.267919e-02 0.074996678 0.03427110&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;prediction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prediction&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p &amp;lt;- predict(pca, newdata = test)

# Plot prediction

plot &amp;lt;- fviz_pca_ind(pca, repel=T, geom = &amp;#39;point&amp;#39;)
fviz_add(plot, p, color=&amp;#39;red&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/pca/pca_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-calculate-coordinates-for-test-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to calculate coordinates for test data?&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;sources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/&#34;&gt;http://www.sthda.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Differential expression analysis</title>
      <link>https://suvar.netlify.com/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis/</link>
      <pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#gene-expression-analysis-of-histone-deacetylase-1-hdac1-knockout-mouse.&#34;&gt;Gene expression analysis of histone deacetylase 1 (HDAC1) knockout mouse.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sources&#34;&gt;Sources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;gene-expression-analysis-of-histone-deacetylase-1-hdac1-knockout-mouse.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gene expression analysis of histone deacetylase 1 (HDAC1) knockout mouse.&lt;/h2&gt;
&lt;p&gt;This short tutorial should help to understand the basic principal of gene expression analysis using simple dataset and nearly basic R.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Affymetrix microarray&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Dataset&lt;/em&gt;: &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE5583&#34;&gt;GSE5583&lt;/a&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Paper&lt;/em&gt;: Mol Cell Biol 2006 Nov;26(21):7913-28. &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pubmed/16940178&#34;&gt;PMID: 16940178&lt;/a&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;R code&lt;/em&gt;: Ahmed Moustafa&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Read the data into R
library (RCurl)
url = getURL (&amp;quot;http://bit.ly/GSE5583_data&amp;quot;, followlocation = TRUE)
data = as.matrix(read.table (text = url, row.names = 1, header = T))

# Check the loaded dataset
dim(data) # Dimension of the dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 12488     6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data shows gene experssion levels in 6 samples:
# rows correspond to samples (3 wild type WT and 3 knock-out KO)
# columns correspond to genes ids
head(data) # First few rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           WT.GSM130365 WT.GSM130366 WT.GSM130367 KO.GSM130368 KO.GSM130369
## 100001_at         11.5          5.6         69.1         15.7         36.0
## 100002_at         20.5         32.4         93.3         31.8         14.4
## 100003_at         72.4         89.0         79.2         80.5        130.1
## 100004_at        261.0        226.2        365.1        432.0        447.3
## 100005_at       1086.2       1555.6       1487.1       1062.2       1365.9
## 100006_at         49.7         52.9         15.0         25.8         48.8
##           KO.GSM130370
## 100001_at         42.0
## 100002_at         22.9
## 100003_at         86.7
## 100004_at        288.1
## 100005_at       1436.2
## 100006_at         54.8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;###################
# Exploratory plots
###################

# Check the behavior of the data
hist(data, col = &amp;quot;gray&amp;quot;, main=&amp;quot;GSE5583 - Histogram&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Log2 transformation (why?)
data2 = log2(data)

# Check the behavior of the data after log-transformation
hist(data2, col = &amp;quot;gray&amp;quot;, main=&amp;quot;GSE5583 (log2) - Histogram&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Boxplot
boxplot(data2, col=c(&amp;quot;darkgreen&amp;quot;, &amp;quot;darkgreen&amp;quot;, &amp;quot;darkgreen&amp;quot;,
                     &amp;quot;darkred&amp;quot;, &amp;quot;darkred&amp;quot;, &amp;quot;darkred&amp;quot;),
        main=&amp;quot;GSE5583 - boxplots&amp;quot;, las=2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Hierarchical clustering of the &amp;quot;samples&amp;quot; based on
# the correlation coefficients of the expression values
hc = hclust(as.dist(1-cor(data2)))
plot(hc, main=&amp;quot;GSE5583 - Hierarchical Clustering&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#######################################
# Differential expression (DE) analysis
#######################################

# Separate the two conditions into two smaller data frames
wt = data2[,1:3]
ko = data2[,4:6]

# Compute the means of the samples of each condition
wt.mean = apply(wt, 1, mean)
ko.mean = apply(ko, 1, mean)

head(wt.mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 100001_at 100002_at 100003_at 100004_at 100005_at 100006_at 
##  4.039868  5.306426  6.320360  8.120503 10.408872  5.089087&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(ko.mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 100001_at 100002_at 100003_at 100004_at 100005_at 100006_at 
##  4.844978  4.452076  6.597451  8.576804 10.318839  5.358071&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Just get the maximum of all the means
limit = max(wt.mean, ko.mean)

# Scatter plot
plot(ko.mean ~ wt.mean, xlab = &amp;quot;WT&amp;quot;, ylab = &amp;quot;KO&amp;quot;,
     main = &amp;quot;GSE5583 - Scatter&amp;quot;, xlim = c(0, limit), ylim = c(0, limit))
# Diagonal line
abline(0, 1, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute fold-change (biological significance)
# Difference between the means of the conditions
fold = wt.mean - ko.mean

# Histogram of the fold differences
hist(fold, col = &amp;quot;gray&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute statistical significance (using t-test)
pvalue = NULL # Empty list for the p-values
tstat = NULL # Empty list of the t test statistics

for(i in 1 : nrow(data)) { # For each gene : 
  x = wt[i,] # WT of gene number i
  y = ko[i,] # KO of gene number i
  
  # Compute t-test between the two conditions
  t = t.test(x, y)
  
  # Put the current p-value in the pvalues list
  pvalue[i] = t$p.value
  # Put the current t-statistic in the tstats list
  tstat[i] = t$statistic
}

head(pvalue)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5449730 0.3253745 0.3287830 0.1892376 0.6928410 0.7180077&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Histogram of p-values (-log10)
hist(-log10(pvalue), col = &amp;quot;gray&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-7.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Volcano: put the biological significance (fold-change)
# and statistical significance (p-value) in one plot
plot(fold, -log10(pvalue), main = &amp;quot;GSE5583 - Volcano&amp;quot;)

fold_cutoff = 2
pvalue_cutoff = 0.01
abline(v = fold_cutoff, col = &amp;quot;blue&amp;quot;, lwd = 3)
abline(v = -fold_cutoff, col = &amp;quot;red&amp;quot;, lwd = 3)
abline(h = -log10(pvalue_cutoff), col = &amp;quot;green&amp;quot;, lwd = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-8.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Screen for the genes that satisfy the filtering criteria

# Fold-change filter for &amp;quot;biological&amp;quot; significance
filter_by_fold = abs(fold) &amp;gt;= fold_cutoff
dim(data2[filter_by_fold, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 210   6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# P-value filter for &amp;quot;statistical&amp;quot; significance
filter_by_pvalue = pvalue &amp;lt;= pvalue_cutoff
dim(data2[filter_by_pvalue, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 429   6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Combined filter (both biological and statistical)
filter_combined = filter_by_fold &amp;amp; filter_by_pvalue

filtered = data2[filter_combined,]
dim(filtered)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 42  6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(filtered)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             WT.GSM130365 WT.GSM130366 WT.GSM130367 KO.GSM130368
## 100716_at       4.852998     4.906891     5.626439     7.572890
## 100914_at      10.340852     9.917074    10.250062    12.248787
## 101368_at       9.937227    10.204693    10.385215    12.270354
## 101550_at       5.526695     5.439623     6.221104     2.137504
## 101635_f_at     7.105385     6.722466     6.943687     5.266787
## 101883_s_at     5.768184     6.127221     5.133399    11.564292
##             KO.GSM130369 KO.GSM130370
## 100716_at       7.791163     7.299208
## 100914_at      12.185526    12.127124
## 101368_at      12.213499    12.078184
## 101550_at       2.906891     2.035624
## 101635_f_at     4.842979     4.643856
## 101883_s_at    11.679568    11.663514&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Let&amp;#39;s generate the volcano plot again,
# highlighting the significantly differential expressed genes
plot(fold, -log10(pvalue), main = &amp;quot;GSE5583 - Volcano #2&amp;quot;)
points (fold[filter_combined], -log10(pvalue[filter_combined]),
        pch = 16, col = &amp;quot;red&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-9.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Highlighting up-regulated in red and down-regulated in blue
plot(fold, -log10(pvalue), main = &amp;quot;GSE5583 - Volcano #3&amp;quot;)
points (fold[filter_combined &amp;amp; fold &amp;lt; 0],
        -log10(pvalue[filter_combined &amp;amp; fold &amp;lt; 0]),
        pch = 16, col = &amp;quot;red&amp;quot;)
points (fold[filter_combined &amp;amp; fold &amp;gt; 0],
        -log10(pvalue[filter_combined &amp;amp; fold &amp;gt; 0]),
        pch = 16, col = &amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-10.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Cluster the rows (genes) &amp;amp; columns (samples) by correlation
rowv = as.dendrogram(hclust(as.dist(1-cor(t(filtered)))))
colv = as.dendrogram(hclust(as.dist(1-cor(filtered))))

# Generate a heatmap
heatmap(filtered, Rowv=rowv, Colv=colv, cexCol=0.7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-1-11.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gplots)

# Enhanced heatmap
heatmap.2(filtered, Rowv=rowv, Colv=colv, cexCol=0.7,
          col = rev(redblue(256)), scale = &amp;quot;row&amp;quot;,
          trace=&amp;quot;none&amp;quot;, density.info=&amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Save the heatmap to a PDF file
pdf (&amp;quot;GSE5583_DE_Heatmap.pdf&amp;quot;)
heatmap.2(filtered, Rowv=rowv, Colv=colv, cexCol=0.7,
          col = rev(redblue(256)), scale = &amp;quot;row&amp;quot;)
dev.off()

# Save the DE genes to a text file
write.table (filtered, &amp;quot;GSE5583_DE.txt&amp;quot;, sep = &amp;quot;\t&amp;quot;,
             quote = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n = nrow(filtered)

cor.table = NULL
x = NULL
y = NULL
cor.val = NULL
cor.sig = NULL

for (i in 1 : (n-1)) {
  x_name = rownames(filtered)[i]
  x_exps = filtered[i, ]    
  
  for (j in (i+1) : n) {
    y_name = rownames(filtered)[j]
    y_exps = filtered[j, ]
    
    output = cor.test(x_exps,y_exps)
    
    x = c(x, x_name)
    y = c(y, y_name)
    cor.val = c(cor.val, output$estimate)
    cor.sig = c(cor.sig, output$p.value)
  }
}

cor.table = data.frame (x, y, cor.val, cor.sig)

dim(cor.table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 861   4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(cor.table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           x           y    cor.val      cor.sig
## 1 100716_at   100914_at  0.9732295 0.0010653980
## 2 100716_at   101368_at  0.9897688 0.0001564799
## 3 100716_at   101550_at -0.9060431 0.0128271221
## 4 100716_at 101635_f_at -0.9433403 0.0047245418
## 5 100716_at 101883_s_at  0.9508680 0.0035616301
## 6 100716_at   102712_at  0.9676037 0.0015572795&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sig_cutoff = 0.001

cor.filtered = subset (cor.table, cor.sig &amp;lt; sig_cutoff)

dim(cor.filtered)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 314   4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(cor.filtered)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            x         y    cor.val      cor.sig
## 2  100716_at 101368_at  0.9897688 1.564799e-04
## 8  100716_at 103088_at -0.9761495 8.464861e-04
## 10 100716_at 103299_at -0.9991089 1.190632e-06
## 14 100716_at 104700_at -0.9792543 6.411095e-04
## 15 100716_at 160172_at  0.9833552 4.132702e-04
## 16 100716_at 160943_at  0.9814703 5.118449e-04&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sources&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://gist.github.com/ahmedmoustafa&#34;&gt;Ahmed Moustafa githab&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Exploratory analysis in R</title>
      <link>https://suvar.netlify.com/post/statistics/exploration/exploration/</link>
      <pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/exploration/exploration/</guid>
      <description>


&lt;div id=&#34;example-1-iris&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Example 1: Iris&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Data description&lt;/strong&gt;: Iris is a famous (Fisher’s or Anderson’s) data set gives the measurements in centimeters of the variables &lt;strong&gt;sepal&lt;/strong&gt; length and width and &lt;strong&gt;petal&lt;/strong&gt; length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris &lt;strong&gt;setosa&lt;/strong&gt;, &lt;strong&gt;versicolor&lt;/strong&gt;, and &lt;strong&gt;virginica&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;structure-of-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Structure of the data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# select numerical columns 1-4
df = iris[1:4]

head(df, n=5) # first rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1          5.1         3.5          1.4         0.2
## 2          4.9         3.0          1.4         0.2
## 3          4.7         3.2          1.3         0.2
## 4          4.6         3.1          1.5         0.2
## 5          5.0         3.6          1.4         0.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(df, n=5) # last rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Sepal.Length Sepal.Width Petal.Length Petal.Width
## 146          6.7         3.0          5.2         2.3
## 147          6.3         2.5          5.0         1.9
## 148          6.5         3.0          5.2         2.0
## 149          6.2         3.4          5.4         2.3
## 150          5.9         3.0          5.1         1.8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(df)       # data dimention&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 150   4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nrow(df)      # number of rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 150&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ncol(df)      # number of columns&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(df)  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    150 obs. of  4 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-between-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Correlation between variables&lt;/h2&gt;
&lt;p&gt;Let’s find if data in columns are correlated using &lt;code&gt;corrplot&lt;/code&gt;.&lt;br /&gt;
&lt;a href=&#34;http://www.sthda.com/english/wiki/visualize-correlation-matrix-using-correlogram&#34;&gt;Here&lt;/a&gt; you can find how to tune correlogram.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(corrplot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## corrplot 0.84 loaded&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# build correlation matrix 
cor.mx &amp;lt;- cor(df)
# correlation plot ordered correlation coefficients
corrplot::corrplot.mixed(cor.mx, order=&amp;quot;hclust&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/exploration/exploration_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(cor.mx)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411
## Sepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259
## Petal.Length    0.8717538  -0.4284401    1.0000000   0.9628654
## Petal.Width     0.8179411  -0.3661259    0.9628654   1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)
# prepare training scheme
control &amp;lt;- caret::trainControl(method=&amp;quot;repeatedcv&amp;quot;, number=10, repeats=3)
#model &amp;lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&amp;quot;lvq&amp;quot;, preProcess=&amp;quot;scale&amp;quot;, trControl=control)
# estimate variable importance
#importance &amp;lt;- varImp(model, scale=FALSE)
# summarize importance
#print(importance)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How to show correlation table? Alternatively we can use &lt;code&gt;corrplot&lt;/code&gt; with &lt;code&gt;method = &amp;quot;number&amp;quot;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Missing data How to remove ‘NA’, ‘Inf’, and ‘0’&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colSums(sapply(df, is.na))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
##            0            0            0            0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Scatterplot Matrices
pairs(iris[, -5], bg = c(&amp;quot;yellow&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;black&amp;quot;)[iris$Species], pch = 21)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/exploration/exploration_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt; How to choose variables with the best correlation? Can we sort them by importance?&lt;/p&gt;
&lt;p&gt;Can we exclude data based on low correlation? This can be also done by dimentionality reduction methods such as &lt;a href=&#34;https://suvar.netlify.com/posts/statistics/pca/pca/&#34;&gt;PCA&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Can we remove outliers&lt;/p&gt;
&lt;p&gt;Do we need to remove duplicated data (rows, columns)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;filtering&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Filtering&lt;/h1&gt;
&lt;p&gt;You can see how to filter and subset your data in folloing posts: 1.&lt;br /&gt;
2.&lt;br /&gt;
3.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Compare Models And Select The Best Using The Caret R Package</title>
      <link>https://suvar.netlify.com/post/statistics/compare_models_caret/compare_models_caret/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/compare_models_caret/compare_models_caret/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#compare-models-and-select-the-best-using-the-caret-r-package&#34;&gt;Compare Models And Select The Best Using The Caret R Package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bibliography&#34;&gt;Bibliography&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;compare-models-and-select-the-best-using-the-caret-r-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compare Models And Select The Best Using The Caret R Package&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Data&lt;/em&gt;: mlbench::PimaIndiansDiabetes. Find the best model to predict diabetes from all given parameters.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Models&lt;/em&gt;:&lt;br /&gt;
* &lt;strong&gt;Learning Vector Quantization&lt;/strong&gt; (LVQ)&lt;br /&gt;
* &lt;strong&gt;Gradient Boosted Machine&lt;/strong&gt; (GBM)&lt;br /&gt;
* &lt;strong&gt;Support Vector Machine&lt;/strong&gt; (SVM)&lt;/p&gt;
&lt;p&gt;Each model is automatically tuned and is evaluated using 3 repeats of 10-fold cross validation.&lt;br /&gt;
The random number seed is set before each algorithm is trained to ensure that each algorithm gets the same data partitions and repeats.&lt;br /&gt;
The best models have 30 results (3 repeats of 10-fold cross validation).&lt;br /&gt;
The objective of comparing results is to compare the accuracy distributions (30 values) between the models.&lt;/p&gt;
&lt;p&gt;This is done in three ways. The distributions are summarized in terms of the percentiles. The distributions are summarized as box plots and finally the distributions are summarized as dot plots.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mlbench)
library(caret)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the dataset
data(PimaIndiansDiabetes)

# training scheme
control &amp;lt;- trainControl(method=&amp;quot;repeatedcv&amp;quot;, number=10, repeats=3)

# train the LVQ model
set.seed(7)
modelLvq &amp;lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&amp;quot;lvq&amp;quot;, trControl=control)

# train the GBM model
set.seed(7)
modelGbm &amp;lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&amp;quot;gbm&amp;quot;, trControl=control, verbose=FALSE)

# train the SVM model
set.seed(7)
modelSvm &amp;lt;- train(diabetes~., data=PimaIndiansDiabetes, method=&amp;quot;svmRadial&amp;quot;, trControl=control)

# collect resamples
results &amp;lt;- resamples(list(LVQ=modelLvq, GBM=modelGbm, SVM=modelSvm))

# summarize the distributions
summary(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## summary.resamples(object = results)
## 
## Models: LVQ, GBM, SVM 
## Number of resamples: 30 
## 
## Accuracy 
##          Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&amp;#39;s
## LVQ 0.5974026 0.6623377 0.7012987 0.6992538 0.7402597 0.7922078    0
## GBM 0.7012987 0.7402597 0.7662338 0.7678685 0.8045540 0.8552632    0
## SVM 0.6973684 0.7305195 0.7662338 0.7665243 0.7922078 0.8441558    0
## 
## Kappa 
##           Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&amp;#39;s
## LVQ 0.04251905 0.2444627 0.3210038 0.3064691 0.3989071 0.5276074    0
## GBM 0.24798301 0.3770808 0.4441549 0.4563312 0.5264481 0.6814024    0
## SVM 0.25171233 0.3670435 0.4590164 0.4500126 0.5211405 0.6457055    0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# boxplots of results
bwplot(results)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/compare_models_caret/compare_models_caret_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# dot plots of results
dotplot(results)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/compare_models_caret/compare_models_caret_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://machinelearningmastery.com/compare-models-and-select-the-best-using-the-caret-r-package/&#34;&gt;Compare Models And Select The Best Using The Caret R Package&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Gradient Boosting Machine</title>
      <link>https://suvar.netlify.com/post/statistics/gbm/gbm/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/gbm/gbm/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#gradient-boosting-machine&#34;&gt;Gradient Boosting Machine&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;gradient-boosting-machine&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gradient Boosting Machine&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Type of problem&lt;/em&gt;: regression and supervised classification.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>k-means</title>
      <link>https://suvar.netlify.com/post/statistics/k_means/k_means/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/k_means/k_means/</guid>
      <description>


&lt;div id=&#34;k-means-cluster-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;K-means cluster analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(datasets)
df &amp;lt;- datasets::iris
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 3 species:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique(df$Species)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] setosa     versicolor virginica 
## Levels: setosa versicolor virginica&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We try to predict them by &lt;em&gt;Petal.Lenght&lt;/em&gt;and &lt;em&gt;Petal.Width&lt;/em&gt; variables using k-means clustering.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot Petal.Length ~ Petal.Width data
plot(df$Petal.Length ~ df$Petal.Width)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/k_means/k_means_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Find number of clusters using wss
wss &amp;lt;- (nrow(df[, 3:4])-1)*sum(apply(df[, 3:4],2,var))
for (i in 2:15) wss[i] &amp;lt;- sum(kmeans(df[, 3:4], i)$withinss)
plot(1:15, wss, type=&amp;quot;b&amp;quot;, xlab=&amp;quot;Number of Clusters&amp;quot;, ylab=&amp;quot;Within groups sum of squares&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/k_means/k_means_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt; More than 3 clusters give no obvious advantages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Make k-means with 3 clasters
ncl &amp;lt;- 3
cl &amp;lt;- stats::kmeans(df[, 3:4], ncl, nstart = 20)
cl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## K-means clustering with 3 clusters of sizes 48, 50, 52
## 
## Cluster means:
##   Petal.Length Petal.Width
## 1     5.595833    2.037500
## 2     1.462000    0.246000
## 3     4.269231    1.342308
## 
## Clustering vector:
##   [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
##  [36] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [71] 3 3 3 3 3 3 3 1 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1
## [106] 1 3 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 3 1
## [141] 1 1 1 1 1 1 1 1 1 1
## 
## Within cluster sum of squares by cluster:
## [1] 16.29167  2.02200 13.05769
##  (between_SS / total_SS =  94.3 %)
## 
## Available components:
## 
## [1] &amp;quot;cluster&amp;quot;      &amp;quot;centers&amp;quot;      &amp;quot;totss&amp;quot;        &amp;quot;withinss&amp;quot;    
## [5] &amp;quot;tot.withinss&amp;quot; &amp;quot;betweenss&amp;quot;    &amp;quot;size&amp;quot;         &amp;quot;iter&amp;quot;        
## [9] &amp;quot;ifault&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compair result of clustering with real data (3 species of iris are in analysis)
table(cl$cluster, df$Species)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    
##     setosa versicolor virginica
##   1      0          2        46
##   2     50          0         0
##   3      0         48         4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot data
clusters &amp;lt;- split.data.frame(df, cl$cluster)
xlim &amp;lt;- c(min(df$Petal.Width), max(df$Petal.Width))
ylim &amp;lt;- c(min(df$Petal.Length), max(df$Petal.Length))
col &amp;lt;- c(&amp;#39;red&amp;#39;, &amp;#39;green&amp;#39;, &amp;#39;blue&amp;#39;)
plot(0, xlab=&amp;#39;Petal width&amp;#39;, ylab=&amp;#39;Petal length&amp;#39;, xlim=xlim, ylim=ylim)
for ( i in 1:ncl ) {
  points(clusters[[i]]$Petal.Length ~ clusters[[i]]$Petal.Width, col=col[i], xlim=xlim, ylim=ylim)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/k_means/k_means_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Learning Vector Quantization</title>
      <link>https://suvar.netlify.com/post/statistics/lvq/lvq/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/lvq/lvq/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#learning-vector-quantization&#34;&gt;Learning Vector Quantization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;learning-vector-quantization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Learning Vector Quantization&lt;/h2&gt;
&lt;p&gt;Learning Vector Quantiztion (LVQ) is a supervised classification algorithm for binary and multiclass problems. LVQ is a special case of a neural network.&lt;br /&gt;
LVQ model creates codebook vectors by learning training dataset. Codebook vectors represent class regions. They contain elements that placed around the respective class according to their matching level. If the element matches, it comes closer to the target class, if it does not match, it moves farther from it. With this codebooks, the model classifies new data. &lt;a href=&#34;http://jsalatas.ictpro.gr/implementation-of-competitive-learning-networks-for-weka/&#34;&gt;Here&lt;/a&gt; is a nice explanation how it works.&lt;/p&gt;
&lt;p&gt;There are several versions of &lt;strong&gt;LVQ&lt;/strong&gt; function:&lt;br /&gt;
&lt;code&gt;lvq1()&lt;/code&gt;, &lt;code&gt;olvq1()&lt;/code&gt;, &lt;code&gt;lvq2()&lt;/code&gt;, &lt;code&gt;lvq3()&lt;/code&gt;, &lt;code&gt;dlvq()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(class) # olvq1()
library(caret) # to split data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# generate dataset
df &amp;lt;- iris

id = caret::createDataPartition(df$Species, p = .8, list = F)

train = df[id, ]
test = df[-id, ]

# initialize an LVQ codebook
cb = class::lvqinit(train[1:4], train$Species)

# training set in a codebook.
build.cb = class::olvq1(train[1:4], train$Species, cb)

# classify test set from LVQ Codebook for test data
predict = class::lvqtest(build.cb, test[1:4])

# confusion matrix.
caret::confusionMatrix(test$Species, predict)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         10          0         0
##   versicolor      0          9         1
##   virginica       0          0        10
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9667          
##                  95% CI : (0.8278, 0.9992)
##     No Information Rate : 0.3667          
##     P-Value [Acc &amp;gt; NIR] : 4.476e-12       
##                                           
##                   Kappa : 0.95            
##                                           
##  Mcnemar&amp;#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            1.0000           0.9091
## Specificity                 1.0000            0.9524           1.0000
## Pos Pred Value              1.0000            0.9000           1.0000
## Neg Pred Value              1.0000            1.0000           0.9500
## Prevalence                  0.3333            0.3000           0.3667
## Detection Rate              0.3333            0.3000           0.3333
## Detection Prevalence        0.3333            0.3333           0.3333
## Balanced Accuracy           1.0000            0.9762           0.9545&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Naive Bayes</title>
      <link>https://suvar.netlify.com/post/statistics/naive_bayes/naive_bayes/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/naive_bayes/naive_bayes/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#naive-bayes&#34;&gt;Naive Bayes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;naive-bayes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Naive Bayes&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(P(c|x) = \frac{P(x|c)(P(c))}{P(x)}\)&lt;/span&gt;, where&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(P(c|x)\)&lt;/span&gt; - posteriour probability&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(P(x|c)\)&lt;/span&gt; - Likelihood&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(P(c)\)&lt;/span&gt; - Class Prior Probbility&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(P(x)\)&lt;/span&gt; - Predictor Prior Probability&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Support Vector Machine</title>
      <link>https://suvar.netlify.com/post/statistics/svm/svm/</link>
      <pubDate>Thu, 08 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/svm/svm/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#support-vector-machine&#34;&gt;Support Vector Machine&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;support-vector-machine&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Support Vector Machine&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;Type of problem&lt;/em&gt;: regression, supervised classification.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to split data into train and test subsets?</title>
      <link>https://suvar.netlify.com/post/statistics/split_data_ways/split_data_ways/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/split_data_ways/split_data_ways/</guid>
      <description>


&lt;p&gt;Here you can find several simple approaches to split data into train and test subset to fit and to test parameters of your model. We want to take 0.8 of our initial data to train our model.&lt;br /&gt;
Data: &lt;code&gt;datasets::iris&lt;/code&gt;.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;First approach is to create a &lt;strong&gt;vector containing randomly selected row ids&lt;/strong&gt; and to apply this vector to split data.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inTrain = sample(nrow(iris), nrow(iris)*0.8)

# split data
train = iris[inTrain, ]
test = iris[-inTrain, ]&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The same idea to split data as before using &lt;code&gt;caret&lt;/code&gt; package.&lt;br /&gt;
The advantage is that &lt;code&gt;createDataPartition&lt;/code&gt; function allows to split data many &lt;code&gt;times&lt;/code&gt; and use these subsets to estimate parameters of our model.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caret)
trainIndex &amp;lt;- createDataPartition(iris$Species, p=.8,
                                  list = FALSE,        # if FALSE - create a vector/matrix, if TRUE - create a list
                                  times = 1)           # how many subsets
# split data
train &amp;lt;- iris[trainIndex, ]
test &amp;lt;- iris[-trainIndex, ]&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Another approch is to create a &lt;strong&gt;logical vecotor&lt;/strong&gt; containing randomly distributed true/false and apply this vector to subset data.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;inTrain = sample(c(TRUE, FALSE), nrow(iris), replace = T, prob = c(0.8,0.2))

# select data
train = iris[inTrain, ]
test = iris[!inTrain, ]&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Using &lt;a href=&#34;https://cran.r-project.org/web/packages/caTools/index.html&#34;&gt;&lt;code&gt;caTools&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caTools)
inTrain = sample.split(iris, SplitRatio = .8)
train = subset(iris, inTrain == TRUE)
test  = subset(iris, inTrain == FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Using &lt;a href=&#34;https://cran.r-project.org/web/packages/dplyr/&#34;&gt;&lt;code&gt;dplyr&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
iris$id &amp;lt;- 1:nrow(iris)
train &amp;lt;- iris %&amp;gt;% dplyr::sample_frac(.8)
test  &amp;lt;- dplyr::anti_join(iris, train, by = &amp;#39;id&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Neuralnet R</title>
      <link>https://suvar.netlify.com/post/statistics/neuralnet_r/neuralnet_r/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/neuralnet_r/neuralnet_r/</guid>
      <description>


&lt;p&gt;Using of &lt;code&gt;neuralnet&lt;/code&gt; R package to tran Neural Network for classification.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(neuralnet)
library(datasets)

# data
df &amp;lt;- datasets::iris
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# split data
train.ratio = 0.9
n = nrow(df)

train.id &amp;lt;- sample(n, size = n*train.ratio)
train &amp;lt;- df[train.id,]
test &amp;lt;- df[-train.id,]&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;binary-classification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Binary classification&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nn &amp;lt;- neuralnet(Species == &amp;quot;setosa&amp;quot; ~ Petal.Length + Petal.Width, train, linear.output = FALSE)

# Predict for test data
pred &amp;lt;- predict(nn, test)
# Confusion matrix
t &amp;lt;- table(test$Species == &amp;quot;setosa&amp;quot;, pred[, 1] &amp;gt; 0.5)
t&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        
##         FALSE TRUE
##   FALSE    13    0
##   TRUE      0    2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot NN
plot(nn)

# Confusion matrix using caret
library(caret)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: ggplot2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;caret::confusionMatrix(t)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##        
##         FALSE TRUE
##   FALSE    13    0
##   TRUE      0    2
##                                     
##                Accuracy : 1         
##                  95% CI : (0.782, 1)
##     No Information Rate : 0.8667    
##     P-Value [Acc &amp;gt; NIR] : 0.1169    
##                                     
##                   Kappa : 1         
##                                     
##  Mcnemar&amp;#39;s Test P-Value : NA        
##                                     
##             Sensitivity : 1.0000    
##             Specificity : 1.0000    
##          Pos Pred Value : 1.0000    
##          Neg Pred Value : 1.0000    
##              Prevalence : 0.8667    
##          Detection Rate : 0.8667    
##    Detection Prevalence : 0.8667    
##       Balanced Accuracy : 1.0000    
##                                     
##        &amp;#39;Positive&amp;#39; Class : FALSE     
## &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;multiclass-classification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiclass classification&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nn &amp;lt;- neuralnet((Species == &amp;quot;setosa&amp;quot;) + (Species == &amp;quot;versicolor&amp;quot;) + (Species == &amp;quot;virginica&amp;quot;) 
                ~ Petal.Length + Petal.Width, train, linear.output = FALSE)

# Predict for test data
pred &amp;lt;- predict(nn, test)
table(test$Species, apply(pred, 1, which.max))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             
##              1 2 3
##   setosa     2 0 0
##   versicolor 0 3 1
##   virginica  0 0 9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot NN
plot(nn)

# Confusion matrix using &amp;#39;caret&amp;#39;
caret::confusionMatrix(t)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##        
##         FALSE TRUE
##   FALSE    13    0
##   TRUE      0    2
##                                     
##                Accuracy : 1         
##                  95% CI : (0.782, 1)
##     No Information Rate : 0.8667    
##     P-Value [Acc &amp;gt; NIR] : 0.1169    
##                                     
##                   Kappa : 1         
##                                     
##  Mcnemar&amp;#39;s Test P-Value : NA        
##                                     
##             Sensitivity : 1.0000    
##             Specificity : 1.0000    
##          Pos Pred Value : 1.0000    
##          Neg Pred Value : 1.0000    
##              Prevalence : 0.8667    
##          Detection Rate : 0.8667    
##    Detection Prevalence : 0.8667    
##       Balanced Accuracy : 1.0000    
##                                     
##        &amp;#39;Positive&amp;#39; Class : FALSE     
## &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;neuralnet-with-2-hidden-neurons&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Neuralnet with 2 hidden neurons&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(datasets)
library(neuralnet)
library(caret)

# data
df &amp;lt;- datasets::iris
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# split data
train.ratio = 0.9
n = nrow(df)

train.id &amp;lt;- sample(n, size = n*train.ratio)
train &amp;lt;- df[train.id,]
test &amp;lt;- df[-train.id,]

nn &amp;lt;- neuralnet(Species == &amp;quot;versicolor&amp;quot; ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, train, hidden = 2, linear.output = FALSE)

# Predict for test data
pred &amp;lt;- predict(nn, test)
# Confusion matrix
t &amp;lt;- table(test$Species == &amp;quot;versicolor&amp;quot;, pred[, 1] &amp;gt; 0.5)
# Confusion matrix
caret::confusionMatrix(t)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Confusion Matrix and Statistics
## 
##        
##         FALSE TRUE
##   FALSE     5    6
##   TRUE      0    4
##                                           
##                Accuracy : 0.6             
##                  95% CI : (0.3229, 0.8366)
##     No Information Rate : 0.6667          
##     P-Value [Acc &amp;gt; NIR] : 0.79696         
##                                           
##                   Kappa : 0.3077          
##                                           
##  Mcnemar&amp;#39;s Test P-Value : 0.04123         
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.4000          
##          Pos Pred Value : 0.4545          
##          Neg Pred Value : 1.0000          
##              Prevalence : 0.3333          
##          Detection Rate : 0.3333          
##    Detection Prevalence : 0.7333          
##       Balanced Accuracy : 0.7000          
##                                           
##        &amp;#39;Positive&amp;#39; Class : FALSE           
## &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot NN
plot(nn, rep=&amp;#39;best&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/neuralnet_r/neuralnet_r_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# compair predicted and actual values
test$predicted &amp;lt;- ifelse(pred&amp;gt;0.5, 1, 0)
test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species predicted
## 12           4.8         3.4          1.6         0.2     setosa         0
## 19           5.7         3.8          1.7         0.3     setosa         0
## 25           4.8         3.4          1.9         0.2     setosa         0
## 40           5.1         3.4          1.5         0.2     setosa         0
## 42           4.5         2.3          1.3         0.3     setosa         0
## 64           6.1         2.9          4.7         1.4 versicolor         1
## 70           5.6         2.5          3.9         1.1 versicolor         1
## 74           6.1         2.8          4.7         1.2 versicolor         1
## 90           5.5         2.5          4.0         1.3 versicolor         1
## 101          6.3         3.3          6.0         2.5  virginica         1
## 103          7.1         3.0          5.9         2.1  virginica         1
## 114          5.7         2.5          5.0         2.0  virginica         1
## 123          7.7         2.8          6.7         2.0  virginica         1
## 124          6.3         2.7          4.9         1.8  virginica         1
## 145          6.7         3.3          5.7         2.5  virginica         1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Uniform Manifold Approximation and Projection (UMAP)</title>
      <link>https://suvar.netlify.com/post/statistics/umap/umap/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/umap/umap/</guid>
      <description>


&lt;div id=&#34;uniform-manifold-approximation-and-projection-umap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Uniform Manifold Approximation and Projection (UMAP)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(umap)

head(iris, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# select determinative variables and lables
dat = iris[, grep(&amp;quot;Sepal|Petal&amp;quot;, colnames(iris))]
lbls = iris[, &amp;quot;Species&amp;quot;]

# apply UMAP transformation
set.seed(123)
iris.umap = umap::umap(dat)

# layout matrix (coordinates for vis.)
mx &amp;lt;- iris.umap$layout
head(mx, 5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]      [,2]
## [1,] 16.34555 1.7888418
## [2,] 14.74415 0.8264381
## [3,] 14.58204 1.3219384
## [4,] 14.55112 1.3763256
## [5,] 16.64683 2.1515565&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot UMAP components
plot(subset(mx, lbls == &amp;#39;setosa&amp;#39;), col = &amp;#39;red&amp;#39;,
     xlim=c(min(mx[,1]), max(mx[,1])),
     ylim=c(min(mx[,2]), max(mx[,2])))
points(subset(mx, lbls == &amp;#39;virginica&amp;#39;), col = &amp;#39;green&amp;#39;)
points(subset(mx, lbls == &amp;#39;versicolor&amp;#39;), col = &amp;#39;blue&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/umap/umap_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# generate test data by adding noise to original data
iris.wnoise = dat + matrix(rnorm(150*40, 0, 0.1), ncol=4)
colnames(iris.wnoise) = colnames(dat)
head(iris.wnoise, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1     5.043952    3.578774     1.328476   0.3430402
## 2     4.876982    3.076904     1.324731   0.3046629
## 3     4.855871    3.233220     1.206146   0.2435289&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# predict
pmx = predict(iris.umap, iris.wnoise)
head(pmx, 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       [,1]      [,2]
## 1 16.28067 1.5141526
## 2 14.81119 0.4305712
## 3 15.15975 1.3219315&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot generated test data
plot(subset(pmx, lbls == &amp;#39;setosa&amp;#39;), col = &amp;#39;red&amp;#39;,
     xlim=c(min(pmx[,1]), max(pmx[,1])),
     ylim=c(min(pmx[,2]), max(pmx[,2])))
points(subset(pmx, lbls == &amp;#39;virginica&amp;#39;), col = &amp;#39;green&amp;#39;)
points(subset(pmx, lbls == &amp;#39;versicolor&amp;#39;), col = &amp;#39;blue&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/umap/umap_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are two strategies for tuning: via configuration objects (1) and via additional arguments (2).&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;umap.defaults&lt;/code&gt; - configuration object.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set parameters
custom.config = umap.defaults
custom.config$random_state = 321
umap3 = umap(dat, custom.config)
mx2 = iris.umap$layout

plot(subset(mx2, lbls == &amp;#39;setosa&amp;#39;), col = &amp;#39;red&amp;#39;,
     xlim=c(min(mx2[,1]), max(mx2[,1])),
     ylim=c(min(mx2[,2]), max(mx2[,2])),
     main=&amp;quot;Another UMAP visualization (different seed)&amp;quot;)
points(subset(mx2, lbls == &amp;#39;virginica&amp;#39;), col = &amp;#39;green&amp;#39;)
points(subset(mx2, lbls == &amp;#39;versicolor&amp;#39;), col = &amp;#39;blue&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/umap/umap_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Additional arguments &lt;code&gt;iris.umap.3 = umap(iris.data, random_state=123)&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1802.03426&#34;&gt;UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://cran.r-project.org/web/packages/umap/vignettes/umap.html&#34;&gt;Uniform Manifold Approximation and Projection in R&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hierarchical cluster analysis</title>
      <link>https://suvar.netlify.com/post/statistics/hierarchical_cluster/hierarchical_cluser/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/hierarchical_cluster/hierarchical_cluser/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#hierarchical-cluster-analysis&#34;&gt;Hierarchical cluster analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#choosing-optimal-number-of-clusters&#34;&gt;Choosing optimal number of clusters&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#elbow-method&#34;&gt;Elbow method&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#choosing-the-best-clustering-model&#34;&gt;Choosing the best clustering model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hierarchical-cluster-analysis-1&#34;&gt;Hierarchical cluster analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bibliography&#34;&gt;Bibliography&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;hierarchical-cluster-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical cluster analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare data for analysis &lt;code&gt;datasets::USArrests&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(datasets)
df &amp;lt;- datasets::USArrests
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Murder Assault UrbanPop Rape
## Alabama      13.2     236       58 21.2
## Alaska       10.0     263       48 44.5
## Arizona       8.1     294       80 31.0
## Arkansas      8.8     190       50 19.5
## California    9.0     276       91 40.6
## Colorado      7.9     204       78 38.7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# check if &amp;#39;NA&amp;#39; values present in data
any(is.na(df))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# remove &amp;#39;NA&amp;#39; if necessary
df &amp;lt;- na.omit(df)
# normalize
df &amp;lt;- scale(df)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-optimal-number-of-clusters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing optimal number of clusters&lt;/h2&gt;
&lt;div id=&#34;elbow-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Elbow method&lt;/h3&gt;
&lt;p&gt;plot &lt;strong&gt;k&lt;/strong&gt; ~ &lt;strong&gt;wss&lt;/strong&gt;, where &lt;strong&gt;k&lt;/strong&gt; - is a cluseter number and &lt;strong&gt;wss&lt;/strong&gt; is a total within-cluster sum of square.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wss &amp;lt;- (nrow(df)-1)*sum(apply(df,2,var))
for (i in 2:15) wss[i] &amp;lt;- sum(kmeans(df, centers=i)$withinss)
plot(1:15, wss, type=&amp;quot;b&amp;quot;,
     xlab=&amp;quot;Number of Clusters&amp;quot;,
     ylab=&amp;quot;Within groups sum of squares&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/hierarchical_cluster/hierarchical_cluser_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt; This diagram shows that 4 number of clusers is optimal for this dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-the-best-clustering-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing the best clustering model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc2 &amp;lt;- cluster::agnes(df, method=&amp;#39;complete&amp;#39;)
hc2$ac&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8531583&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This metric allows to estimate the quality of cluster. Closer to 1 is better.&lt;br /&gt;
Using this metric we can try several models and choose the best one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- c( &amp;quot;average&amp;quot;, &amp;quot;single&amp;quot;, &amp;quot;complete&amp;quot;, &amp;quot;ward&amp;quot;)
names(m) &amp;lt;- c( &amp;quot;average&amp;quot;, &amp;quot;single&amp;quot;, &amp;quot;complete&amp;quot;, &amp;quot;ward&amp;quot;)
# function to compute coefficient
ac &amp;lt;- function(x) { cluster::agnes(df, method = x)$ac }

purrr::map_dbl(m, ac)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   average    single  complete      ward 
## 0.7379371 0.6276128 0.8531583 0.9346210&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see the &lt;strong&gt;ward method&lt;/strong&gt; gives the best clustering.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-cluster-analysis-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical cluster analysis&lt;/h2&gt;
&lt;p&gt;Let’s first split data into 4 groups using &lt;code&gt;clust::hclust&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calculate distances
d &amp;lt;- dist(df, method=&amp;#39;euclidean&amp;#39;)

# hierarchical cluster analysis
# &amp;#39;ward.D2&amp;#39; method is equivalent of agnes &amp;#39;ward&amp;#39;
hc1 &amp;lt;- hclust(d, method=&amp;#39;ward.D2&amp;#39;)

# Plot the obtained dendrogram
plot(hc1, hang = -1, cex = 0.6)

# show 4 clusers
rect.hclust(hc1, k=4, border=&amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/hierarchical_cluster/hierarchical_cluser_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# group data by clusters
groups &amp;lt;- cutree(hc1, k=3)
names(groups[groups == 1])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Alabama&amp;quot;        &amp;quot;Alaska&amp;quot;         &amp;quot;Arizona&amp;quot;        &amp;quot;California&amp;quot;    
##  [5] &amp;quot;Colorado&amp;quot;       &amp;quot;Florida&amp;quot;        &amp;quot;Georgia&amp;quot;        &amp;quot;Illinois&amp;quot;      
##  [9] &amp;quot;Louisiana&amp;quot;      &amp;quot;Maryland&amp;quot;       &amp;quot;Michigan&amp;quot;       &amp;quot;Mississippi&amp;quot;   
## [13] &amp;quot;Nevada&amp;quot;         &amp;quot;New Mexico&amp;quot;     &amp;quot;New York&amp;quot;       &amp;quot;North Carolina&amp;quot;
## [17] &amp;quot;South Carolina&amp;quot; &amp;quot;Tennessee&amp;quot;      &amp;quot;Texas&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# check for cluster metrics
names(hc1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;merge&amp;quot;       &amp;quot;height&amp;quot;      &amp;quot;order&amp;quot;       &amp;quot;labels&amp;quot;      &amp;quot;method&amp;quot;     
## [6] &amp;quot;call&amp;quot;        &amp;quot;dist.method&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can split data into 4 groups using &lt;code&gt;cluster::agnes&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# using &amp;#39;agnes&amp;#39; for hierarhical clustering
hc3 &amp;lt;- cluster::agnes(df, method=&amp;#39;ward&amp;#39;)
# plot slaster
cluster::pltree(hc3, hang = -1, cex = 0.6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/hierarchical_cluster/hierarchical_cluser_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# split into groups
groups &amp;lt;- cutree(as.hclust(hc3), k = 4)
groups[groups==1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        Alabama        Georgia      Louisiana    Mississippi North Carolina 
##              1              1              1              1              1 
## South Carolina      Tennessee 
##              1              1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://uc-r.github.io/hc_clustering&#34;&gt;UC Business Analytics R Programming Guide&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Logistic regression</title>
      <link>https://suvar.netlify.com/post/statistics/logistic_regression/logistic_regression/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/logistic_regression/logistic_regression/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#background&#34;&gt;Background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#binomial-logistic-regression&#34;&gt;Binomial logistic regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression&#34;&gt;Logistic regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-discriminant-analysis-lda&#34;&gt;Linear Discriminant Analysis (LDA)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quadratic-discriminant-analysis-qda&#34;&gt;Quadratic Discriminant Analysis (QDA)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#roc-curve-for-lda&#34;&gt;ROC-curve for LDA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tasks&#34;&gt;Tasks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Logistic regression builds model for binary dependent variables (0/1, True/False).&lt;/p&gt;
&lt;p&gt;Logistic function: &lt;span class=&#34;math display&#34;&gt;\[Y = \frac{1}{1+e^l} = \frac{e^l}{e^l+1}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is a linear combination of all observations (log-odds): &lt;span class=&#34;math inline&#34;&gt;\(l = \beta_0 + \beta_{1}x_{1} + \beta_{2}x_{2} + ... + \beta_{p}x_{p} + \epsilon\)&lt;/span&gt;&lt;br /&gt;
See also: &lt;a href=&#34;https://en.wikipedia.org/wiki/Sigmoid_function&#34;&gt;Sigmoid functions&lt;/a&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;binomial-logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Binomial logistic regression&lt;/h2&gt;
&lt;p&gt;Probability of passing an exam versus hours of study.&lt;br /&gt;
Data from &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34;&gt;wiki&lt;/a&gt; describe if students &lt;strong&gt;pass&lt;/strong&gt; exam depending of how many &lt;strong&gt;hours&lt;/strong&gt; they studied.&lt;br /&gt;
We build &lt;strong&gt;logistic regression&lt;/strong&gt; model to predict if ‘pass’ depending on learning ‘hours’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# put data into dataframe
hours=c(0.50,.75,1,1.25,1.5,1.75,1.75,2,2.25,2.5,2.75,3,3.25,3.50,4,4.25,4.5,4.75,5,5.5)
pass=c(0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1)
df = data.frame(hours, pass)

# Logistic Regression model
model.logit &amp;lt;- glm(pass ~ hours, data = df, family = &amp;#39;binomial&amp;#39;)

summary(model.logit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = pass ~ hours, family = &amp;quot;binomial&amp;quot;, data = df)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.70557  -0.57357  -0.04654   0.45470   1.82008  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)  
## (Intercept)  -4.0777     1.7610  -2.316   0.0206 *
## hours         1.5046     0.6287   2.393   0.0167 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 27.726  on 19  degrees of freedom
## Residual deviance: 16.060  on 18  degrees of freedom
## AIC: 20.06
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Coefficients Intercept = -4.0777 and Hours = 1.5046 are entered in the &lt;strong&gt;logistic regression equation&lt;/strong&gt; to estimate the odds (probability) of passing the exam: &lt;span class=&#34;math inline&#34;&gt;\(1/(1+e^{-(-4.0777+1.5046\cdot hours)})\)&lt;/span&gt; Calculate the probability to pass exam if studied 4 hours:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1/(1+exp(-(-4.0777+1.5046*4)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.874429&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s find a critical point where probability is 0.5:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;crit = -coef(model.logit)[1]/coef(model.logit)[2]
crit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept) 
##    2.710083&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# predict &amp;#39;pass&amp;#39; for given data
df$predic.prob &amp;lt;- predict(model.logit, df, type=&amp;quot;response&amp;quot;)
df$predic.pass &amp;lt;-  ifelse(df$predic.prob &amp;gt; 0.5, 1, 0)
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    hours pass predic.prob predic.pass
## 1   0.50    0  0.03471034           0
## 2   0.75    0  0.04977295           0
## 3   1.00    0  0.07089196           0
## 4   1.25    0  0.10002862           0
## 5   1.50    0  0.13934447           0
## 6   1.75    0  0.19083650           0
## 7   1.75    1  0.19083650           0
## 8   2.00    0  0.25570318           0
## 9   2.25    1  0.33353024           0
## 10  2.50    0  0.42162653           0
## 11  2.75    1  0.51501086           1
## 12  3.00    0  0.60735865           1
## 13  3.25    1  0.69261733           1
## 14  3.50    0  0.76648084           1
## 15  4.00    1  0.87444750           1
## 16  4.25    1  0.91027764           1
## 17  4.50    1  0.93662366           1
## 18  4.75    1  0.95561071           1
## 19  5.00    1  0.96909707           1
## 20  5.50    1  0.98519444           1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot data
plot(df$hours, df$pass, pch=19, col=&amp;#39;black&amp;#39;,
     main=&amp;#39;Probability of Passing Exam vs Hours Studying&amp;#39;,
     ylab=&amp;#39;Probability of Passing Exam&amp;#39;,
     xlab=&amp;#39;Hours Studying&amp;#39;)

# data frame to build a logistic function curve &amp;#39;hours~pass&amp;#39;
df2 &amp;lt;- data.frame(hours=seq(min(df$hours),max(df$hours),0.1), pass=NA)

# predict &amp;#39;pass&amp;#39; from our model
df2$pass &amp;lt;- predict(model.logit, df2, type=&amp;quot;response&amp;quot;)
# draw logistic function for our data sets
lines(df2$pass~df2$hours, lwd=2)
# critical point
abline(h=0.5, col=&amp;#39;green&amp;#39;) # 
abline(v=crit, col=&amp;#39;red&amp;#39;) # 

# draw predicted points (-0.02 to avoid overlapping with actual data)
points(df$hours, df$predic.pass-0.03, pch=19, col=&amp;#39;red&amp;#39;)

legend(&amp;#39;bottomright&amp;#39;, lty=c(1,1,1,1),
       col = c(&amp;#39;black&amp;#39;, &amp;#39;red&amp;#39;, &amp;#39;black&amp;#39;, &amp;#39;green&amp;#39;, &amp;#39;red&amp;#39;),
       legend = c(&amp;#39;actual data&amp;#39;, &amp;#39;predicted&amp;#39;, &amp;#39;Logistic function&amp;#39;, &amp;#39;Decision p&amp;#39;, &amp;#39;Decision bound&amp;#39;),
       lwd=c(NA,NA,1,1,1), pch=c(19,19,NA,NA,NA), bty = &amp;#39;n&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/logistic_regression/logistic_regression_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic regression&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Data&lt;/em&gt;: generated credit card balance &lt;code&gt;Default{ISLR}&lt;/code&gt;. 10000 observations for 4 variables:&lt;br /&gt;
&lt;strong&gt;default&lt;/strong&gt; – binary variable: Yes, if credit card holder did not return debt;&lt;br /&gt;
&lt;strong&gt;student&lt;/strong&gt; – binary variable: Yes, if credit card holder is a student;&lt;br /&gt;
&lt;strong&gt;balance&lt;/strong&gt; – average month balance on the bank account;&lt;br /&gt;
&lt;strong&gt;income&lt;/strong&gt; – income of credit card holder.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;#39;ISLR&amp;#39;)
head(Default)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   default student   balance    income
## 1      No      No  729.5265 44361.625
## 2      No     Yes  817.1804 12106.135
## 3      No      No 1073.5492 31767.139
## 4      No      No  529.2506 35704.494
## 5      No      No  785.6559 38463.496
## 6      No     Yes  919.5885  7491.559&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
# train subset rate is 0.85
inTrain &amp;lt;- sample(seq_along(Default$default), nrow(Default)*0.85)
df &amp;lt;- Default[inTrain, ]

# logistic regression model &amp;#39;default ~ f(balance)&amp;#39;
model.logit &amp;lt;- glm(default ~ balance, data = df, family = &amp;#39;binomial&amp;#39;)
summary(model.logit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = default ~ balance, family = &amp;quot;binomial&amp;quot;, data = df)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3282  -0.1420  -0.0553  -0.0201   3.7934  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -1.088e+01  4.022e-01  -27.07   &amp;lt;2e-16 ***
## balance      5.657e-03  2.448e-04   23.11   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2509.1  on 8499  degrees of freedom
## Residual deviance: 1336.5  on 8498  degrees of freedom
## AIC: 1340.5
## 
## Number of Fisher Scoring iterations: 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Predict &amp;#39;default&amp;#39; by &amp;#39;balance&amp;#39;
p.logit &amp;lt;- predict(model.logit, df, type = &amp;#39;response&amp;#39;)
predicted &amp;lt;- factor(ifelse(p.logit &amp;gt; 0.5, 2, 1),
                  levels = c(1, 2),
                  labels = c(&amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39;))

# true values for &amp;#39;default&amp;#39; in train data
actual &amp;lt;- df$default

# confusion matrix
conf.m &amp;lt;- table(actual, predicted)
conf.m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       predicted
## actual   No  Yes
##    No  8172   41
##    Yes  194   93&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sensitivity
conf.m[2, 2] / sum(conf.m[2, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3240418&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# specificity
conf.m[1, 1] / sum(conf.m[1, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9950079&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# probability
sum(diag(conf.m)) / sum(conf.m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9723529&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-discriminant-analysis-lda&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Discriminant Analysis (LDA)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#library(&amp;#39;GGally&amp;#39;)
library(&amp;#39;MASS&amp;#39;)
model.lda &amp;lt;- lda(default ~ balance, data = Default[inTrain, ])
model.lda&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## lda(default ~ balance, data = Default[inTrain, ])
## 
## Prior probabilities of groups:
##         No        Yes 
## 0.96623529 0.03376471 
## 
## Group means:
##       balance
## No   801.1297
## Yes 1757.2025
## 
## Coefficients of linear discriminants:
##                LD1
## balance 0.00220817&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Predict
p.lda &amp;lt;- predict(model.lda, df, type = &amp;#39;response&amp;#39;)
actual &amp;lt;- factor(ifelse(p.lda$posterior[, &amp;#39;Yes&amp;#39;] &amp;gt; 0.5, 
                         2, 1),
                  levels = c(1, 2),
                  labels = c(&amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39;))

# confusion matrix
conf.m &amp;lt;- table(actual, predicted)
conf.m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       predicted
## actual   No  Yes
##    No  8366   42
##    Yes    0   92&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sensitivity
conf.m[2, 2] / sum(conf.m[2, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# specificity
conf.m[1, 1] / sum(conf.m[1, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9950048&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# true
sum(diag(conf.m)) / sum(conf.m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9950588&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;quadratic-discriminant-analysis-qda&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quadratic Discriminant Analysis (QDA)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.qda &amp;lt;- qda(default ~ balance, data = Default[inTrain, ])
model.qda&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## qda(default ~ balance, data = Default[inTrain, ])
## 
## Prior probabilities of groups:
##         No        Yes 
## 0.96623529 0.03376471 
## 
## Group means:
##       balance
## No   801.1297
## Yes 1757.2025&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# predict
p.qda &amp;lt;- predict(model.qda, df, type = &amp;#39;response&amp;#39;)
predict &amp;lt;- factor(ifelse(p.qda$posterior[, &amp;#39;Yes&amp;#39;] &amp;gt; 0.5, 
                         2, 1),
                  levels = c(1, 2),
                  labels = c(&amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39;))

# confusion matrix
conf.m &amp;lt;- table(actual, predict)
conf.m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       predict
## actual   No  Yes
##    No  8390   18
##    Yes    0   92&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sensitivity
conf.m[2, 2] / sum(conf.m[2, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# specificity
conf.m[1, 1] / sum(conf.m[1, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9978592&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# true
sum(diag(conf.m)) / sum(conf.m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9978824&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;roc-curve-for-lda&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ROC-curve for LDA&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# считаем 1-SPC и TPR для всех вариантов границы отсечения
x &amp;lt;- NULL    # для (1 - SPC)
y &amp;lt;- NULL    # для TPR

# confusion matrix
tbl &amp;lt;- as.data.frame(matrix(rep(0, 4), 2, 2))
rownames(tbl) &amp;lt;- c(&amp;#39;fact.No&amp;#39;, &amp;#39;fact.Yes&amp;#39;)
colnames(tbl) &amp;lt;- c(&amp;#39;predict.No&amp;#39;, &amp;#39;predict.Yes&amp;#39;)

# probability vector
p.vector &amp;lt;- seq(0, 1, length = 501)

# цикл по вероятностям отсечения
for (p in p.vector){
    # prediction
    prediction &amp;lt;- factor(ifelse(p.lda$posterior[, &amp;#39;Yes&amp;#39;] &amp;gt; p, 
                             2, 1),
                      levels = c(1, 2),
                      labels = c(&amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39;))
    
    # data frame to compare data with prediction
    df.compare &amp;lt;- data.frame(actual = actual, prediction = prediction)
    
    # fill confusion matrix
    tbl[1, 1] &amp;lt;- nrow(df.compare[df.compare$Факт == &amp;#39;No&amp;#39; &amp;amp; df.compare$Прогноз == &amp;#39;No&amp;#39;, ])
    tbl[2, 2] &amp;lt;- nrow(df.compare[df.compare$Факт == &amp;#39;Yes&amp;#39; &amp;amp; df.compare$Прогноз == &amp;#39;Yes&amp;#39;, ])
    tbl[1, 2] &amp;lt;- nrow(df.compare[df.compare$Факт == &amp;#39;No&amp;#39; &amp;amp; df.compare$Прогноз == &amp;#39;Yes&amp;#39;, ])
    tbl[2, 1] &amp;lt;- nrow(df.compare[df.compare$Факт == &amp;#39;Yes&amp;#39; &amp;amp; df.compare$Прогноз == &amp;#39;No&amp;#39;, ])
    
    # calculate metrix
    TPR &amp;lt;- tbl[2, 2] / sum(tbl[2, 2] + tbl[2, 1])
    y &amp;lt;- c(y, TPR)
    SPC &amp;lt;- tbl[1, 1] / sum(tbl[1, 1] + tbl[1, 2])
    x &amp;lt;- c(x, 1 - SPC)
}

# ROC-curve
par(mar = c(5, 5, 1, 1))
# curve
plot(x, y, type = &amp;#39;l&amp;#39;, col = &amp;#39;blue&amp;#39;, lwd = 3,
     xlab = &amp;#39;(1 - SPC)&amp;#39;, ylab = &amp;#39;TPR&amp;#39;, 
     xlim = c(0, 1), ylim = c(0, 1))
# line of random classifier
abline(a = 0, b = 1, lty = 3, lwd = 2)

# oint for probability 0.5
points(x[p.vector == 0.5], y[p.vector == 0.5], pch = 16)
text(x[p.vector == 0.5], y[p.vector == 0.5], &amp;#39;p = 0.5&amp;#39;, pos = 4)
# point for probability 0.2
points(x[p.vector == 0.2], y[p.vector == 0.2], pch = 16)
text(x[p.vector == 0.2], y[p.vector == 0.2], &amp;#39;p = 0.2&amp;#39;, pos = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://suvar.netlify.com/post/statistics/logistic_regression/logistic_regression_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict &amp;lt;- factor(ifelse(p.lda$posterior[, &amp;#39;Yes&amp;#39;] &amp;gt; 0.2, 2, 1),
                      levels = c(1, 2),
                      labels = c(&amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39;))

conf.m &amp;lt;- table(actual, predict)
conf.m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       predict
## actual   No  Yes
##    No  8124  284
##    Yes    0   92&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sensitivity
conf.m[2, 2] / sum(conf.m[2, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# specificity
conf.m[1, 1] / sum(conf.m[1, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9662226&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# true
sum(diag(conf.m)) / sum(conf.m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9665882&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tasks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tasks&lt;/h2&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hypothesis testing</title>
      <link>https://suvar.netlify.com/post/statistics/hypothesis_testing/hypothesis_testing/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/hypothesis_testing/hypothesis_testing/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#t-test&#34;&gt;t-test&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#t-test-and-normal-distribution&#34;&gt;t-test and normal distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#one-sample-t-test&#34;&gt;One-sample t-test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#two-samples-t-test&#34;&gt;Two samples t-test&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-of-r-functions-for-t-tests&#34;&gt;Summary of R functions for t-tests&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-that&#34;&gt;What is that?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#non-parametric-tests&#34;&gt;Non-parametric tests&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#mann-whitney-u-rank-sum-test&#34;&gt;Mann-Whitney U Rank Sum Test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wilcoxon-test&#34;&gt;Wilcoxon test&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tests-for-categorical-variables&#34;&gt;Tests for categorical variables&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#chi-squared-tests&#34;&gt;Chi-squared tests&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-testing&#34;&gt;Multiple testing&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-bonferroni-correction&#34;&gt;The Bonferroni correction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sources&#34;&gt;Sources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Null hypothesis (H&lt;sub&gt;0&lt;/sub&gt;):&lt;br /&gt;
1. H&lt;sub&gt;0&lt;/sub&gt;: m = μ&lt;br /&gt;
2. H&lt;sub&gt;0&lt;/sub&gt;: m &lt;span class=&#34;math inline&#34;&gt;\(\leq\)&lt;/span&gt; μ&lt;br /&gt;
3. H&lt;sub&gt;0&lt;/sub&gt;: m &lt;span class=&#34;math inline&#34;&gt;\(\geq\)&lt;/span&gt; μ&lt;/p&gt;
&lt;p&gt;Alternative hypotheses (H&lt;sub&gt;a&lt;/sub&gt;): 1. H&lt;sub&gt;a&lt;/sub&gt;:m ≠ μ (different)&lt;br /&gt;
2. H&lt;sub&gt;a&lt;/sub&gt;:m &amp;gt; μ (greater)&lt;br /&gt;
3. H&lt;sub&gt;a&lt;/sub&gt;:m &amp;lt; μ (less)&lt;/p&gt;
&lt;p&gt;Note: Hypothesis 1. are called &lt;strong&gt;two-tailed tests&lt;/strong&gt; and hypotheses 2. &amp;amp; 3. are called &lt;strong&gt;one-tailed tests&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The p-value is the probability that the observed data could happen, under the condition that the null hypothesis is true.&lt;/p&gt;
&lt;p&gt;Note: p-value is not the probability that the null hypothesis is true.&lt;br /&gt;
Note: Absence of evidence ⧧ evidence of absence.&lt;/p&gt;
&lt;p&gt;Cutoffs for hypothesis testing *p &amp;lt; 0.05, **p &amp;lt; 0.01, ***p &amp;lt; 0.001. If p value is less than significance level alpha (0.05), the hull hypothesies is rejected.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;not rejected (‘negative’)&lt;/th&gt;
&lt;th&gt;rejected (‘positive’)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;H&lt;sub&gt;0&lt;/sub&gt; true&lt;/td&gt;
&lt;td&gt;True negative (specificity)&lt;/td&gt;
&lt;td&gt;False Positive (Type I error)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;H&lt;sub&gt;0&lt;/sub&gt; false&lt;/td&gt;
&lt;td&gt;False Negative (Type II error)&lt;/td&gt;
&lt;td&gt;True positive (sensitivity)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Type II errors are usually more dangerous.&lt;/p&gt;
&lt;div id=&#34;t-test&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;t-test&lt;/h1&gt;
&lt;div id=&#34;t-test-and-normal-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;t-test and normal distribution&lt;/h2&gt;
&lt;p&gt;t-distribution assumes that the observations are &lt;strong&gt;independent&lt;/strong&gt; and that they follow a &lt;strong&gt;normal distribution&lt;/strong&gt;. If the data are &lt;strong&gt;dependent&lt;/strong&gt;, then p-values will likely be totally wrong (e.g., for positive correlation, too optimistic). Type II errors?&lt;br /&gt;
It is good to test if observations are normally distributed. Otherwise we assume that data is normally distributed.&lt;br /&gt;
Independence of observations is usually not testable, but can be reasonably assumed if the data collection process was random without replacement.&lt;/p&gt;
&lt;p&gt;FIXME: I do not understand this. Deviation data from normalyty will lead to type-I errors. I data is deviated from normal distribution, use &lt;strong&gt;Wilcoxon test&lt;/strong&gt; or &lt;strong&gt;permutation tests&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-sample-t-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;One-sample t-test&lt;/h2&gt;
&lt;p&gt;One-sample t-test is used to compare the mean of one sample to a known standard (or theoretical/hypothetical) mean (μ).&lt;br /&gt;
t-statistics: &lt;span class=&#34;math inline&#34;&gt;\(t = \frac{m - \mu}{s/\sqrt{n}}\)&lt;/span&gt;, where&lt;br /&gt;
&lt;strong&gt;m&lt;/strong&gt; is the sample &lt;strong&gt;mean&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;n&lt;/strong&gt; is the sample &lt;strong&gt;size&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;s&lt;/strong&gt; is the sample &lt;strong&gt;standard deviation&lt;/strong&gt; with n−1 degrees of freedom&lt;br /&gt;
&lt;strong&gt;μ&lt;/strong&gt; is the &lt;strong&gt;theoretical value&lt;/strong&gt;&lt;br /&gt;
Q: And what should I do with this t-statistics?&lt;br /&gt;
Q: What is the difference between t-test and ANOVA?&lt;br /&gt;
Q: What is the smallest sample size which can be tested by t-test?&lt;br /&gt;
Q: Show diagrams explaining why p-value of one-sided is smaller than two-sided tests.&lt;/p&gt;
&lt;p&gt;R example:&lt;br /&gt;
We want to test if N is different from given mean μ=0:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N = c(-0.01, 0.65, -0.17, 1.77, 0.76, -0.16, 0.88, 1.09, 0.96, 0.25)
t.test(N, mu = 0, alternative = &amp;quot;less&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  N
## t = 3.0483, df = 9, p-value = 0.9931
## alternative hypothesis: true mean is less than 0
## 95 percent confidence interval:
##      -Inf 0.964019
## sample estimates:
## mean of x 
##     0.602&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(N, mu = 0, alternative = &amp;quot;two.sided&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  N
## t = 3.0483, df = 9, p-value = 0.01383
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  0.1552496 1.0487504
## sample estimates:
## mean of x 
##     0.602&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(N, mu = 0, alternative = &amp;quot;greater&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  N
## t = 3.0483, df = 9, p-value = 0.006916
## alternative hypothesis: true mean is greater than 0
## 95 percent confidence interval:
##  0.239981      Inf
## sample estimates:
## mean of x 
##     0.602&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;FIXME: why it accepts all alternatives at the same time (less and greater?)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-samples-t-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Two samples t-test&lt;/h2&gt;
&lt;p&gt;Do two different samples have the same mean?&lt;br /&gt;
H&lt;sub&gt;0&lt;/sub&gt;:&lt;br /&gt;
1. H&lt;sub&gt;0&lt;/sub&gt;: m&lt;sub&gt;1&lt;/sub&gt; - m&lt;sub&gt;2&lt;/sub&gt; = 0&lt;br /&gt;
2. H&lt;sub&gt;0&lt;/sub&gt;: m&lt;sub&gt;1&lt;/sub&gt; - m&lt;sub&gt;2&lt;/sub&gt; &lt;span class=&#34;math inline&#34;&gt;\(\leq\)&lt;/span&gt; 0&lt;br /&gt;
3. H&lt;sub&gt;0&lt;/sub&gt;: m&lt;sub&gt;1&lt;/sub&gt; - m&lt;sub&gt;2&lt;/sub&gt; &lt;span class=&#34;math inline&#34;&gt;\(\geq\)&lt;/span&gt; 0&lt;/p&gt;
&lt;p&gt;H&lt;sub&gt;a&lt;/sub&gt;:&lt;br /&gt;
1. H&lt;sub&gt;a&lt;/sub&gt;: m&lt;sub&gt;1&lt;/sub&gt; - m&lt;sub&gt;2&lt;/sub&gt; ≠ 0 (different)&lt;br /&gt;
2. H&lt;sub&gt;a&lt;/sub&gt;: m&lt;sub&gt;1&lt;/sub&gt; - m&lt;sub&gt;2&lt;/sub&gt; &amp;gt; 0 (greater)&lt;br /&gt;
3. H&lt;sub&gt;a&lt;/sub&gt;: m&lt;sub&gt;1&lt;/sub&gt; - m&lt;sub&gt;2&lt;/sub&gt; &amp;lt; 0 (less)&lt;/p&gt;
&lt;p&gt;The paired sample t-test has four main assumptions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The dependent variable must be &lt;strong&gt;continuous&lt;/strong&gt; (interval/ratio).&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;The observations are &lt;strong&gt;independent&lt;/strong&gt; of one another.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;The dependent variable should be approximately &lt;strong&gt;normally distributed&lt;/strong&gt;.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;The dependent variable should not contain any &lt;strong&gt;outliers&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Continuous data can take on any value within a range (income, height, weight, etc.). The opposite of continuous data is discrete data, which can only take on a few values (Low, Medium, High, etc.). Occasionally, discrete data can be used to approximate a continuous scale, such as with &lt;strong&gt;Likert-type scales&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;t-statistics: &lt;span class=&#34;math inline&#34;&gt;\(t=\frac{y - x}{SE}\)&lt;/span&gt;, where y and x are the samples means. SE is the standard error for the difference. If H&lt;sub&gt;0&lt;/sub&gt; is correct, test statistic follows a t-distribution with n+m-2 degrees of freedom (n, m the number of observations in each sample).&lt;/p&gt;
&lt;p&gt;To apply t-test samples must be tested if they have equal variance:&lt;br /&gt;
equal variance (homoscedastic). Type 3 means two samples, unequal variance (heteroscedastic).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### t-test
a = c(175, 168, 168, 190, 156, 181, 182, 175, 174, 179)
b = c(185, 169, 173, 173, 188, 186, 175, 174, 179, 180)

# test homogeneity of variances using Fisher’s F-test
var.test(a,b)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  F test to compare two variances
## 
## data:  a and b
## F = 2.1028, num df = 9, denom df = 9, p-value = 0.2834
## alternative hypothesis: true ratio of variances is not equal to 1
## 95 percent confidence interval:
##  0.5223017 8.4657950
## sample estimates:
## ratio of variances 
##           2.102784&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# variance is homogene (can use var.equal=T in t.test)

# t-test
t.test(a,b, 
       var.equal=TRUE,   # variance is homogene (tested by var.test(a,b)) 
       paired=FALSE)     # samples are independent&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Two Sample t-test
## 
## data:  a and b
## t = -0.94737, df = 18, p-value = 0.356
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -10.93994   4.13994
## sample estimates:
## mean of x mean of y 
##     174.8     178.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-of-r-functions-for-t-tests&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary of R functions for t-tests&lt;/h1&gt;
&lt;p&gt;One-sample t-test&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(x, mu = 0, alternative = c(&amp;quot;two.sided&amp;quot;, &amp;quot;less&amp;quot;, &amp;quot;greater&amp;quot;), paired = FALSE, var.equal = FALSE, conf.level = 0.95)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;what-is-that&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is that?&lt;/h2&gt;
&lt;p&gt;one-way ANOVA or 2-way ANOVA with &lt;strong&gt;Bonferroni multiple comparison&lt;/strong&gt; or &lt;strong&gt;Dunnett’s post-test&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;non-parametric-tests&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Non-parametric tests&lt;/h1&gt;
&lt;div id=&#34;mann-whitney-u-rank-sum-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mann-Whitney U Rank Sum Test&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The dependent variable is ordinal or continuous.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;The data consist of a randomly selected sample of independent observations from two independent groups.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;The dependent variables for the two independent groups share a similar shape.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;wilcoxon-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wilcoxon test&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;Wilcoxon&lt;/strong&gt; is a &lt;strong&gt;non-parametric test&lt;/strong&gt; which works on normal and non-normal data. However, we usually prefer not to use it if we can assume that the data is normally distributed. The non-parametric test comes with less statistical power, this is a price that one has to pay for more flexible assumptions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tests-for-categorical-variables&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tests for categorical variables&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Categorical variable&lt;/strong&gt; can take fixed number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property.&lt;/p&gt;
&lt;div id=&#34;chi-squared-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chi-squared tests&lt;/h2&gt;
&lt;p&gt;The chi-squared test is most suited to large datasets. As a general rule, the chi-squared test is appropriate if at least 80% of the cells have an expected frequency of 5 or greater. In addition, none of the cells should have an expected frequency less than 1. If the expected values are very small, categories may be combined (if it makes sense to do so) to create fewer larger categories. Alternatively, Fisher’s exact test can be used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data = rbind(c(83,35), c(92,43))
data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]   83   35
## [2,]   92   43&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;chisq.test(data, correct=F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s Chi-squared test
## 
## data:  data
## X-squared = 0.14172, df = 1, p-value = 0.7066&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;chisq.test(testor,correct=F) ## Fisher’s Exact test R Example:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Group&lt;/th&gt;
&lt;th&gt;TumourShrinkage-No&lt;/th&gt;
&lt;th&gt;TumourShrinkage-Yes&lt;/th&gt;
&lt;th&gt;Total&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1 Treatment&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;2 Placebo&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;3 Total&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;24&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The &lt;strong&gt;null hypothesis&lt;/strong&gt; is that there is &lt;strong&gt;no association&lt;/strong&gt; between treatment and tumour shrinkage.&lt;br /&gt;
The &lt;strong&gt;alternative hypothesis&lt;/strong&gt; is that there is &lt;strong&gt;some association&lt;/strong&gt; between treatment group and tumour shrinkage.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data = rbind(c(8,3), c(9,4))
data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    8    3
## [2,]    9    4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fisher.test(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Fisher&amp;#39;s Exact Test for Count Data
## 
## data:  data
## p-value = 1
## alternative hypothesis: true odds ratio is not equal to 1
## 95 percent confidence interval:
##   0.1456912 10.6433317
## sample estimates:
## odds ratio 
##   1.176844&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output Fisher’s exact test tells us that the probability of observing such an extreme combination of frequencies is high, our p-value is 1.000 which is clearly greater than 0.05. In this case, there is &lt;strong&gt;no evidence of an association&lt;/strong&gt; between treatment group and tumour shrinkage.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Multiple testing&lt;/h1&gt;
&lt;p&gt;When performing a large number of tests, the type I error is inflated: for α=0.05 and performing n tests, the probability of no false positive result is: 0.095 x 0.95 x … (n-times) &amp;lt;&amp;lt;&amp;lt; 0.095&lt;br /&gt;
The larger the number of tests performed, the higher the probability of a false rejection!&lt;br /&gt;
Many data analysis approaches in genomics rely on itemby-item (i.e. multiple) testing:&lt;br /&gt;
Microarray or RNA-Seq expression profiles of “normal” vs “perturbed” samples: gene-by-gene&lt;br /&gt;
ChIP-chip: locus-by-locus&lt;br /&gt;
RNAi and chemical compound screens&lt;br /&gt;
Genome-wide association studies: marker-by-marker&lt;br /&gt;
QTL analysis: marker-by-marker and trait-by-trait&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;False positive rate&lt;/strong&gt; (FPR) - the proportion of false positives among all resulst.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;False discovery rate&lt;/strong&gt; (FDR) - the proportion of false positives among all significant results.&lt;/p&gt;
&lt;p&gt;Example: 20,000 genes, 100 hits, 10 of them wrong.&lt;br /&gt;
FPR: 0.05%&lt;br /&gt;
FDR: 10%&lt;/p&gt;
&lt;div id=&#34;the-bonferroni-correction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Bonferroni correction&lt;/h2&gt;
&lt;p&gt;The Bonferroni correction sets the significance cut-off at α/n.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sources&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sources&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;http://www.sthda.com/english/wiki/one-sample-t-test-in-r&#34;&gt;One-Sample T-test in R&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Model evaluation metrics</title>
      <link>https://suvar.netlify.com/post/statistics/model_evaluation_metrics/model_evaluation_metrics/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/model_evaluation_metrics/model_evaluation_metrics/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#confusion-matrix&#34;&gt;Confusion Matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gain-and-lift-chart&#34;&gt;Gain and Lift Chart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kolmogorov-smirnov-chart&#34;&gt;Kolmogorov Smirnov Chart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#auc-roc&#34;&gt;AUC – ROC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gini-coefficient&#34;&gt;Gini Coefficient&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#concordant-discordant-ratio&#34;&gt;Concordant – Discordant Ratio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#root-mean-squared-error&#34;&gt;Root Mean Squared Error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bibliography&#34;&gt;Bibliography&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2016/02/7-important-model-evaluation-error-metrics/&#34;&gt;check this link&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;confusion-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Confusion Matrix&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;True Y&lt;/th&gt;
&lt;th&gt;True N&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Predicted Y&lt;/td&gt;
&lt;td&gt;True Positive (TP)&lt;/td&gt;
&lt;td&gt;False Positive (FP)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Predicted N&lt;/td&gt;
&lt;td&gt;False Negative (FN)&lt;/td&gt;
&lt;td&gt;True Negatives&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Common performance metrcs: &lt;strong&gt;False Positive Rate&lt;/strong&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\frac{FP}{N}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;True Positive Rate (sensitivity)&lt;/strong&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\frac{TP}{P}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Precision&lt;/strong&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\frac{TP}{TP+FP}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Accuracy&lt;/strong&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\frac{TP+TN}{P+N}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Specificity&lt;/strong&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\frac{TN}{FP+TN}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Precision (PPV)&lt;/strong&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\frac{TP}{TP+FP} = 1 - FDR\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;False Discovery Rate (FDR)&lt;/strong&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\frac{FP}{FP+TP} = 1 - PPV\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;See more on &lt;a href=&#34;https://en.wikipedia.org/wiki/Confusion_matrix&#34;&gt;wiki&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gain-and-lift-chart&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gain and Lift Chart&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;kolmogorov-smirnov-chart&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Kolmogorov Smirnov Chart&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;auc-roc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;AUC – ROC&lt;/h2&gt;
&lt;p&gt;ROC graphs are two-dimensional graphs in which &lt;strong&gt;True Positive&lt;/strong&gt; rate is plotted on the Y axis and &lt;strong&gt;False Positive&lt;/strong&gt; rate is plotted on the X axis.&lt;br /&gt;
An ROC graph depicts relative tradeoffs between benefits (true positives) and costs (false positives) (fawcett_introduction_2006).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gini-coefficient&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gini Coefficient&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;concordant-discordant-ratio&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Concordant – Discordant Ratio&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;root-mean-squared-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Root Mean Squared Error&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simple Markov process</title>
      <link>https://suvar.netlify.com/post/statistics/markov_process/simple_markov_process/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>https://suvar.netlify.com/post/statistics/markov_process/simple_markov_process/</guid>
      <description>


&lt;p&gt;Here, we will consider a simple example of Markov process with implementation in R.&lt;br /&gt;
The following example is taken from &lt;a href=&#34;http://www.bodowinter.com&#34;&gt;Bodo Winter website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;Markov process&lt;/strong&gt; is characterized by (1) &lt;strong&gt;a finite set of states&lt;/strong&gt; and (2) &lt;strong&gt;fixed transition probabilities between the states&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let’s consider an example. Assume you have a classroom, with students who could be either in the state &lt;strong&gt;alert&lt;/strong&gt; or in the state &lt;strong&gt;bored&lt;/strong&gt;. And then, at any given time point, there’s a certain probability of an alert student becoming bored (say 0.2), and there’s a probability of a bored student becoming alert (say 0.25).&lt;/p&gt;
&lt;p&gt;Let’s say there are 20 alert and 80 bored students in a particular class. This is your initial condition at time point &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Given the transition probabilities above, what’s the number of alert and bored students at the next point in time, &lt;span class=&#34;math inline&#34;&gt;\(t+1\)&lt;/span&gt;?&lt;br /&gt;
Multiply 20 by 0.2 (=4) and these will be the alert students that turn bored.&lt;br /&gt;
And then multiply 80 by 0.25 (=20) and these will be the bored students that turn alert.&lt;br /&gt;
So, at &lt;span class=&#34;math inline&#34;&gt;\(t+1\)&lt;/span&gt;, there’s going to be 20-4+20 alert students. And there’s going to be 80+4-20 bored students. Before, 80% of the students were bored and now, only 64% of the students are bored. Conversely, 36% are alert.&lt;/p&gt;
&lt;p&gt;A handy way of representing this Markov process is by defining a transition probability matrix:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;A&lt;/th&gt;
&lt;th&gt;B&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;A&lt;span class=&#34;math inline&#34;&gt;\(_{t+1}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0.25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;B&lt;span class=&#34;math inline&#34;&gt;\(_{t+1}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;0.75&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;What this matrix says is: A proportion of 0.8 of the people who are in state A (alert) will also be at state A at time point &lt;span class=&#34;math inline&#34;&gt;\(t+1\)&lt;/span&gt;. And, a proportion of 0.25 of the people who are in state B (bored) will switch to alert at t+1. This is what the first row says. The next row is simply one minus the probabilities of the first row, because probabilities (or proportions) have to add up to 1. Now think about multiplying this matrix with the initial proportions of alert and bored students that we had above. 0.8 are bored and 0.2 are alert. In linear algebra this would look the following way:&lt;/p&gt;
&lt;div&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
 0.8 &amp;amp; 0.25 \\
 0.2 &amp;amp; 0.75
\end{bmatrix}\times\begin{bmatrix}
 0.2 \\
 0.8
\end{bmatrix} = \begin{bmatrix}
 0.8\times0.2 + 0.25\times0.8 \\
 0.2\times0.2 + 0.75\times0.8
\end{bmatrix} = \begin{bmatrix}
0.36 \\
0.64
\end{bmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The results of these calculations are exactly the proportions that we saw above: 36% alert student and 64% bored students.&lt;/p&gt;
&lt;p&gt;Now, you might ask yourself: What happens if this process continues? What happens at &lt;span class=&#34;math inline&#34;&gt;\(t+2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(t+3\)&lt;/span&gt; etc.? Will it be the case that at one point there are no bored students any more? Let’s simulate this in R and find out! Let’s call this &lt;strong&gt;tpm&lt;/strong&gt; for &lt;strong&gt;transition probability matrix&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tpm = matrix(c(0.8,0.25, 0.2,0.75), nrow=2, byrow=TRUE)
colnames(tpm) = c(&amp;#39;A&amp;#39;,&amp;#39;B&amp;#39;)
rownames(tpm) = c(&amp;#39;At+1&amp;#39;, &amp;#39;Bt+1&amp;#39;)
tpm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        A    B
## At+1 0.8 0.25
## Bt+1 0.2 0.75&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again this matrix shows that 0.8 students who were in state A at time point t will still be in state A at &lt;span class=&#34;math inline&#34;&gt;\(t+1\)&lt;/span&gt;. And 0.25 students who were in state B at time point t will be in state A at &lt;span class=&#34;math inline&#34;&gt;\(t+1\)&lt;/span&gt;. The second row has a similar interpretation for alert and bored students becoming bored at &lt;span class=&#34;math inline&#34;&gt;\(t+1\)&lt;/span&gt;. Remember that Markov processes assume fixed transition probabilities. This means that in the simulation that we’ll be doing, we leave the transition probability matrix unchanged. However, we will define a vector of the actual proportions – and these are allowed to change. In time, we expect more and more students to become alert, because the transition probability from B to A (which, to remind you, was 0.25) is higher than from A to B (which was 0.2).&lt;/p&gt;
&lt;p&gt;Let’s start our simulation by setting the initial condition as 0.1 students are alert and 0.9 students are bored and define a matrix called &lt;strong&gt;sm&lt;/strong&gt; (short for &lt;strong&gt;student matrix&lt;/strong&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sm = as.matrix(c(0.1, 0.9))
rownames(sm)= c(&amp;#39;A&amp;#39;, &amp;#39;B&amp;#39;)
sm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [,1]
## A  0.1
## B  0.9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s repeat by looping:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in 1:10){
    sm = tpm %*% sm
    }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we’re looping 10 times and on each iteration, we multiply the matrix &lt;strong&gt;tpm&lt;/strong&gt; with the student matrix &lt;strong&gt;sm&lt;/strong&gt;. We take this result and store it in &lt;strong&gt;sm&lt;/strong&gt;. This means that at the next iteration, our fixed &lt;strong&gt;transition probability matrix&lt;/strong&gt; will be multiplied by a different student matrix, allowing for the proportions to slowly change over time.&lt;br /&gt;
R operator ’%*%’ is used for matrix multiplication&lt;/p&gt;
&lt;p&gt;Outcome of our ten loop iterations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]
## At+1 0.5544017
## Bt+1 0.4455983&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, after 10 iterations of the Markov process, we now have about 55% alert students and 45% bored ones. What is interesting to me is that even though 80% of the people who are alert at one time point remain alert at the next time point, the process only converged on 55% alert and 45% bored after 10 iterations.&lt;/p&gt;
&lt;p&gt;Let’s reset our initial condition to (0.1 alert and 0.9 bored students) and run a thousand iterations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in 1:1000){
    sm = tpm %*% sm
    }
sm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]
## At+1 0.5555556
## Bt+1 0.4444444&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A 1000 iterations, and we seem to be zoning in onto ~55% and ~44%. This phenomenon is called &lt;strong&gt;Markov convergence&lt;/strong&gt;. You could run even more iterations, and your outcome would get closer and closer to 0.5555 (to infinity). So, the model converges on an equilibrium. However, this is not a fixed equilibrium. It’s not the case that the Markov process comes to a hold or that nobody changes states between alertness and boredness any more. The equilibrium that we’re dealing with here is a statistical equilibrium, where the proportions of alert and bored students remain the same. but there still is constant change (at each time step, 0.2 alert students become bored and 0.25 bored students become alert). Markov models always converge to a statistical equilibrium if the conditions (1) and (2) above are met, and if you can get from any state within your Markov model to any other state (in the case of just two states, that clearly is the case). What’s so cool about this is that it is, at first sight, fairly counterintuitive.&lt;/p&gt;
&lt;p&gt;At least when I thought about the transition probabilities for the first time, I somehow expected all students to become alert but as we saw, that’s not the case. Moreover, this process is not sensitive to initial conditions. That means that when you start with any proportion of alert or bored students (even extreme ones such as 0.0001 alert students), the process will reach the statistical equilibrium – albeit sometimes a little faster or slower. You can play around with different values for the &lt;strong&gt;sm&lt;/strong&gt; object to explore this property of Markov convergence. Another interesting thing is that the process is impervious to intervention: Say, you introduced something that made more students alert – the Markov model would quickly get back to equilibrium. So Markov processes are essentially ahistorical processes: history doesn’t matter. Even with extreme initial conditions or extreme interventions, the process quickly converges to the equilibrium defined by the transition probabilities. The only way to persistently change the system is to change the transition probabilities. Finally, what I find so cool about Markov processes is their computational simplicity.&lt;/p&gt;
&lt;div id=&#34;sources&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sources&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.bodowinter.com&#34;&gt;Bodo Winter website&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
