[{"authors":["admin"],"categories":null,"content":"Mark Goldberg is a researcher of epigenetic processes in invertebrates at Oxford Brookes University. His research interests include epigenetics, computational biology and single-cell technologies.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://suvar.netlify.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Mark Goldberg is a researcher of epigenetic processes in invertebrates at Oxford Brookes University. His research interests include epigenetics, computational biology and single-cell technologies.","tags":null,"title":"Mark Goldberg","type":"authors"},{"authors":null,"categories":["Epigenetics"],"content":"  Cross validation Validation sample Перекрёстная проверка по отдельным наблюдениям (LOOCV) k-кратная перекрёстная проверка  Бутстреп Точность оценки статистичестического параметра Точность оценки параметра регрессии    From the following examples we will learn: 1. 2. 3. Model: linear regression, kNN Dataset: Auto {ISLR}\nВ практических примерах ниже показано:\n как оценить точность модели методом перекрёстной выборки;\n методом проверочной выборки;\n методом перекрёстной проверки по отдельным наблюдениям (LOOCV);\n методом k-кратной перекрёстной проверки;\n как применять бутстреп для оценки точности статистического параметра и оценок параметров модели  Модели: линейная регрессия, kNN.\nДанные: Auto {ISLR}\nlibrary(\u0026#39;ISLR\u0026#39;) # datasets Auto library(\u0026#39;GGally\u0026#39;) # matrix diagrams ## Loading required package: ggplot2 library(\u0026#39;boot\u0026#39;) # cross-validation # data exploration head(Auto) ## mpg cylinders displacement horsepower weight acceleration year origin ## 1 18 8 307 130 3504 12.0 70 1 ## 2 15 8 350 165 3693 11.5 70 1 ## 3 18 8 318 150 3436 11.0 70 1 ## 4 16 8 304 150 3433 12.0 70 1 ## 5 17 8 302 140 3449 10.5 70 1 ## 6 15 8 429 198 4341 10.0 70 1 ## name ## 1 chevrolet chevelle malibu ## 2 buick skylark 320 ## 3 plymouth satellite ## 4 amc rebel sst ## 5 ford torino ## 6 ford galaxie 500 str(Auto) ## \u0026#39;data.frame\u0026#39;: 392 obs. of 9 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cylinders : num 8 8 8 8 8 8 8 8 8 8 ... ## $ displacement: num 307 350 318 304 302 429 454 440 455 390 ... ## $ horsepower : num 130 165 150 150 140 198 220 215 225 190 ... ## $ weight : num 3504 3693 3436 3433 3449 ... ## $ acceleration: num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : num 70 70 70 70 70 70 70 70 70 70 ... ## $ origin : num 1 1 1 1 1 1 1 1 1 1 ... ## $ name : Factor w/ 304 levels \u0026quot;amc ambassador brougham\u0026quot;,..: 49 36 231 14 161 141 54 223 241 2 ... You can check correlation of various parameters using ggpairs(Auto[, -9])\nWe will check the correlation between mpg ~ horsepower\nplot(Auto$horsepower, Auto$mpg, xlab = \u0026#39;horsepower\u0026#39;, ylab = \u0026#39;mpg\u0026#39;, pch = 21, col = rgb(0, 0, 1, alpha = 0.4), bg = rgb(0, 0, 1, alpha = 0.4)) Cross validation  Validation sample Split data to train and test sets and build model using train data.\nn \u0026lt;- nrow(Auto) # number of observation train.percent \u0026lt;- 0.5 # portion of train data attach(Auto) # to call mpg \u0026amp; horsepower instead of Auto$mpg \u0026amp; Auto$horsepower ## The following object is masked from package:ggplot2: ## ## mpg # split data into train and test set.seed(1) inTrain \u0026lt;- sample(n, n * train.percent) # plot train data plot(horsepower[inTrain], mpg[inTrain], xlab = \u0026#39;horsepower\u0026#39;, ylab = \u0026#39;mpg\u0026#39;, pch = 21, col = rgb(0, 0, 1, alpha = 0.4), bg = rgb(0, 0, 1, alpha = 0.4)) # add test data points(horsepower[-inTrain], mpg[-inTrain], pch = 21, col = rgb(1, 0, 0, alpha = 0.4), bg = rgb(1, 0, 0, alpha = 0.4)) legend(\u0026#39;topright\u0026#39;, pch = c(16, 16), col = c(\u0026#39;blue\u0026#39;, \u0026#39;red\u0026#39;), legend = c(\u0026#39;test\u0026#39;, \u0026#39;train\u0026#39;)) Build models for validataion of accuracy:\n\\[ \\hat{mpg} = f(horsepower) \\] Linear model: \\(\\hat{mpg} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot horsepower\\).\n# fit linear model for train data fit.lm.1 \u0026lt;- lm(mpg ~ horsepower, subset = inTrain) # MSE of test data mean((mpg[-inTrain] - predict(fit.lm.1, Auto[-inTrain, ]))^2) ## [1] 26.14142 Строим квадратичную модель: \\(\\hat{mpg} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot horsepower + \\hat{\\beta}_2 \\cdot horsepower^2\\).\n# присоединить таблицу с данными: названия стоблцов будут доступны напрямую attach(Auto) ## The following objects are masked from Auto (pos = 3): ## ## acceleration, cylinders, displacement, horsepower, mpg, name, ## origin, weight, year ## The following object is masked from package:ggplot2: ## ## mpg # подгонка линейной модели на обучающей выборке fit.lm.2 \u0026lt;- lm(mpg ~ poly(horsepower, 2), subset = inTrain) # считаем MSE на тестовой выборке mean((mpg[-inTrain] - predict(fit.lm.2, Auto[-inTrain, ]))^2) ## [1] 19.82259 # отсоединить таблицу с данными detach(Auto) Строим кубическую модель: \\(\\hat{mpg} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot horsepower + \\hat{\\beta}_2 \\cdot horsepower^2 + \\hat{\\beta}_3 \\cdot horsepower^3\\).\n# присоединить таблицу с данными: названия стоблцов будут доступны напрямую attach(Auto) ## The following objects are masked from Auto (pos = 3): ## ## acceleration, cylinders, displacement, horsepower, mpg, name, ## origin, weight, year ## The following object is masked from package:ggplot2: ## ## mpg # подгонка линейной модели на обучающей выборке fit.lm.3 \u0026lt;- lm(mpg ~ poly(horsepower, 3), subset = inTrain) # считаем MSE на тестовой выборке mean((mpg[-inTrain] - predict(fit.lm.3, Auto[-inTrain, ]))^2) ## [1] 19.78252 # отсоединить таблицу с данными detach(Auto) Перекрёстная проверка по отдельным наблюдениям (LOOCV) Это самый затратный в вычислительном плане метод, но и самый надёжный в плане оценки ошибки вне выборки. Попробуем применить его к линейной модели.\n# подгонка линейной модели на обучающей выборке fit.glm \u0026lt;- glm(mpg ~ horsepower, data = Auto) # считаем LOOCV-ошибку cv.err \u0026lt;- cv.glm(Auto, fit.glm) # результат: первое число -- по формуле LOOCV-ошибки, # второе -- с поправкой на смещение cv.err$delta[1] ## [1] 24.23151 Теперь оценим точность полиномиальных моделей, меняя степень, в которой стоит регрессор.\n# вектор с LOOCV-ошибками cv.err.loocv \u0026lt;- rep(0, 5) names(cv.err.loocv) \u0026lt;- 1:5 # цикл по степеням полиномов for (i in 1:5){ fit.glm \u0026lt;- glm(mpg ~ poly(horsepower, i), data = Auto) cv.err.loocv[i] \u0026lt;- cv.glm(Auto, fit.glm)$delta[1] } # результат cv.err.loocv ## 1 2 3 4 5 ## 24.23151 19.24821 19.33498 19.42443 19.03321  k-кратная перекрёстная проверка K-кратная кросс-валидация – компромисс между методом проверочной выборки и LOOCV. Оценка ошибки вне выборки ближе к правде, по сравнению с проверочной выборкой, а объём вычислений меньше, чем при LOOCV. Проведём 10-кратную кросс-валидацию моделей разных степеней.\n# оценим точность полиномиальных моделей, меняя степень # вектор с ошибками по 10-кратной кросс-валидации cv.err.k.fold \u0026lt;- rep(0, 5) names(cv.err.k.fold) \u0026lt;- 1:5 # цикл по степеням полиномов for (i in 1:5){ fit.glm \u0026lt;- glm(mpg ~ poly(horsepower, i), data = Auto) cv.err.k.fold[i] \u0026lt;- cv.glm(Auto, fit.glm, K = 10)$delta[1] } # результат cv.err.k.fold ## 1 2 3 4 5 ## 24.19329 19.29416 19.49610 19.61828 19.15289 Для сравнения напомним результаты расчёта MSE методом проверочной выборки:\nerr.test ## 1 2 3 ## 26.14142 19.82259 19.78252 Опираясь на результаты расчётов с кросс-валидацией, можно заключить, что на самом деле ошибка вне выборки у линейной модели выше, чем показывала MSE на тестовой выборке. А модели со степенями 2 и 3 на самом деле точнее, чем показывала MSE без перекрёстной проверки.\n  Бутстреп Точность оценки статистичестического параметра Пример с инвестиционным портфелем из двух активов: Portfolio {ISLR}. В наборе данных две переменных: * X – доход от актива X,\n* Y – доход от актива Y.\nПортфель инвестиций состоит из активов \\(X\\) и \\(Y\\), долю актива \\(X\\) обозначим как \\(\\alpha\\). Минимум дасперсии доходности:\n\\[ \\mathrm{Var}(\\alpha X + (1 - \\alpha) Y) \\rightarrow \\mathrm{min} \\]\n– достигается при значении параметра:\n\\[ \\alpha = \\frac{\\sigma_Y^2 - \\sigma_{XY}}{\\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{XY}} \\] Данных для оценки \\(\\hat{\\sigma_X^2}\\), \\(\\hat{\\sigma_Y^2}\\) и \\(\\hat{\\sigma_{XY}}\\) немного (100 наблюдений), поэтому применим бутстреп.\nhead(Portfolio) ## X Y ## 1 -0.8952509 -0.2349235 ## 2 -1.5624543 -0.8851760 ## 3 -0.4170899 0.2718880 ## 4 1.0443557 -0.7341975 ## 5 -0.3155684 0.8419834 ## 6 -1.7371238 -2.0371910 str(Portfolio) ## \u0026#39;data.frame\u0026#39;: 100 obs. of 2 variables: ## $ X: num -0.895 -1.562 -0.417 1.044 -0.316 ... ## $ Y: num -0.235 -0.885 0.272 -0.734 0.842 ... # функция для вычисления искомого параметра alpha.fn \u0026lt;- function(data, index){ X = data$X[index] Y = data$Y[index] (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2*cov(X, Y)) } # рассчитать alpha по всем 100 наблюдениям alpha.fn(Portfolio, 1:100) ## [1] 0.5758321 # создать бутстреп-выборку и повторно вычислить alpha set.seed(1) alpha.fn(Portfolio, sample(100, 100, replace = T)) ## [1] 0.5963833 # теперь -- многократное повторение предыдущей операции boot(Portfolio, alpha.fn, R = 1000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Portfolio, statistic = alpha.fn, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 0.5758321 -7.315422e-05 0.08861826 Бутстреп повторяет расчёт параметра много раз, делая повторные выборки из наших 100 наблюдений. В итоге этим методом можно вычислить стандартную ошибку параметра, не опираясь на допущения о законе распределении параметра. В нашем случае \\(\\alpha = 0.576\\) со стандартной ошибкой \\(s_{\\hat{\\alpha}} = 0.089\\).\n Точность оценки параметра регрессии При построении модели регрессии проблемы в остатках приводят к неверной оценке ошибок параметров. Обойти эту проблему можно, применив для расчёта этих ошибок бутстреп.\n# Оценивание точности линейной регрессионной модели ---------------------------- # оценить стандартные ошибки параметров модели # mpg = beta_0 + beta_1 * horsepower с помощью бутстрепа, # сравнить с оценками ошибок по МНК # функция для расчёта коэффициентов ПЛР по выборке из данных boot.fn \u0026lt;- function(data, index){ coef(lm(mpg ~ horsepower, data = data, subset = index)) } boot.fn(Auto, 1:n) ## (Intercept) horsepower ## 39.9358610 -0.1578447 # пример применения функции к бутстреп-выборке set.seed(1) boot.fn(Auto, sample(n, n, replace = T)) ## (Intercept) horsepower ## 38.7387134 -0.1481952 # применяем функцию boot для вычисления стандартных ошибок параметров # (1000 выборок с повторами) boot(Auto, boot.fn, 1000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Auto, statistic = boot.fn, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 39.9358610 0.0296667441 0.860440524 ## t2* -0.1578447 -0.0003113047 0.007411218 # сравним с МНК attach(Auto) ## The following objects are masked from Auto (pos = 3): ## ## acceleration, cylinders, displacement, horsepower, mpg, name, ## origin, weight, year ## The following object is masked from package:ggplot2: ## ## mpg summary(lm(mpg ~ horsepower))$coef ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 39.9358610 0.717498656 55.65984 1.220362e-187 ## horsepower -0.1578447 0.006445501 -24.48914 7.031989e-81 detach(Auto) # оценки отличаются из-за того, что МНК -- параметрический метод с допущениями # вычислим оценки параметров квадратичной модели регрессии boot.fn.2 \u0026lt;- function(data, index){ coef(lm(mpg ~ poly(horsepower, 2), data = data, subset = index)) } # применим функцию к 1000 бутсреп-выборкам set.seed(1) boot(Auto, boot.fn, 1000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Auto, statistic = boot.fn, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 39.9358610 0.0269563085 0.859851825 ## t2* -0.1578447 -0.0002906457 0.007402954 В модели регрессии, для которой проводился расчёт, похоже, не нарушаются требования к остаткам, и оценки стандартных ошибок параметров, рассчитанные по МНК, очень близки к ошибкам этих же параметров, полученных бутстрепом.\n2 Оценить стандартные ошибки параметров модели регрессии методом бутстрепа. Сравнить с оценками стандартных ошибок параметров по МНК.\nИсточники\nJames G., Witten D., Hastie T. and Tibshirani R. An Introduction to Statistical Learning with Applications in R. URL: http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf    ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"dade6611ea0ce27ea8c6ebdb04e5b8a1","permalink":"https://suvar.netlify.com/post/statistics/cross_validation_bootstrap/cross_validation_bootstrap/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/post/statistics/cross_validation_bootstrap/cross_validation_bootstrap/","section":"post","summary":"Cross validation Validation sample Перекрёстная проверка по отдельным наблюдениям (LOOCV) k-кратная перекрёстная проверка  Бутстреп Точность оценки статистичестического параметра Точность оценки параметра регрессии    From the following examples we will learn: 1. 2. 3. Model: linear regression, kNN Dataset: Auto {ISLR}\nВ практических примерах ниже показано:\n как оценить точность модели методом перекрёстной выборки;\n методом проверочной выборки;\n методом перекрёстной проверки по отдельным наблюдениям (LOOCV);","tags":["Epigenetics","PcG","Trx"],"title":"Cross-validation and bootstrap","type":"post"},{"authors":null,"categories":["Epigenetics"],"content":"  Polycomb group PcG proteins PRC1 PRC2 Pho-complex Mechanisms of inactivation of gene expression TrxG complex components PcG/TrxG response elements Nuclear architecture Open questions Glossary Cited literature   Polycomb group PcG proteins Polycomb group (PcG) proteins generally maintain gene repression, whereas Trithorax group (TrxG) proteins maintain the active gene expression.\nPcG and TrxG proteins were initially isolated in Drosophila as factors involved in maintaining the expression patterns of HOX genes, which encode transcription factors that are important determinants of patterning during embryonic development.\nIn mammals, PRCs are targeted to a subset of CpG islands (CGIs) at the promoters of developmental genes and prevent unscheduled cellular differentiation.\nIn drosophila PcG proteins form Polycomb repressive complex 1 (PRC1), Polycomb repressive complex 2 (PRC2) and Pleiohomeotic (Pho) repressive complex (PhoRC). PRC2 and PRC1 are recruited to chromatin by PhoRC which directly binds polycomb response elements (PREs).\n PRC1 PRC1 ubiquitinylates histone H2AK119 (H2AK118 in Drosophila) and alter chromatin structure whereas PRC2 trimethylates histone H3K27.\nPRC1 is composed of the core components Polycomb (Pc), Polyhomeotic (Ph), Posterior sex combs (Psc) and Sex combs extra.\nPc can bind the H3K27me3 that facilitate anchoring the complex to chromatin.\nPsc and Sce form a heterodimer, which enhances the E3 ubiquitin ligase activity of the complex. This activity is provided by the Sce subunit of\nPRC1, which monoubiquitylates H2AK118 (K119 in mammals). This ubiquitylation event is thought to restrict RNA polymerase II (Pol II) elongation, but was also shown to recruit PRC2 members.\nPh subunit is able to bind itself that promotes spreading of PRC1 complex.\n PRC2 PRC2 core complex is composed of Enhancer of zeste E(z), Suppressor of zeste 12 Su(z)12, Extra sex combs (Esc) and p55 (Nurf55 or Caf1).\nE(z) methylate H3K27.\nEsc bind H3K27me3 and facilitates multimerization of complex.\nPRC1-mediated events are also thought to compact chromatin to limit the access of activating factors and the Psc subunit in particular has been linked to this function.\np55 is present in a number of chromatin remodeling complexes and interacts with Su(z)12, H3 and H4. The loss of loss of p55 appears to have little consequence on PRC2 activity.\n Pho-complex The first complex, referred to as Pho repressive complex (PhoRC), is composed of Pho and Sfmbt. A second Pho-containing complex has also been described (Pho-INO80) that, in addition to Pho, contains the INO80 nucleosome remodeling complex (Klymenko, 2006). Pho binds DNA in a sequence-specific manner and help to recruit PcG complexes to their response elements (Grossniklaus and Paro, 2014).\n Mechanisms of inactivation of gene expression After the initial recruitment of PRC2 and PRC1 by Pho protein, Enhancer of zeste E(z), a member of PRC2, methylate H3 histone (H3K27me3) at both PREs and along the gene body. This modification is then recognized by Pc, a member of PRC1, which, in turn, ubiquitylates H2A119 via Sex combs extra (Sce), another PRC1 member, and stabilizes PRC2. The accumulation of PRC1 and PRC2 within gene bodies results in the compaction of local nucleosomes and the further silencing of the inactive genes.\n TrxG complex components TrxG include the COMPASS, COMPASS-like, TAC1 and ASH1 complexes, and SET domain HMTs.\nCommon subunits for COMPASS and COMPASS-like complexes include Ash2, Dpy30 (Dpy-30L1), Hcf1 (Hcf), Rbbp5 and Wds.\n PcG/TrxG response elements In Drosophila, the genomic nucleation sites of PcG- and TrxG-mediated epigenetic memory have been referred to as PcG/TrxG response elements (PRE/TREs).\n Nuclear architecture Nuclear architecture by PRC1 complex is disscussed in the recent review (Illingworth 2019)\n Open questions Although some PRE/TRE-like elements have been identified, it is unclear if all of the properties of the Drosophila PRE/TRE (e.g. epigenetic memory) are maintained in the mammalian system.\n How PcG and TrxG proteins are recruited to these elements?\nWe currently lack a clear understanding of the hierarchical recruitment of PcG and TrxG proteins to PRE/TREs, and elucidating these recruitment mechanisms is thus an area of active research.\n What determines the active (TRE) versus repressed (PRE) state?\n What factors d which PRC1 targets will physically interact?\n What is the impact of stoichiometry of PRC1 subunits during development?\n  This page summarizes several recent reviews (Geisler and Paro 2015) to aggregate information in this actively studied area of research.\n Glossary PRC1 - Polycomb Repressive Complex 1\nPRC2 - Polycomb Repressive Complex 2\nPho - Pleiohomeotic\nPhoRC - Pleiohomeotic Repressive Complex\nPREs - Polycomb Response Elements\nPsc - Posterior Sex Combs\nE(z) - Enhancer of zeste\nSu(z)12 - Suppressor of zeste 12\nEsc - Extra sex combs\n Cited literature Geisler, Sarah J., and Renato Paro. 2015. “Trithorax and Polycomb Group-Dependent Regulation: A Tale of Opposing Activities.” Development 142 (17): 2876–87. doi:10.1242/dev.120030.\n Illingworth, Robert S. 2019. “Chromatin Folding and Nuclear Architecture: PRC1 Function in 3d.” Current Opinion in Genetics \u0026amp; Development 55 (July): 82–90. doi:10.1016/j.gde.2019.06.006.\n   ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"31008be030b2907bb4e1c1d8c0525ca9","permalink":"https://suvar.netlify.com/post/biology/pcg_trx/pcg_trx/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/post/biology/pcg_trx/pcg_trx/","section":"post","summary":"Polycomb group PcG proteins PRC1 PRC2 Pho-complex Mechanisms of inactivation of gene expression TrxG complex components PcG/TrxG response elements Nuclear architecture Open questions Glossary Cited literature   Polycomb group PcG proteins Polycomb group (PcG) proteins generally maintain gene repression, whereas Trithorax group (TrxG) proteins maintain the active gene expression.\nPcG and TrxG proteins were initially isolated in Drosophila as factors involved in maintaining the expression patterns of HOX genes, which encode transcription factors that are important determinants of patterning during embryonic development.","tags":["Epigenetics","PcG","Trx"],"title":"Polycomb and Trithorax group proteins","type":"post"},{"authors":null,"categories":["R"],"content":" Here you can find some simple interesting concepts for coding in R.\nHow to select all numeric columns in a data frame? df = data.frame(x = c(1:3), y = c(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;), z = c(0.1, 0.2, 0.3)) df ## x y z ## 1 1 A 0.1 ## 2 2 B 0.2 ## 3 3 C 0.3 # Select all numeric columns df[sapply(df,is.numeric)] ## x z ## 1 1 0.1 ## 2 2 0.2 ## 3 3 0.3  ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"5621f0671b7114034878c191aff0f0b5","permalink":"https://suvar.netlify.com/post/programming/r_tips_tricks/r_tips_tricks/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/post/programming/r_tips_tricks/r_tips_tricks/","section":"post","summary":"Here you can find some simple interesting concepts for coding in R.\nHow to select all numeric columns in a data frame? df = data.frame(x = c(1:3), y = c(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;), z = c(0.1, 0.2, 0.3)) df ## x y z ## 1 1 A 0.1 ## 2 2 B 0.2 ## 3 3 C 0.3 # Select all numeric columns df[sapply(df,is.numeric)] ## x z ## 1 1 0.1 ## 2 2 0.","tags":["R"],"title":"R tips and coding tricks","type":"post"},{"authors":null,"categories":["Statistics","R"],"content":" Here, we will consider a simple example of Markov process with implementation in R.\nThe following example is taken from Bodo Winter website.\nA Markov process is characterized by (1) a finite set of states and (2) fixed transition probabilities between the states.\nLet’s consider an example. Assume you have a classroom, with students who could be either in the state alert or in the state bored. And then, at any given time point, there’s a certain probability of an alert student becoming bored (say 0.2), and there’s a probability of a bored student becoming alert (say 0.25).\nLet’s say there are 20 alert and 80 bored students in a particular class. This is your initial condition at time point \\(t\\). Given the transition probabilities above, what’s the number of alert and bored students at the next point in time, \\(t+1\\)?\nMultiply 20 by 0.2 (=4) and these will be the alert students that turn bored.\nAnd then multiply 80 by 0.25 (=20) and these will be the bored students that turn alert.\nSo, at \\(t+1\\), there’s going to be 20-4+20 alert students. And there’s going to be 80+4-20 bored students. Before, 80% of the students were bored and now, only 64% of the students are bored. Conversely, 36% are alert.\nA handy way of representing this Markov process is by defining a transition probability matrix:\n   A B    A\\(_{t+1}\\) 0.8 0.25  B\\(_{t+1}\\) 0.2 0.75    What this matrix says is: A proportion of 0.8 of the people who are in state A (alert) will also be at state A at time point \\(t+1\\). And, a proportion of 0.25 of the people who are in state B (bored) will switch to alert at t+1. This is what the first row says. The next row is simply one minus the probabilities of the first row, because probabilities (or proportions) have to add up to 1. Now think about multiplying this matrix with the initial proportions of alert and bored students that we had above. 0.8 are bored and 0.2 are alert. In linear algebra this would look the following way:\n \\[ \\begin{bmatrix} 0.8 \u0026amp; 0.25 \\\\ 0.2 \u0026amp; 0.75 \\end{bmatrix}\\times\\begin{bmatrix} 0.2 \\\\ 0.8 \\end{bmatrix} = \\begin{bmatrix} 0.8\\times0.2 + 0.25\\times0.8 \\\\ 0.2\\times0.2 + 0.75\\times0.8 \\end{bmatrix} = \\begin{bmatrix} 0.36 \\\\ 0.64 \\end{bmatrix} \\]\n The results of these calculations are exactly the proportions that we saw above: 36% alert student and 64% bored students.\nNow, you might ask yourself: What happens if this process continues? What happens at \\(t+2\\), \\(t+3\\) etc.? Will it be the case that at one point there are no bored students any more? Let’s simulate this in R and find out! Let’s call this tpm for transition probability matrix:\ntpm = matrix(c(0.8,0.25, 0.2,0.75), nrow=2, byrow=TRUE) colnames(tpm) = c(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;) rownames(tpm) = c(\u0026#39;At+1\u0026#39;, \u0026#39;Bt+1\u0026#39;) tpm ## A B ## At+1 0.8 0.25 ## Bt+1 0.2 0.75 Again this matrix shows that 0.8 students who were in state A at time point t will still be in state A at \\(t+1\\). And 0.25 students who were in state B at time point t will be in state A at \\(t+1\\). The second row has a similar interpretation for alert and bored students becoming bored at \\(t+1\\). Remember that Markov processes assume fixed transition probabilities. This means that in the simulation that we’ll be doing, we leave the transition probability matrix unchanged. However, we will define a vector of the actual proportions – and these are allowed to change. In time, we expect more and more students to become alert, because the transition probability from B to A (which, to remind you, was 0.25) is higher than from A to B (which was 0.2).\nLet’s start our simulation by setting the initial condition as 0.1 students are alert and 0.9 students are bored and define a matrix called sm (short for student matrix):\nsm = as.matrix(c(0.1, 0.9)) rownames(sm)= c(\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;) sm ## [,1] ## A 0.1 ## B 0.9 Now let’s repeat by looping:\nfor(i in 1:10){ sm = tpm %*% sm } Here, we’re looping 10 times and on each iteration, we multiply the matrix tpm with the student matrix sm. We take this result and store it in sm. This means that at the next iteration, our fixed transition probability matrix will be multiplied by a different student matrix, allowing for the proportions to slowly change over time.\nR operator ’%*%’ is used for matrix multiplication\nOutcome of our ten loop iterations:\nsm ## [,1] ## At+1 0.5544017 ## Bt+1 0.4455983 So, after 10 iterations of the Markov process, we now have about 55% alert students and 45% bored ones. What is interesting to me is that even though 80% of the people who are alert at one time point remain alert at the next time point, the process only converged on 55% alert and 45% bored after 10 iterations.\nLet’s reset our initial condition to (0.1 alert and 0.9 bored students) and run a thousand iterations.\nfor(i in 1:1000){ sm = tpm %*% sm } sm ## [,1] ## At+1 0.5555556 ## Bt+1 0.4444444 A 1000 iterations, and we seem to be zoning in onto ~55% and ~44%. This phenomenon is called Markov convergence. You could run even more iterations, and your outcome would get closer and closer to 0.5555 (to infinity). So, the model converges on an equilibrium. However, this is not a fixed equilibrium. It’s not the case that the Markov process comes to a hold or that nobody changes states between alertness and boredness any more. The equilibrium that we’re dealing with here is a statistical equilibrium, where the proportions of alert and bored students remain the same. but there still is constant change (at each time step, 0.2 alert students become bored and 0.25 bored students become alert). Markov models always converge to a statistical equilibrium if the conditions (1) and (2) above are met, and if you can get from any state within your Markov model to any other state (in the case of just two states, that clearly is the case). What’s so cool about this is that it is, at first sight, fairly counterintuitive.\nAt least when I thought about the transition probabilities for the first time, I somehow expected all students to become alert but as we saw, that’s not the case. Moreover, this process is not sensitive to initial conditions. That means that when you start with any proportion of alert or bored students (even extreme ones such as 0.0001 alert students), the process will reach the statistical equilibrium – albeit sometimes a little faster or slower. You can play around with different values for the sm object to explore this property of Markov convergence. Another interesting thing is that the process is impervious to intervention: Say, you introduced something that made more students alert – the Markov model would quickly get back to equilibrium. So Markov processes are essentially ahistorical processes: history doesn’t matter. Even with extreme initial conditions or extreme interventions, the process quickly converges to the equilibrium defined by the transition probabilities. The only way to persistently change the system is to change the transition probabilities. Finally, what I find so cool about Markov processes is their computational simplicity.\nSources Bodo Winter website\n ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"e28695185e4e15861510ad12bc133a6d","permalink":"https://suvar.netlify.com/post/statistics/markov_process/simple_markov_process/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/post/statistics/markov_process/simple_markov_process/","section":"post","summary":"Here, we will consider a simple example of Markov process with implementation in R.\nThe following example is taken from Bodo Winter website.\nA Markov process is characterized by (1) a finite set of states and (2) fixed transition probabilities between the states.\nLet’s consider an example. Assume you have a classroom, with students who could be either in the state alert or in the state bored. And then, at any given time point, there’s a certain probability of an alert student becoming bored (say 0.","tags":["R","Markov process","Statistics"],"title":"Simple Markov process","type":"post"}]