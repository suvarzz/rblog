[{"authors":["admin"],"categories":null,"content":"Mark Goldberg is a researcher of epigenetic processes in invertebrates at Oxford Brookes University. His research interests include epigenetics, computational biology and single-cell technologies.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Mark Goldberg is a researcher of epigenetic processes in invertebrates at Oxford Brookes University. His research interests include epigenetics, computational biology and single-cell technologies.","tags":null,"title":"Mark Goldberg","type":"authors"},{"authors":null,"categories":["Statistics"],"content":"  Statistics Variation analysis Unsupervised Learning Supervised Regression Supervised Classification Estimation of model parameters Time Series Deep Learning   Statistics  Correlation Hypothesis testing   Variation analysis  Unsupervised Learning  Hierarchical Cluster Analysis K-means Cluster Analysis Principal Component Analysis Uniform Manifold Approximation and Projection (UMAP) t-Distributed Stochastic Neighbor Embedding (tSNE)   Supervised Regression  Linear Regression Multiple regression Linear Model Selection Polynomial Regression Stepwise Regression Regularized Regression Regression Trees \u0026amp; Bagging Random Forests Imprecise Regression Lasso regression Ridge regression ElasticNet regression   Supervised Classification  Naïve Bayes Logistic Regression Multinomial logistic regression Ordinal logistic regression Linear \u0026amp; Quadratic Discriminant Analysis Support Vector Machines Random Forests and Boosting   Estimation of model parameters  Model evaluation metrics Resampling Methods   Time Series  Exploring \u0026amp; Visualizing Times Series Benchmark Methods \u0026amp; Forecast Accuracy Moving Averages Exponential Smoothing   Deep Learning  Neural Network for Regression Neural Network for Classification Learning Vector Quantization Feedforward Deep Learning with Keras \u0026amp; Tensorflow Hopfield Neural Network   ","date":1565222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565222400,"objectID":"40007c945fcb7442469516dce1a8f8e1","permalink":"/post/statistics/statistical_learning_topics/","publishdate":"2019-08-08T00:00:00Z","relpermalink":"/post/statistics/statistical_learning_topics/","section":"post","summary":"Basic topics of statistical learning.","tags":["R","Statistics","Machine Learning"],"title":"Statistical learning - topics","type":"post"},{"authors":null,"categories":["Statistics"],"content":"  tSNE Barnes-Hut t-Distributed Stochastic Neighbor Embedding Compare with Principal Component Analysis (PCA) Bibliography   tSNE library(tsne) # tSNE set.seed(5) tsne_iris = tsne(iris[,1:4], k=2, perplexity=30, max_iter=1000) # set colors colors = rainbow(length(unique(iris$Species))) names(colors) = unique(iris$Species) # plot tSNE plot(tsne_iris, t=\u0026#39;n\u0026#39;) points(tsne_iris, col=colors[iris$Species])  Barnes-Hut t-Distributed Stochastic Neighbor Embedding Rtsne package. It is faster than tsne and better separate elements in groups.\nlibrary(Rtsne) # remove duplicates iris_unique \u0026lt;- unique(iris) mx \u0026lt;- as.matrix(iris_unique[,1:4]) normalize_input(mx) set.seed(3) rtsne \u0026lt;- Rtsne(mx, dims=2, perplexity=30, theta=0.5) names(rtsne) ## [1] \u0026quot;N\u0026quot; \u0026quot;Y\u0026quot; \u0026quot;costs\u0026quot; ## [4] \u0026quot;itercosts\u0026quot; \u0026quot;origD\u0026quot; \u0026quot;perplexity\u0026quot; ## [7] \u0026quot;theta\u0026quot; \u0026quot;max_iter\u0026quot; \u0026quot;stop_lying_iter\u0026quot; ## [10] \u0026quot;mom_switch_iter\u0026quot; \u0026quot;momentum\u0026quot; \u0026quot;final_momentum\u0026quot; ## [13] \u0026quot;eta\u0026quot; \u0026quot;exaggeration_factor\u0026quot; # plot tsne plot(rtsne[[\u0026#39;Y\u0026#39;]], t=\u0026#39;n\u0026#39;) points(rtsne[[\u0026#39;Y\u0026#39;]], col=colors[iris_unique$Species])  Compare with Principal Component Analysis (PCA) # compare to PCA pca_iris = princomp(iris[,1:4])$scores[,1:2] # plot PCA plot(pca_iris, t=\u0026#39;n\u0026#39;) points(pca_iris, col=colors[iris$Species])  Bibliography  ","date":1565136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565136000,"objectID":"201c2de5e4281ef0ad0689cdaf6c0f7e","permalink":"/post/statistics/tsne/tsne/","publishdate":"2019-08-07T00:00:00Z","relpermalink":"/post/statistics/tsne/tsne/","section":"post","summary":"tSNE.","tags":["R","Statistics","tSNE"],"title":"tSNE","type":"post"},{"authors":null,"categories":["R"],"content":"  Select columns Filter rows by condition Slice rows by index Adding rows and columns Remove duplicates grouping and aggregation Operations on several data frames Join functions  Miscellaneous Adding rows and columns Remove duplicates Select Filter rows Summarize Sort Pipes Combine data ‘by’ is a common variable (primary key) to join by.  Nested If_Else if() Family of Functions Vectorize functions to columns How to … Convert empty spaces to NA Randomly select n rows  Sources    Select columns # Select columns select(A,B) # select columns by name select(c(A,B)) # select multiple columns by name select(A,B:D) # select multiple oclumns by names select(-B,-B) # exclude columns by name select(-c(A,B)) # exclude multiple columns by name select(starts_with(\u0026#39;a\u0026#39;)) # select by names of fields starting with ... select(-starts_with(\u0026#39;a\u0026#39;)) # exclude by names of fields starting with ... select(ends_with(\u0026#39;a\u0026#39;)) # ends with a prefix select(contains(\u0026#39;a\u0026#39;)) # select by names of fields contains a literal string select(matches()) # matches a regular expression select(num_range()) # numerical range like x01, x02, x03. select(one_of()) # variables in character vector select(everything()) # all variables select(A, B:D, contains(\u0026#39;foo\u0026#39;), contains(\u0026#39;bar\u0026#39;)) # Reorder columns select(A, everything()) # reorder variables, that A will be in the 1st column select(C, B, A, everything()) # reordrer columns select(C, B, A) # the same as previous # Rename columns rename(A=X) # rename column A as X  Filter rows by condition # Filter rows filter(A==1) # rows where column A is equal to 1 filter(A\u0026gt;1 \u0026amp; B\u0026lt;2) # multiple condition filter(A %in% c(1,2,3)) # select rows if A is from given vector filter(A %in% c(1,2) \u0026amp; C \u0026gt; 3) # multiple filtering AND filter(A %in% c(1, 2) | C == 3) # multiple filtering OR filter(!A %in% c(2, 3)) # filtering NOT filter(grepl(\u0026quot;a\u0026quot;, A)) # filter rorws by grep values in column A filter(is.na(A)) # filter rows if column A contains \u0026#39;NA\u0026#39; filter(!is.na(A)) # filter \u0026#39;NA\u0026#39;  Slice rows by index slice(1L) # first row slice(2:5) # 2-5 rows slice(n()) # last row # Indices must be either all positive or all negative slice(5:n()) # from 5th to the last rows slice(-5:-n()) # 1-4 rows slice(1:4) # 1-4 rows  Adding rows and columns mutate(mycol = NA) # add column \u0026#39;mycol\u0026#39; filled with \u0026#39;NA\u0026#39; mutate(mycol = A*B) # add column \u0026#39;mycol\u0026#39; as A*B mutate(mycol1, mycol2) # add multiple columns cbind(mycol = NA) # add column \u0026#39;mycol\u0026#39; rbind(myrow = NA) # add row filled with \u0026#39;NA\u0026#39;  Remove duplicates distinct() # remove duplicated rows distinct(A, .keep_all=TRUE) # remove rows when field \u0026#39;A\u0026#39; is duplicated distinct(A, B, .keep_all=TRUE) # remove rows when several columns at the same row are duplicated  grouping and aggregation df %\u0026gt;% group_by(A) %\u0026gt;% summarize(avg_b = mean(B, na.rm = TRUE))  Operations on several data frames # Compair data all_equal(x,y) # compair two data frames # Combine data intersect(x,y) # rows that appear in both x and y. union(x,y) # rows that appear in either or both x and y. setdiff(x,y) # rows that appear in x but not y. # Sort arrange(A) # sort rows by A column arrange(desc(A)) # sort by descendence of values in column A arrange(A, B) # sort by A \u0026amp; B group_by(A) # group rows by A column group_by(A = as.factor(A)) # group rows by A column group_by(A = cut(A, 3)) # group by 3 ranges from A Join functions inner_join(x, y, by = ) left_join(x, y, by = ) right_join(x, y, by = ) full_join(x, y, by = ) semi_join(x, y, by = ) anti_join(x, y, by = ) library(dplyr) a \u0026lt;- data.frame(x1=c(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;), x2=c(1,2,3)) b \u0026lt;- data.frame(x1=c(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;D\u0026#39;), x3=c(\u0026#39;T\u0026#39;,\u0026#39;F\u0026#39;,\u0026#39;T\u0026#39;)) a ## x1 x2 ## 1 A 1 ## 2 B 2 ## 3 C 3 b ## x1 x3 ## 1 A T ## 2 B F ## 3 D T # Retain only rows in both sets. dplyr::inner_join(a, b, by=\u0026#39;x1\u0026#39;) ## Warning: Column `x1` joining factors with different levels, coercing to ## character vector ## x1 x2 x3 ## 1 A 1 T ## 2 B 2 F # Join matching rows from b to a dplyr::left_join(a, b, by=\u0026#39;x1\u0026#39;) ## Warning: Column `x1` joining factors with different levels, coercing to ## character vector ## x1 x2 x3 ## 1 A 1 T ## 2 B 2 F ## 3 C 3 \u0026lt;NA\u0026gt; # Join matching rows from a to b. dplyr::right_join(a, b, by=\u0026#39;x1\u0026#39;) ## Warning: Column `x1` joining factors with different levels, coercing to ## character vector ## x1 x2 x3 ## 1 A 1 T ## 2 B 2 F ## 3 D NA T # Retain all values, all rows dplyr::full_join(a, b, by = \u0026quot;x1\u0026quot;) ## Warning: Column `x1` joining factors with different levels, coercing to ## character vector ## x1 x2 x3 ## 1 A 1 T ## 2 B 2 F ## 3 C 3 \u0026lt;NA\u0026gt; ## 4 D NA T # All rows in a that have a match in b. dplyr::semi_join(a, b, by = \u0026quot;x1\u0026quot;) ## Warning: Column `x1` joining factors with different levels, coercing to ## character vector ## x1 x2 ## 1 A 1 ## 2 B 2 # All rows in a that do not have a match in b. dplyr::anti_join(a, b, by = \u0026quot;x1\u0026quot;)  ## Warning: Column `x1` joining factors with different levels, coercing to ## character vector ## x1 x2 ## 1 C 3   Miscellaneous df %\u0026gt;% na_if(\u0026quot;\u0026quot;) # Convert empty spaces to \u0026#39;NA\u0026#39; sample_frac(df, size=0.8) # Randomly select fraction of rows sample_n(10) # Randomly select number of rows # toy data df \u0026lt;- data.frame(\u0026quot;Age\u0026quot; = c(10,15,10,15), \u0026quot;Name\u0026quot; = c(\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;, \u0026quot;C\u0026quot;, \u0026quot;B\u0026quot;), \u0026quot;Gender\u0026quot;=c(1,0,1,0)) df ## Age Name Gender ## 1 10 A 1 ## 2 15 B 0 ## 3 10 C 1 ## 4 15 B 0 Adding rows and columns df %\u0026gt;% mutate(N = NA) # add new last column \u0026#39;N\u0026#39; filled NA ## Age Name Gender N ## 1 10 A 1 NA ## 2 15 B 0 NA ## 3 10 C 1 NA ## 4 15 B 0 NA df %\u0026gt;% rbind(N = NA) # add new last row \u0026#39;N\u0026#39; filled \u0026#39;NA\u0026#39; ## Age Name Gender ## 1 10 A 1 ## 2 15 B 0 ## 3 10 C 1 ## 4 15 B 0 ## 5 NA \u0026lt;NA\u0026gt; NA  Remove duplicates df %\u0026gt;% distinct() # remove duplicated rows ## Age Name Gender ## 1 10 A 1 ## 2 15 B 0 ## 3 10 C 1 df %\u0026gt;% distinct(Age, .keep_all=TRUE) # remove rows when field \u0026#39;Age\u0026#39; is duplicated ## Age Name Gender ## 1 10 A 1 ## 2 15 B 0 df %\u0026gt;% distinct(Age, Gender, .keep_all=TRUE) # remove rows when fields \u0026#39;A\u0026#39; \u0026amp; \u0026#39;B\u0026#39; are duplicated ## Age Name Gender ## 1 10 A 1 ## 2 15 B 0  Select df %\u0026gt;% select(Name, Gender) # select fields ## Name Gender ## 1 A 1 ## 2 B 0 ## 3 C 1 ## 4 B 0 df %\u0026gt;% select(-Name, -Gender) # exclude fields ## Age ## 1 10 ## 2 15 ## 3 10 ## 4 15 df %\u0026gt;% select(-c(Name, Gender)) # same as privious ## Age ## 1 10 ## 2 15 ## 3 10 ## 4 15 df %\u0026gt;% select(starts_with(\u0026quot;A\u0026quot;)) # select names of fields: A -\u0026gt; Age ## Age ## 1 10 ## 2 15 ## 3 10 ## 4 15 select(df, -starts_with(\u0026quot;A\u0026quot;)) # select all except A -\u0026gt; Age ## Name Gender ## 1 A 1 ## 2 B 0 ## 3 C 1 ## 4 B 0 select(df, contains(\u0026quot;G\u0026quot;)) # Contains a literal string ## Age Gender ## 1 10 1 ## 2 15 0 ## 3 10 1 ## 4 15 0 select(df, ends_with(\u0026quot;r\u0026quot;)) # Ends with a prefix ## Gender ## 1 1 ## 2 0 ## 3 1 ## 4 0  # matches() Matches a regular expression # num_range() Numerical range like x01, x02, x03. # one_of() Variables in character vector. # everything() All variables. select(df, Age, everything()) # reorder variables, that Age will be in the 1st column ## Age Name Gender ## 1 10 A 1 ## 2 15 B 0 ## 3 10 C 1 ## 4 15 B 0 select(df, Gender, Age, Name, everything()) # reordrer columns ## Gender Age Name ## 1 1 10 A ## 2 0 15 B ## 3 1 10 C ## 4 0 15 B select(df, Gender, Age, Name) # the same as previous ## Gender Age Name ## 1 1 10 A ## 2 0 15 B ## 3 1 10 C ## 4 0 15 B  Filter rows filter(df, Gender==1) ## Age Name Gender ## 1 10 A 1 ## 2 10 C 1 filter(df, Age\u0026gt;10) ## Age Name Gender ## 1 15 B 0 ## 2 15 B 0 filter(df, Name %in% c(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;)) ## Age Name Gender ## 1 10 A 1 ## 2 15 B 0 ## 3 15 B 0 filter(df, Name %in% c(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;) \u0026amp; Age \u0026gt; 10) # multiple filtering AND ## Age Name Gender ## 1 15 B 0 ## 2 15 B 0 filter(df, Name %in% c(\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;) | Gender == 1) # multiple filtering OR ## Age Name Gender ## 1 10 A 1 ## 2 15 B 0 ## 3 10 C 1 ## 4 15 B 0 filter(df, !Name %in% c(\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;)) # filtering NOT ## Age Name Gender ## 1 10 C 1 filter(df, grepl(\u0026quot;A\u0026quot;, Name)) # grepl function ## Age Name Gender ## 1 10 A 1  Summarize summarize(df, avg = mean(Age), m = median(Age))\nsummarise_at(df, vars(Gender, Age), funs(n(), mean, median)) # multiple functions summarise_if(df, is.numeric, funs(n(),mean,median)) # for all numeric columns summarise_at(df, vars(Gender,Age), function(x) var(x - mean(x))) # custom function\nsummarise summarize_all # Allply funs to every column summarize_at # Apply funs to specific columns summarize_if # Apply funs to all cols of one type\n Sort arrange(df, Age) ## Age Name Gender ## 1 10 A 1 ## 2 10 C 1 ## 3 15 B 0 ## 4 15 B 0 arrange(df, desc(Age)) ## Age Name Gender ## 1 15 B 0 ## 2 15 B 0 ## 3 10 A 1 ## 4 10 C 1 arrange(df, Age, Name) ## Age Name Gender ## 1 10 A 1 ## 2 10 C 1 ## 3 15 B 0 ## 4 15 B 0 group_by(df, Age, Name) # ??? ## # A tibble: 4 x 3 ## # Groups: Age, Name [3] ## Age Name Gender ## \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 10 A 1 ## 2 15 B 0 ## 3 10 C 1 ## 4 15 B 0  Pipes df %\u0026gt;% select(Age, Name) %\u0026gt;% arrange(Age) %\u0026gt;% filter(Name %in% c(\u0026#39;C\u0026#39;,\u0026#39;B\u0026#39;)) %\u0026gt;% distinct() # select columns and sort by and select rows and remove duplicated rows ## Age Name ## 1 10 C ## 2 15 B  Combine data intersect(x, y) # Rows that appear in both x and y. union(x, y) # Rows that appear in either or both x and y. setdiff(x, y) # Rows that appear in x but not y.\n ‘by’ is a common variable (primary key) to join by. inner_join(x, y, by = ) left_join(x, y, by = ) right_join(x, y, by = ) full_join(x, y, by = ) semi_join(x, y, by = ) anti_join(x, y, by = )\nif_else(condition, true, false, missing = NULL) mydf =data.frame(x = c(1:5,NA))\n  Nested If_Else mydf %\u0026gt;% mutate(newvar= if_else(is.na(x),“I am missing”, if_else(x==1,“I am one”, if_else(x==2,“I am two”, if_else(x==3,“I am three”,“Others”))))) #TODO bind_rows() bind_cols() ntile()\n if() Family of Functions #TODO select_if mutate_if pull()\n Vectorize functions to columns mutate transmute mutate_all mutate_at add_column rename\nHow to … Convert empty spaces to NA df \u0026lt;- c(\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;d\u0026quot;) df %\u0026gt;% na_if(\u0026quot;\u0026quot;) # \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; NA \u0026quot;d\u0026quot; ## [1] \u0026quot;a\u0026quot; \u0026quot;b\u0026quot; NA \u0026quot;d\u0026quot;  Randomly select n rows df \u0026lt;- data.frame(A=seq(1:10), B=seq(.1,1,.1)) df %\u0026gt;% sample_frac(size=0.3) # Randomly select fraction of rows ## A B ## 1 1 0.1 ## 2 5 0.5 ## 3 8 0.8 sample_n(df, size, …) # Randomly select size rows slice(df # select rows by position\n  Sources   ","date":1588809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588809600,"objectID":"2b7259d11bd8f3b97ffdd3367e67e2cb","permalink":"/post/r/dplyr_tutorial/dplyr_tutorial/","publishdate":"2020-05-07T00:00:00Z","relpermalink":"/post/r/dplyr_tutorial/dplyr_tutorial/","section":"post","summary":"Dplyr - reference","tags":["R","dplyr"],"title":"Dplyr - reference","type":"post"},{"authors":null,"categories":["R","Statistics"],"content":"  Linear regression Test linear moedel  One-way ANOVA   Linear regression lm(y ~ x1 + x2 + x3) # multiple linear regression lm(log(y) ~ x) # log transformed lm(sqrt(y) ~ x) # sqrt transformed lm( y ~ log(x)) # fields transformed llm(log(y) ~ log(x)) # everything is transformed lm(y ~ .) # use all fields for regression model lm(y ~ x + 0) # forced zero intercept lm(y ~ x*k) # interaction of two variables lm(y ~ x + k + x:k) # product of xkl but without interaction lm(y ~ (x + k + ... + l)^2) # all first order interactions lm(y ~ I(x1 + x2)) # sum of variables lm(y ~ I(x1^2)) # product of variables (not interation) lm(y ~ x + I(x^2) + I(x^3)) # polynomial regression lm(y ~ poly(x,3)) # same as previous # Forward/backward stepwise regression # improve model fit \u0026lt;- lm(y ~ x1 + x2) bwd.fit \u0026lt;- step(fit, direction = \u0026#39;backward\u0026#39;) fwd.fit \u0026lt;- step(fit, direction = \u0026#39;forward\u0026#39;, scope( ~ x1 + x2)) Test linear moedel plot(m) # plot residuals car::outlier.test(m) dwtest(m) # Durbin-Watson Test of the model residuals   One-way ANOVA oneway.test(x ~ f) aov(x ~ f) anova(m1, m2) # compair two models  ","date":1588809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588809600,"objectID":"3046b36bf074c958a513e2c701a9ae3c","permalink":"/post/statistics/model_reference/models_reference/","publishdate":"2020-05-07T00:00:00Z","relpermalink":"/post/statistics/model_reference/models_reference/","section":"post","summary":"Linear regression Test linear moedel  One-way ANOVA   Linear regression lm(y ~ x1 + x2 + x3) # multiple linear regression lm(log(y) ~ x) # log transformed lm(sqrt(y) ~ x) # sqrt transformed lm( y ~ log(x)) # fields transformed llm(log(y) ~ log(x)) # everything is transformed lm(y ~ .) # use all fields for regression model lm(y ~ x + 0) # forced zero intercept lm(y ~ x*k) # interaction of two variables lm(y ~ x + k + x:k) # product of xkl but without interaction lm(y ~ (x + k + .","tags":["R","Regression"],"title":"Models - reference","type":"post"},{"authors":null,"categories":["Bioinformatics"],"content":"  Using straw tool to extract contact matrix from .hic files HiC-Pro pipeline multiHiC compare for compare of 2 Hi-C datasets (Rao et al. 2017) Joint normalization of Hi-C Difference detection   Workflow from Stansfield et al., Current Protocols in Bioinformatics, 2019\nHi-C workflow steps:\n1. mapping reads\n2. assigning fragments\n3. filtering fragments\n4. binning\n5. bin-level filtering\n6. balancing (normalization) of individual matrices\nPaired-end reads of Hi-C experiments are mapped using the single-end mode to map each read (of the pair) independently.\nThe theoretical maximum resolution of Hi-C sequencing is set by the restriction enzyme used to cut the DNA. However, most Hi-C datasets are not sequenced deeply enough to reach this theoretical maximum, and typically one of a few fixed-size resolutions are chosen for analyzing the data, including 1 Mb, 100 kb, 50 kb, 40 kb, 20 kb, 10 kb, and 5 kb.\nTwo of the more popular pipelines for aligning Hi-C data are juicer (Durand, Shamim, \u0026amp; Aiden, 2016) and HiC-Pro (Servant et al., 2015).\njuicer takes fastq files and aligns the data into .hic sparse contact maps. Alignment is based on BWA. Contact maps can be extracted from .hic files using juicer or the command line tool straw.\nUsing straw tool to extract contact matrix from .hic files \u0026lt;NONE/VC/VC_SQRT/KR\u0026gt; \u0026lt;hicFile(s)\u0026gt; \u0026lt;chr1\u0026gt;[:x1:x2] \u0026lt;chr2\u0026gt;[:y1:y2] \u0026lt;BP/FRAG\u0026gt; \u0026lt;binsize\u0026gt; VC - vanilla coverage normalization\nVC_SQRT - square root of vanilla coverage normalization\nKR - Knight-Ruiz (KR) normalization\nBP/FRAG - base pare or fragment size. Typically we use BP.\nmkdir ~/GEO cd ~/GEO # download .hic file [9 Gb] wget ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE63nnn/GSE63525/suppl/GSE63525_K562_combined_30.hic # extract raw matrix for 22 chromosome at 500-kb resolution straw NONE GSE63525_K562_combined_30.hic 22 22 BP 500000 \u0026gt; K562.chHCT116_r22.500 kb.txt txt file is in the sparse upper-triangular matrix format, containing 3 columns:\n1. start of interaction\n2. end of interaction\n3. frequency (IF)\n HiC-Pro pipeline Output 2 files (.matrix and .bed):\n* The .matrix is plain-text 3-column sparse upper-triangular matrix with the columns \\(bin_i\\) \\(bin_j\\) and \\(counts_ij\\).\n* The .bed file contains the genomic coordinates for each \\(bin_i\\) and \\(bin_j\\).\nsparse2full - convert sparse upper-triangular matrix to full contact matrix.\nhicpro2bedpe - convert alignments by HiC-Pro into BEDPE format.\nhicpro2bedpe - input .matrix and .bed and convert into sparse upper-triangular matrix.\n multiHiC compare for compare of 2 Hi-C datasets (Rao et al. 2017) mkdir ~/GEO cd ~/GEO # download .hic files from wget ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM2795nnn/GSM2795535/suppl/GSM2795535_Rao-2017-HIC001_30.hic wget ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM2795nnn/GSM2795536/suppl/GSM2795536_Rao-2017-HIC002_30.hic wget ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM2809nnn/GSM2809539/suppl/GSM2809539_Rao-2017-HIC008_30.hic wget ftp://ftp.ncbi.nlm.nih.gov/geo/samples/GSM2809nnn/GSM2809540/suppl/GSM2809540_Rao-2017-HIC009_30.hic # convert .hic to sparse contact matrix for each of 22 chromosomes for i in {1,2,8,9} do mkdir HIC00${i} for j in {1..22} do straw NONE *_Rao-2017-HIC00${i}_30.hic $j $j BP 100000 \u0026gt; HIC00${i}/HIC00${i}.NONE.chr${j}.100000.txt done done library(readr) library(data.table) library(dplyr) library(edgeR) library(BiocParallel) library(HiCcompare) library(multiHiCcompare) options(scipen = 10) # output fixed numbers # Set up parameters for reading in data chr \u0026lt;- paste0(\u0026#39;chr\u0026#39;, c(1:22)) # Chromosome names samples \u0026lt;- paste0(\u0026#39;HIC00\u0026#39;, c(1,2,8,9)) # Sample names res \u0026lt;- 100000 # Data resolution # Read data sample_list \u0026lt;- list() chr_list \u0026lt;- list() wd \u0026lt;- \u0026#39;/home/suvar/GEO/\u0026#39; for(j in 1:length(samples)) { for (i in 1:length(chr)) { chr_list[[i]] \u0026lt;- read_tsv(paste0(wd, samples[j], \u0026#39;/\u0026#39;, samples[j], \u0026#39;.NONE.\u0026#39;, chr[i], \u0026#39;.\u0026#39;, res, \u0026#39;.txt\u0026#39;), col_names = FALSE) %\u0026gt;% as.data.table() # Add column indicating the chromosome chr_list[[i]] \u0026lt;- cbind(i, chr_list[[i]]) colnames(chr_list[[i]]) \u0026lt;- c(\u0026#39;chr\u0026#39;, \u0026#39;region1\u0026#39;, \u0026#39;region2\u0026#39;, \u0026#39;IF\u0026#39;) } sample_list[[j]] \u0026lt;- chr_list chr_list \u0026lt;- list() } # Collapse separate chromosome lists into one table per sample sample_list \u0026lt;- lapply(sample_list, rbindlist) sample_list[[1]]  Joint normalization of Hi-C library(pander) # Create a Hicexp object for use by multiHiCcompare (~10 min) # Four objects are assigned into two groups rao2017 \u0026lt;- make_hicexp(data_list = sample_list, groups = c(1,1,2,2)) rao2017 # class(rao2017) # view the IF information hic_table(rao2017) # MD plots before normalization MD_hicexp(rao2017, plot.chr = 1, plot.loess = TRUE) # Normalize (~2 min) rao2017 \u0026lt;- fastlo(rao2017) # cyclic loesss normalisation is also available # Plot normalization results MD_hicexp(rao2017, plot.chr = 1, plot.loess = TRUE) # Print normalized IFs pander::pandoc.table(head(hic_table(rao2017))) library(BiocParallel) # Check how many cores are available numCores \u0026lt;- parallel::detectCores() # Set the number of cores at least one less than the total number if(Sys.info()[\u0026#39;sysname\u0026#39;] == \u0026#39;Windows\u0026#39;) { # Windows settings register(SnowParam(workers = numCores-1), default = TRUE) }else { # Unix settings register(MulticoreParam(workers = numCores-1), default = TRUE) }  Difference detection # Perform exact test (~10 min) # May use \u0026quot;parallel = TRUE\u0026quot; option to speed up computations rao2017 \u0026lt;- hic_exactTest(rao2017, parallel = TRUE) # Plot a composite MD plot with the results of a comparison MD_composite(rao2017,plot.chr = 1) # Print results as a data frame pander::pandoc.table(head(results(rao2017))) # Save the Hicexp object save(rao2017, file = paste0(wd,\u0026#39;rao2017.RDA\u0026#39;)) # To start the downstream analysis # without re-running multiHiCcompare load the saved file # load(paste0(wd,\u0026#39;rao2017.RDA\u0026#39;))  ","date":1588809600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588809600,"objectID":"0e16c73eca3d1dad69c0807e460407e4","permalink":"/post/bioinformatics/hi-c/hi-c/","publishdate":"2020-05-07T00:00:00Z","relpermalink":"/post/bioinformatics/hi-c/hi-c/","section":"post","summary":"Processing of sequencing data in R.","tags":["R","RNA-seq"],"title":"Processing of sequencing data in R","type":"post"},{"authors":null,"categories":["R"],"content":"  ggplot2   ggplot2 library(ggplot2) df \u0026lt;- data.frame(x=c(1,2), y=c(3,4)) ggplot(df, aes(x, y, other_aesthetics)) ggplot() + aesthetics aesthetics: color = geom_point geom_line geom_abline geom_area geom_bar geom_boxplot geom_vline geom_violin geom_curve geom_blank geom_count ... ggplot(mtcars, aes(x=wt, y=mpg)) + geom_point() # scatterplot qplot(wt, mpg, data=mtcars, geom=\u0026#39;line\u0026#39;) + geom_point() qplot(wt, mpg, data=mtcars, geom=c(\u0026#39;line\u0026#39;, \u0026#39;point\u0026#39;)) # boxplot qplot(as.factor(cyl), mpg, data=mtcars, geom=\u0026#39;boxplot\u0026#39;) qplot(as.factor(cyl), mpg, data=mtcars, geom=c(\u0026#39;boxplot\u0026#39;, \u0026#39;point\u0026#39;)) # histogram qplot(mpg, data=mtcars, geom=\u0026quot;histogram\u0026quot;) qplot(mpg, data=mtcars, geom=\u0026quot;histogram\u0026quot;) + geom_vline(xintercept = median(mtcars$mpg), color=\u0026#39;red\u0026#39;)  ","date":1566000000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566000000,"objectID":"914ed6c1ce6a49dcbf42da04964cf4c6","permalink":"/post/r/r_graphic/r_graphic/","publishdate":"2019-08-17T00:00:00Z","relpermalink":"/post/r/r_graphic/r_graphic/","section":"post","summary":"R graphic - reference","tags":["R","tidyverse"],"title":"R graphic - reference","type":"post"},{"authors":null,"categories":["R"],"content":"  readr - read and write data Read data Write data  tibble - advanced data frames tidyr - operations with tables magrittr - pipelines purrr - functional programming dplyr - data manipulation stringr forcats lubridate broom \u0026amp; modelr   library(tidyverse) ggplot2 tibble tidyr readr purrr dplyr stringr forcats\nreadr - read and write data Read data library(readr) read_delim() # delim not specified read_tsv() # tab-separated read_csv() # comma-separated read_csv2() # semicolon-separated read_table() # space-sepatated (fixed length columns) read_table2() # space-separated (variable length columns) read_file() read_file_raw() read_log() read_csv_chunked() read_csv2_chunked() read_delim_chunked() read_lines_chunked() read_raw_chunked() read_delim(file, delim, quote = \u0026quot;\\\u0026quot;\u0026quot;, escape_backslash = FALSE, escape_double = TRUE, col_names = TRUE, col_types = NULL, locale = default_locale(), na = c(\u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;), quoted_na = TRUE, comment = \u0026quot;\u0026quot;, trim_ws = FALSE, skip = 0, n_max = Inf, guess_max = min(1000, n_max), progress = show_progress(), skip_empty_rows = TRUE) col_types c | col_character() i | col_integer() n | col_number() d | col_double() l | col_logical() f | col_factor() d | col_date() t | col_time() ? | col_guess() # default _ - | cal_skip() # skip the column col_types = \u0026quot;clnt\u0026quot; col_types = cols(A = col_character()) = cols(A = col_factor(levels = c(\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;))) # specify factor = cols(A = col_factor(ordered = TRUE)) # order factor locale = locale(date_names = \u0026quot;en\u0026quot;, date_format = \u0026quot;%AD\u0026quot;, time_format = \u0026quot;%AT\u0026quot;, decimal_mark = \u0026quot;.\u0026quot;, grouping_mark = \u0026quot;,\u0026quot;, tz = \u0026quot;UTC\u0026quot;, encoding = \u0026quot;UTF-8\u0026quot;, asciify = FALSE) date_format(format = \u0026quot;%Y-%m-%d\u0026quot;, tz = \u0026quot;UTC\u0026quot;) time_format(format = \u0026quot;%H:%M:%S\u0026quot;, tz = \u0026quot;UTC\u0026quot;) %d # day %m # month (digits) %B # month full name - Januar, ... %b # month abbr. - Jan, Feb, ... %Y # year 2006 %y # year 06 %H # 24 hours format %I # 12 hours format %M # minutes %p # am/pm %S # second %Z # time zone as name \u0026#39;Berlin\u0026#39; %z # ime zone \u0026#39;UTC\u0026#39; Preset values: %D %m/%d/%y %x %y/%m/%d %F %Y-%m-%d %R %H:%M %T %H:%M:%S parse_character() parse_date() parse_datetime() parse_double() parse_factor() parse_guess() parse_number() parse_logical() parse_vector() parse_date(c(\u0026quot;1975/02/05\u0026quot;), format = \u0026quot;%Y/%m/%d\u0026quot;)  Write data write_delim() write_tsv() write_csv() write_csv2() write_excel_csv() write_excel_csv2() write_lines() write_rds() write_file() write_delim(x, path, delim = \u0026quot; \u0026quot;, na = \u0026quot;NA\u0026quot;, append = FALSE, col_names = !append, quote_escape = \u0026quot;double\u0026quot;)   tibble - advanced data frames library(tibble) x\u0026lt;- tibble(a = c(1,2), b = c(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;)) x \u0026lt;- tribble(~a,~b,~c, 1,2,3, \u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;) as_tibble(x) as.data.frame(x) print(x, n=2, width=5) x[1] # get column by id x[\u0026#39;A\u0026#39;] # get column by column name x$A # get column x[1:5] # get columns x[c(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;)] # get columns x[[1]] # get column as vector x[[\u0026#39;A\u0026#39;]] # get column as vector x[row, column] # subset x[5, 10] # get value row 5 and column 10 x[1:5, 10:20] # subset tibble x[1:3,] # get 3 rows x[,1:3] # get 3 columns  tidyr - operations with tables gather() spread() separate() extract() unite() expand() crossing() complete() drop_na() replace_na() fill() nest() unnest()  magrittr - pipelines df %\u0026gt;% f() %\u0026gt;% g() %\u0026gt;% h() df %\u0026gt;% f %\u0026gt;% g %\u0026gt;% h df %\u0026lt;\u0026gt;% f # df \u0026lt;- df %\u0026gt;% f df %$% { a - b} # to refer df columns by names df %T\u0026gt;% f # return df df %\u0026gt;% calculate %T\u0026gt;% print %T\u0026gt;% plot %\u0026gt;% calculate  purrr - functional programming keep() # true subset by result of logical function discard() # false subset by result of logical function compact() # remove empty elements f \u0026lt;- function(x) x%%2 == 0 1:4 %\u0026gt;% keep(f) # 2 4 1:4 %\u0026gt;% discard(f) # 1 3 1:4 %\u0026gt;% keep(negate(f)) # 1 3 1:4 %\u0026gt;% map(f) # return logicals in list 1:4 %\u0026gt;% map_lgl(f) # return logicals in vector map() map_lgl() map_dbl() map_chr() map_call() map_at() map_if() map_depth() map_int() map_dfr() # return vector of input length lmap() lmap_at() lmap_if() pmap() pmap_chr() pmap_dbl() pmap_df() pmap_dfc() pmap_dfr() pmap_int() pmap_lgl() pmap_raw() # lambda function f \u0026lt;- function(x) x \u0026gt; 2 1:4 %\u0026gt;% keep(f) # 3 4 1:4 %\u0026gt;% keep(function(x) x \u0026gt; 2) # 3 4 1:4 %\u0026gt;% keep(~ .x \u0026gt; 2) # 3 4 list(1:3, 1:3, 1:3) %\u0026gt;% pmap_dbl(~ .1 + .2 + .3) # 0.6 0.6 0.6  dplyr - data manipulation  stringr  forcats  lubridate  broom \u0026amp; modelr  ","date":1565913600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565913600,"objectID":"769c222eb90fef129632ce0779ddcdbb","permalink":"/post/r/tidyverse_reference/tidyverse_reference/","publishdate":"2019-08-16T00:00:00Z","relpermalink":"/post/r/tidyverse_reference/tidyverse_reference/","section":"post","summary":"R tidyverse package - reference","tags":["R","tidyverse"],"title":"R tidyverse package - reference","type":"post"},{"authors":null,"categories":["Biology"],"content":"  How to select all numeric columns in a data frame?   Here you can find some simple interesting concepts for coding in R.\nHow to select all numeric columns in a data frame? df = data.frame(x = c(1:3), y = c(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;), z = c(0.1, 0.2, 0.3)) df ## x y z ## 1 1 A 0.1 ## 2 2 B 0.2 ## 3 3 C 0.3 # Select all numeric columns df[sapply(df,is.numeric)] ## x z ## 1 1 0.1 ## 2 2 0.2 ## 3 3 0.3  ","date":1565913600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565913600,"objectID":"64030f47a55c38c28dac55432184221c","permalink":"/post/r/r_tips_tricks/r_tips_tricks/","publishdate":"2019-08-16T00:00:00Z","relpermalink":"/post/r/r_tips_tricks/r_tips_tricks/","section":"post","summary":"Here you can find some simple interesting concepts for coding in R.","tags":["GWAS"],"title":"R tips and coding tricks","type":"post"},{"authors":null,"categories":["Bioinformatics"],"content":"  SNP analysis using SNPasoc R package GWAS using PLINK   SNP analysis using SNPasoc R package Example demonstrate an association test for an illness for one single SNP.\ninstall.packages(\u0026quot;SNPassoc\u0026quot;) library(\u0026quot;SNPassoc\u0026quot;) data(SNPs) head(SNPs) head(SNPs.info.pos) # select 6-40 SNP and create SNP object mySNP \u0026lt;- setupSNP(SNPs, 6:40, sep=\u0026quot;\u0026quot;) # casco - 1 for case, 0 for control) mySNP # association test res \u0026lt;- association(casco~sex+snp10001+blood.pre, data = mySNP, model.interaction = c(\u0026quot;dominant\u0026quot;,\u0026quot;codominant\u0026quot;)) res 0 - control sample size\n% - percent for each variant\n1 - case sample size\n% - percent for each varian\nOR - odd ratio\nlower/upper - 95% confidence interval for odd ratio\np-value of likelihood ratio test\nAIC - Akaike Information Criterion\n# association scan for SNPs - separately for all models res \u0026lt;- WGassociation(protein, data = mySNP, model = \u0026#39;all\u0026#39;) # same formula as protein~1, # p-values for dominant model dominant(res) # p-values for recessive model recessive(res) # complete statistics WGstats(res) summary(res) # Plot p-values for all models plot(res) # whole genome association - one log model resHapMap \u0026lt;- WGassociation(protein, data= mySNP, model=\u0026#39;log\u0026#39;) plot(resHapMap) Another examplw for all genome association # two population groups (CEU and YRI), 60 samples for each group data(HapMap) str(HapMap) str(HapMap.SNPs.pos) # SNP class object myHapMap \u0026lt;- setupSNP(HapMap, colSNPs=3:9307, sort=TRUE, info=HapMap.SNPs.pos, sep=\u0026quot;\u0026quot;) # association for dominant model myHapMapres \u0026lt;- WGassociation(group, data= myHapMap, model=\u0026quot;dominant\u0026quot;) head(myHapMapres) print(myHapMapres) # plot association for all chromosomes plot(myHapMapres, whole=TRUE)  GWAS using PLINK The PLINK format of the GWAS data consists of two separate files, one containing the SNP information (.ped)and the other containing the mapping information (.map). For dependence analysis, it can be combined with the phenotype data separately.\n ","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565740800,"objectID":"8e4782f5fc51d806418976e4416b2a60","permalink":"/post/bioinformatics/gwas/gwas/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/post/bioinformatics/gwas/gwas/","section":"post","summary":"Genome Wide Associated Studies (GWAS)","tags":["R","GWAS"],"title":"Genome Wide Associated Studies (GWAS)","type":"post"},{"authors":null,"categories":["Biology"],"content":"  GWAS Polygenic score Tools for PRS calculation  Single-nucleotide polymorphism Genetic association Expression quantitative trait locus (eQTL)  SNP genotyping Expression quantitative trait loci Odds ratio Risk ratio FEV1/FVC ratio    GWAS To correctly account for the large number of statistical tests in genome-wide studies, a significance level of 5 × 10^8 was shown to effectively control type-I error rate.\n Polygenic score Polygenic score also known as genetic risk score and polygenic risc score is an individual‐level score that is calculated based on variation in multiple genetic loci and their associated weights, derived from GWAS (genetic markers, usually SNPs). Other words, the number of risk variants that a person carries, weighted by SNP effect sizes that are derived from an independent large‐scaled discovery GWAS.\nIt serves as the best prediction for the trait that can be made when taking into account variation in multiple genetic variants.\nPolygenic risc score (PRS) combines the effect sizes of multiple SNPs into a single aggregated score that can be used to predict disease risk.\nThe scores can be used in a (logistic) regression analysis to predict any trait that is expected to show genetic overlap with the trait of interest.\nThe prediction accuracy can be expressed with the (pseudo‐)\\(R^2\\) measure of the regression analysis.\nIt is important to include at least a few MDS components as covariates in the regression analysis to control for population stratification.\nTo estimate how much variation is explained by the PRS, the \\(R^2\\) of a model that includes only the covariates (e.g., MDS components) and the R 2 of a model that includes covariates + PRS will be compared. The increase in R 2 due to the PRS indicates the increase in prediction accuracy explained by genetic risk factors.\nThe prediction accuracy of PRS depends mostly on the (co‐)heritability of the analysed traits, the number of SNPs, and the size of the discovery sample. The size of the target sample only affects the reliability of R 2 and typically a few thousand of subjects in the target sample are sufficient to achieve a significant R 2 if the (co‐)heritability of the trait(s) of interest and the sample size of the discovery sample used are sufficiently large.\nTools for PRS calculation  POLYGENE script\n PRSice | Tutorial for PRsice\n PLINK ‐‐score  Effect sizes are estimated for each marker’s association with the trait of interest.\nThese weights are then used to assign individualized polygenic scores in an independent replication sample. The estimated score, \\(\\hat {S}\\), generally follows the form:\n\\(\\hat S = \\sum_{j=1}^{m} X_j \\hat\\beta_j\\), where\n\\(\\hat S\\) of an individual is equal to the weighted sum of the individual’s marker genotypes,\n\\(X_{j}\\), at \\(m\\) SNPs (Dudbridge, 2013 Plos genetics). Weights are estimated using regression analysis.\nNaïve methods\nThe simplest so-called “naïve” method of construction sets weights equal to the coefficient estimates from a regression of the trait on each genetic variant. The included SNPs may be selected using an algorithm that attempts to ensure that each marker is approximately independent. Failing to account for non-random association of genetic variants will typically reduce the score’s predictive accuracy. This is important because genetic variants are often correlated with other nearby variants, such that the weight of a causal variant will be attenuated if it is more strongly correlated with its neighbors than a null variant. This is called linkage disequilibrium, a common phenomenon that arises from the shared evolutionary history of neighboring genetic variants. Further restriction can be achieved by multiple-testing different sets of SNPs selected at various thresholds, such as all SNPs which are genome-wide statistically-significant hits or all SNPs p \u0026lt; 0.05 or all SNPs with p \u0026lt; 0.50, and the one with greatest performance used for further analysis; especially for highly polygenic traits, the best polygenic score will tend to use most or all SNPs. (Ware, E. B.; et al. (2017). “Heterogeneity in polygenic scores for common human traits”. BioRxiv. doi:10.1101/106062.)\nBayesian methods\nBayesian approaches, originally pioneered in concept in 2001, attempt to explicitly model preexisting genetic architecture, thereby accounting for the distribution of effect sizes with a prior that should improve the accuracy of a polygenic score. One of the most popular modern Bayesian methods uses “linkage disequilibrium prediction” (LDpred for short) to set the weight for each SNP equal to the average of its posterior distribution after linkage disequilibrium has been accounted for. LDpred tends to outperform simpler methods of pruning and thresholding, especially at large sample sizes; for example, its estimations have improved the predicted variance of a polygenic score for schizophrenia in a large data set from 20.1% to 25.3%. (Vilhjálmsson, 2015)\nPenalized regression\nPenalized regression methods, such as LASSO and ridge regression, can also be used to improve the accuracy of polygenic scores. Penalized regression can be interpreted as placing informative prior probabilities on how many genetic variants are expected to affect a trait, and the distribution of their effect sizes. In other words, these methods in effect “penalize” the large coefficients in a regression model and shrink them conservatively. Ridge regression accomplishes this by shrinking the prediction with a term that penalizes the sum of the squared coefficients. LASSO accomplishes something similar by penalizing the sum of absolute coefficients. Bayesian counterparts exist for LASSO and ridge regression, and other priors have been suggested and used. They can perform better in some circumstances. A multi-dataset, multi-method study found that of 15 different methods compared across four datasets, minimum redundancy maximum relevance was the best performing method. Furthermore, variable selection methods tended to outperform other methods. Variable selection methods do not use all the available genomic variants present in a dataset, but attempt to select an optimal subset of variants to use. This leads to less overfitting but more bias (see bias-variance tradeoff).\nPredictive validity\nThe benefit of polygenic score is that they can be used to predict the future. This has large practical benefits for animal breeding because it increases the selection precision and allows for shorter generations, both of which speed up evolution. For humans, it can be used to predict future disease susceptibility and for embryo selection.\nWikipedia: Polygenic score\nGenetic correlation\nGenetic correlation\n  Single-nucleotide polymorphism SNP is a substitution of a single nucleotide that occures at a specific position in the genome, where each variation is present to some appreciable degree within a population (e.g. \u0026gt; 1%). SNPs underline differences in our susceptibility to a wide range of diseases. More than 335 million SNPs have been found across humans from multiple populations. A typical genome differs from the reference human genome at 4 to 5 million sites, most of which (more than 99.9%) consist of SNPs and short indels.\nsingle-nucleotide variant (SNV) is a variation in a single nucleotide without any limitations of frequency and may arise in somatic cells. A somatic single-nucleotide variation (e.g., caused by cancer) may also be called a single-nucleotide alteration.\nSNPs in the coding region are of two types: synonymous and nonsynonymous SNPs. Synonymous SNPs do not affect the protein sequence, while nonsynonymous SNPs change the amino acid sequence of protein. The nonsynonymous SNPs are of two types: missense and nonsense. missense – single change in the base results in change in amino acid of protein and its malfunction which leads to disease.\nnonsense – point mutation in a sequence of DNA that results in a premature stop codon, or a nonsense codon in the transcribed mRNA, and in a truncated, incomplete, and usually nonfunctional protein product.\nDatabases As there are for genes, bioinformatics databases exist for SNPs.\ndbSNP is a SNP database from the National Center for Biotechnology Information (NCBI). As of June 8, 2015, dbSNP listed 149,735,377 SNPs in humans.[35][36] Kaviar[37] is a compendium of SNPs from multiple data sources including dbSNP. SNPedia is a wiki-style database supporting personal genome annotation, interpretation and analysis. The OMIM database describes the association between polymorphisms and diseases (e.g., gives diseases in text form) dbSAP – single amino-acid polymorphism database for protein variation detection[38] The Human Gene Mutation Database provides gene mutations causing or associated with human inherited diseases and functional SNPs The International HapMap Project, where researchers are identifying Tag SNPs to be able to determine the collection of haplotypes present in each subject. GWAS Central allows users to visually interrogate the actual summary-level association data in one or more genome-wide association studies.\nSNPs that are not in protein-coding regions may still affect gene splicing, transcription factor binding, messenger RNA degradation, or the sequence of noncoding RNA. Gene expression affected by this type of SNP is referred to as an eSNP (expression SNP) and may be upstream or downstream from the gene.\nWiki: SNP\nAssociation studies can determine whether a genetic variant is associated with a disease or trait.[6] A tag SNP is a representative single-nucleotide polymorphism in a region of the genome with high linkage disequilibrium (the non-random association of alleles at two or more loci). Tag SNPs are useful in whole-genome SNP association studies, in which hundreds of thousands of SNPs across the entire genome are genotyped. Haplotype mapping: sets of alleles or DNA sequences can be clustered so that a single SNP can identify many linked SNPs. Linkage disequilibrium (LD), a term used in population genetics, indicates non-random association of alleles at two or more loci, not necessarily on the same chromosome. It refers to the phenomenon that SNP allele or DNA sequence that are close together in the genome tend to be inherited together. LD is affected by two parameters: 1) The distance between the SNPs [the larger the distance, the lower the LD]. 2) Recombination rate [the lower the recombination rate, the higher the LD].[7]\nGenetic association Wiki: Genetic association\nGenetic association is when one or more genotypes within a population co-occur with a phenotypic trait more often than would be expected by chance occurrence.\nStudies of genetic association aim to test whether single-locus alleles or genotype frequencies (or more generally, multilocus haplotype frequencies) differ between two groups of individuals (usually diseased subjects and healthy controls). Genetic association studies today are based on the principle that genotypes can be compared “directly”, i.e. with the sequences of the actual genomes or exomes via whole genome sequencing or whole exome sequencing. Before 2010, DNA sequencing methods were used. Genetic association can be between phenotypes, such as visible characteristics such as flower colour or height, between a phenotype and a genetic polymorphism, such as a single nucleotide polymorphism (SNP), or between two genetic polymorphisms. Association between genetic polymorphisms occurs when there is non-random association of their alleles as a result of their proximity on the same chromosome; this is known as genetic linkage.\nLinkage disequilibrium (LD) is a term used in the study of population genetics for the non-random association of alleles at two or more loci, not necessarily on the same chromosome. It is not the same as linkage, which is the phenomenon whereby two or more loci on a chromosome have reduced recombination between them because of their physical proximity to each other. LD describes a situation in which some combinations of alleles or genetic markers occur more or less frequently in a population than would be expected from a random formation of haplotypes from alleles based on their frequencies.\nGenetic association studies are performed to determine whether a genetic variant is associated with a disease or trait: if association is present, a particular allele, genotype or haplotype of a polymorphism or polymorphisms will be seen more often than expected by chance in an individual carrying the trait. Thus, a person carrying one or two copies of a high-risk variant is at increased risk of developing the associated disease or having the associated trait.\n Expression quantitative trait locus (eQTL) SNP prediciton tools: SIFT This program provides insight into how a laboratory induced missense or nonsynonymous mutation will affect protein function based on physical properties of the amino acid and sequence homology. LIST[48] (Local Identity and Shared Taxa) estimates the potential deleteriousness of mutations resulted from altering their protein functions. It is based on the assumption that variations observed in species closely related to human are more significant when assessing conservation compared to those in distantly related species. SNAP2 SuSPect PolyPhen-2 PredictSNP MutationTaster: official website Variant Effect Predictor from the Ensembl project SNPViz[49] This program provides a 3D representation of the protein affected, highlighting the amino acid change so doctors can determine pathogenicity of the mutant protein. PROVEAN PhyreRisk is a database which maps variants to experimental and predicted protein structures.[50] Missense3D is a tool which provides a stereochemical report on the effect of missense variants on protein structure.[51]\n  SNP genotyping SNP genotyping is the measurement of genetic variations of single nucleotide polymorphisms (SNPs) between members of a species. It is a form of genotyping, which is the measurement of more general genetic variation.\nProbabilistic methods for variant calling are based on Bayes’ Theorem. In the context of variant calling, Bayes’ Theorem defines the probability of each genotype being the true genotype given the observed data, in terms of the prior probabilities of each possible genotype, and the probability distribution of the data given each possible genotype. The formula is:\n\\(P(G|D) = \\frac{P(D|G)P(G)}{P(D)} = \\frac {P(D|G)P(G)}{\\sum_{i=1}^{n} P(D|G_i) P(G_i)\\) In the above equation:\n\\(D\\) refers to the observed data; that is, the aligned reads \\(G\\) is the genotype whose probability is being calculated \\(G_i\\) refers to the ith possible genotype, out of n possibilities Given the above framework, different software solutions for detecting SNVs vary based on how they calculate the prior probabilities {P(G)} P(G), the error model used to model the probabilities {P(D|G)} P(D|G), and the partitioning of the overall genotypes into separate sub-genotypes, whose probabilities can be individually estimated in this framework.\nPrior genotype probability estimation The calculation of prior probabilities depends on available data from the genome being studied, and the type of analysis being performed. For studies where good reference data containing frequencies of known mutations is available (for example, in studying human genome data), these known frequencies of genotypes in the population can be used to estimate priors. Given population wide allele frequencies, prior genotype probabilities can be calculated at each locus according to the Hardy Weinberg Equilibrium.[6] In the absence of such data, constant priors can be used, independent of the locus. These can be set using heuristically chosen values, possibly informed by the kind of variations being sought by the study. Alternatively, supervised machine-learning procedures have been investigated that seek to learn optimal prior values for individuals in a sample, using supplied NGS data from these individuals.\nWiki: SNV calling from NGS data\nHardy–Weinberg principle - The Hardy–Weinberg principle, also known as the Hardy–Weinberg equilibrium, model, theorem, or law, states that allele and genotype frequencies in a population will remain constant from generation to generation in the absence of other evolutionary influences. These influences include genetic drift, mate choice, assortative mating, natural selection, sexual selection, mutation, gene flow, meiotic drive, genetic hitchhiking, population bottleneck, founder effect and inbreeding.\nIn the simplest case of a single locus with two alleles denoted A and a with frequencies f(A) = p and f(a) = q, respectively, the expected genotype frequencies under random mating are f(AA) = p2 for the AA homozygotes, f(aa) = q2 for the aa homozygotes, and f(Aa) = 2pq for the heterozygotes. In the absence of selection, mutation, genetic drift, or other forces, allele frequencies p and q are constant between generations, so equilibrium is reached.\nThe principle is named after G. H. Hardy and Wilhelm Weinberg, who first demonstrated it mathematically. Hardy’s paper was focused on debunking the then-commonly held view that a dominant allele would automatically tend to increase in frequency; today, confusion between dominance and selection is less common. Today, tests for Hardy-Weinberg genotype frequencies are used primarily to test for population stratification and other forms of non-random mating.\nWiki: Hardy-Weinberg principle\nExpression quantitative trait loci Expression quantitative trait loci (eQTLs) are genomic loci that explain all or a fraction of variation in expression levels of mRNAs.\nDistant and local, trans- and cis-eQTLs, respectively Expression traits differ from most other classical complex traits in one important respect—the measured mRNA or protein trait is almost always the product of a single gene with a specific chromosomal location. eQTLs that map to the approximate location of their gene-of-origin are referred to as local eQTLs. In contrast, those that map far from the location of their gene of origin, often on different chromosomes, are referred to as distant eQTLs. Often, these two types of eQTLs are referred to as cis and trans, respectively, but these terms are best reserved for instances when the regulatory mechanism (cis vs. trans) of the underlying sequence has been established. The first genome-wide study of gene expression was carried out in yeast and published in 2002.[2] The initial wave of eQTL studies employed microarrays to measure genome-wide gene expression; more recent studies have employed massively parallel RNA sequencing. Many expression QTL studies were performed in plants and animals, including humans[3], non-human primates[4][5] and mice[6].\nSome cis eQTLs are detected in many tissue types but the majority of trans eQTLs are tissue-dependent (dynamic).[7] eQTLs may act in cis (locally) or trans (at a distance) to a gene.[8] The abundance of a gene transcript is directly modified by polymorphism in regulatory elements. Consequently, transcript abundance might be considered as a quantitative trait that can be mapped with considerable power. These have been named expression QTLs (eQTLs).[9] The combination of whole-genome genetic association studies and the measurement of global gene expression allows the systematic identification of eQTLs. By assaying gene expression and genetic variation simultaneously on a genome-wide basis in a large number of individuals, statistical genetic methods can be used to map the genetic factors that underpin individual differences in quantitative levels of expression of many thousands of transcripts.[10] Studies have shown that single nucleotide polymorphisms (SNPs) reproducibly associated with complex disorders [11] as well as certain pharmacologic phenotypes [12] are found to be significantly enriched for eQTLs, relative to frequency-matched control SNPs.\nDetecting eQTLs Mapping eQTLs is done using standard QTL mapping methods that test the linkage between variation in expression and genetic polymorphisms. The only considerable difference is that eQTL studies can involve a million or more expression microtraits. Standard gene mapping software packages can be used, although it is often faster to use custom code such as QTL Reaper or the web-based eQTL mapping system GeneNetwork. GeneNetwork hosts many large eQTL mapping data sets and provide access to fast algorithms to map single loci and epistatic interactions. As is true in all QTL mapping studies, the final steps in defining DNA variants that cause variation in traits are usually difficult and require a second round of experimentation. This is especially the case for trans eQTLs that do not benefit from the strong prior probability that relevant variants are in the immediate vicinity of the parent gene. Statistical, graphical, and bioinformatic methods are used to evaluate positional candidate genes and entire systems of interactions.\nClumping: This is a procedure in which only the most significant SNP (i.e., lowest p value) in each LD block is identified and selected for further analyses. This reduces the correlation between the remaining SNPs, while retaining SNPs with the strongest statistical evidence.\nCo‐heritability: This is a measure of the genetic relationship between disorders. The SNP‐based co‐heritability is the proportion of covariance between disorder pairs (e.g., schizophrenia and bipolar disorder) that is explained by SNPs.\nGene: This is a sequence of nucleotides in the DNA that codes for a molecule (e.g., a protein)\nHeterozygosity: This is the carrying of two different alleles of a specific SNP. The heterozygosity rate of an individual is the proportion of heterozygous genotypes. High levels of heterozygosity within an individual might be an indication of low sample quality whereas low levels of heterozygosity may be due to inbreeding.\nIndividual‐level missingness: This is the number of SNPs that is missing for a specific individual. High levels of missingness can be an indication of poor DNA quality or technical problems.\nLinkage disequilibrium (LD): This is a measure of non‐random association between alleles at different loci at the same chromosome in a given population. SNPs are in LD when the frequency of association of their alleles is higher than expected under random assortment. LD concerns patterns of correlations between SNPs.\nMinor allele frequency (MAF): This is the frequency of the least often occurring allele at a specific location. Most studies are underpowered to detect associations with SNPs with a low MAF and therefore exclude these SNPs.\nPopulation stratification: This is the presence of multiple subpopulations (e.g., individuals with different ethnic background) in a study. Because allele frequencies can differ between subpopulations, population stratification can lead to false positive associations and/or mask true associations. An excellent example of this is the chopstick gene, where a SNP, due to population stratification, accounted for nearly half of the variance in the capacity to eat with chopsticks (Hamer \u0026amp; Sirota, 2000).\nPruning: This is a method to select a subset of markers that are in approximate linkage equilibrium. In PLINK, this method uses the strength of LD between SNPs within a specific window (region) of the chromosome and selects only SNPs that are approximately uncorrelated, based on a user‐specified threshold of LD. In contrast to clumping, pruning does not take the p value of a SNP into account.\nRelatedness: This indicates how strongly a pair of individuals is genetically related. A conventional GWAS assumes that all subjects are unrelated (i.e., no pair of individuals is more closely related than second‐degree relatives). Without appropriate correction, the inclusion of relatives could lead to biased estimations of standard errors of SNP effect sizes. Note that specific tools for analysing family data have been developed.\nSex discrepancy: This is the difference between the assigned sex and the sex determined based on the genotype. A discrepancy likely points to sample mix‐ups in the lab. Note, this test can only be conducted when SNPs on the sex chromosomes (X and Y) have been assessed.\nSingle nucleotide polymorphism (SNP): This is a variation in a single nucleotide (i.e., A, C, G, or T) that occurs at a specific position in the genome. A SNP usually exists as two different forms (e.g., A vs. T). These different forms are called alleles. A SNP with two alleles has three different genotypes (e.g., AA, AT, and TT).\nSNP‐heritability: This is the fraction of phenotypic variance of a trait explained by all SNPs in the analysis.\nSNP‐level missingness: This is the number of individuals in the sample for whom information on a specific SNP is missing. SNPs with a high level of missingness can potentially lead to bias.\nSummary statistics: These are the results obtained after conducting a GWAS, including information on chromosome number, position of the SNP, SNP(rs)‐identifier, MAF, effect size (odds ratio/beta), standard error, and p value. Summary statistics of GWAS are often freely accessible or shared between researchers.\nThe Hardy–Weinberg (dis)equilibrium (HWE) law: This concerns the relation between the allele and genotype frequencies. It assumes an indefinitely large population, with no selection, mutation, or migration. The law states that the genotype and the allele frequencies are constant over generations. Violation of the HWE law indicates that genotype frequencies are significantly different from expectations (e.g., if the frequency of allele A = 0.20 and the frequency of allele T = 0.80; the expected frequency of genotype AT is 20.20.8 = 0.32) and the observed frequency should not be significantly different. In GWAS, it is generally assumed that deviations from HWE are the result of genotyping errors. The HWE thresholds in cases are often less stringent than those in controls, as the violation of the HWE law in cases can be indicative of true genetic association with disease risk.\n Odds ratio An odds ratio (OR) is a statistic that quantifies the strength of the association between two events, A and B. The odds ratio is defined as the ratio of the odds of A in the presence of B and the odds of A in the absence of B, or equivalently (due to symmetry), the ratio of the odds of B in the presence of A and the odds of B in the absence of A. Two events are independent if and only if the OR equals 1: the odds of one event are the same in either the presence or absence of the other event. If the OR is greater than 1, then A and B are associated (correlated) in the sense that, compared to the absence of B, the presence of B raises the odds of A, and symmetrically the presence of A raises the odds of B. Conversely, if the OR is less than 1, then A and B are negatively correlated, and the presence of one event reduces the odds of the other event.\nNote that the odds ratio is symmetric in the two events, and there is no causal direction implied (correlation does not imply causation): a positive OR does not establish that B causes A, or that A causes B.\nTwo similar statistics that are often used to quantify associations are the risk ratio (RR) and the absolute risk reduction (ARR). Often, the parameter of greatest interest is actually the RR, which is the ratio of the probabilities analogous to the odds used in the OR. However, available data frequently do not allow for the computation of the RR or the ARR but do allow for the computation of the OR, as in case-control studies, as explained below. On the other hand, if one of the properties (A or B) is sufficiently rare (in epidemiology this is called the rare disease assumption), then the OR is approximately equal to the corresponding RR.\nThe OR plays an important role in the logistic model.\nImagine there is a rare disease, afflicting, say, only one in many thousands of adults in a country. Imagine we suspect that being exposed to something (say, having had a particular sort of injury in childhood) makes one more likely to develop that disease in adulthood. The most informative thing to compute would be the risk ratio, RR. To do this in the ideal case, for all the adults in the population we would need to know whether they (a) had the exposure to the injury as children and (b) whether they developed the disease as adults. From this we would extract the following information: the total number of people exposed to the childhood injury, {N_{E},} {N_{E},} out of which {D_{E}} {D_{E}} developed the disease and {H_{E}} H_E stayed healthy; and the total number of people not exposed, {N_{N},} {N_{N},} out of which {D_{N}} D_N developed the disease and {H_{N}} {H_{N}} stayed healthy. Since {N_{E}=D_{E}+H_{E}} {N_{E}=D_{E}+H_{E}} and similarly for the {N_{N}} {N_{N}} numbers, we only have four independent numbers, which we can organize in a table:\n   Diseased Healthy    Exposed \\(D_E\\) \\(H_E\\)  Not exposed \\(D_N\\) \\(H_N\\)    To avoid possible confusion, we emphasize that all these numbers refer to the entire population, and not to some sample of it.\nNow the risk of developing the disease given exposure is {D_{E}/N_{E}} D_{{E}}/N_{{E}} (where {N_{E}=D_{E}+H_{E}} {N_{E}=D_{E}+H_{E}}), and of developing the disease given non-exposure is {D_{N}/N_{N}.} {D_{N}/N_{N}.} The risk ratio, RR, is just the ratio of the two,\n{RR={},,} {RR={},,} which can be rewritten as {RR={}={}.} {RR={}={}.}\nIn contrast, the odds of developing the disease given exposure is {D_{E}/H_{E},,} {D_{E}/H_{E},,} and of developing the disease given non-exposure is {D_{N}/H_{N},.} {D_{N}/H_{N},.} The odds ratio, OR, is the ratio of the two,\n{OR={},,} {OR={},,} which can be rewritten as {OR={}={}.} {OR={}={}.}\nWe may already note that if the disease is rare, then OR ≈ RR. Indeed, for a rare disease, we will have {D_{E}H_{E},} {D_{E}H_{E},} and so {D_{E}+H_{E}H_{E};} {D_{E}+H_{E}H_{E};} but then {D_{E}/(D_{E}+H_{E})D_{E}/H_{E},} {D_{E}/(D_{E}+H_{E})D_{E}/H_{E},} in other words, for the exposed population, the risk of developing the disease is approximately equal to the odds. Analogous reasoning shows that the risk is approximately equal to the odds for the non-exposed population as well; but then the ratio of the risks, which is RR, is approximately equal to the ratio of the odds, which is OR. Or, we could just notice that the rare disease assumption says that {N_{E}H_{E}} {N_{E}H_{E}} and {N_{N}H_{N},} {N_{N}H_{N},} from which it follows that {N_{E}/N_{N}H_{E}/H_{N},} {N_{E}/N_{N}H_{E}/H_{N},} in other words that the denominators in the final expressions for the RR and the OR are approximately the same. The numerators are exactly the same, and so, again, we conclude that OR ≈ RR. Returning to our hypothetical study, the problem we often face is that we may not have the data to estimate these four numbers. For example, we may not have the population-wide data on who did or did not have the childhood injury.\nOften we may overcome this problem by employing random sampling of the population: namely, if neither the disease nor the exposure to the injury are too rare in our population, then we can pick (say) a hundred people at random, and find out these four numbers in that sample; assuming the sample is representative enough of the population, then the RR computed for this sample will be a good estimate for the RR for the whole population.\nHowever, some diseases may be so rare that, in all likelihood, even a large random sample may not contain even a single diseased individual (or it may contain some, but too few to be statistically significant). This would make it impossible to compute the RR. But, we may nevertheless be able to estimate the OR, provided that, unlike the disease, the exposure to the childhood injury is not too rare. Of course, because the disease is rare, this is then also our estimate for the RR.\nLooking at the final expression for the OR: the fraction in the numerator, {D_{E}/D_{N},} {D_{E}/D_{N},} we can estimate by collecting all the known cases of the disease (presumably there must be some, or else we likely wouldn’t be doing the study in the first place), and seeing how many of the diseased people had the exposure, and how many did not. And the fraction in the denominator, {H_{E}/H_{N},} {H_{E}/H_{N},} is the odds that a healthy individual in the population was exposed to the childhood injury. Now note that this latter odds can indeed be estimated by random sampling of the population—provided, as we said, that the prevalence of the exposure to the childhood injury is not too small, so that a random sample of a manageable size would be likely to contain a fair number of individuals who have had the exposure. So here the disease is very rare, but the factor thought to contribute to it is not quite so rare; such situations are quite common in practice.\nThus we can estimate the OR, and then, invoking the rare disease assumption again, we say that this is also a good approximation of the RR. Incidentally, the scenario described above is a paradigmatic example of a case-control study.[2]\nThe same story could be told without ever mentioning the OR, like so: as soon as we have that {N_{E}H_{E}} N_{{E}}H_{{E}} and {N_{N}H_{N},} {N_{N}H_{N},} then we have that {N_{E}/N_{N}H_{E}/H_{N}.} {N_{E}/N_{N}H_{E}/H_{N}.} Thus if, by random sampling, we manage to estimate {H_{E}/H_{N},} {H_{E}/H_{N},} then, by rare disease assumption, that will be a good estimate of {N_{E}/N_{N},} {N_{E}/N_{N},} which is all we need (besides {D_{E}/D_{N},} {D_{E}/D_{N},} which we presumably already know by studying the few cases of the disease) to compute the RR. However, it is standard in the literature to explicitly report the OR and then claim that the RR is approximately equal to it.\nWiki: Odds ratio\n Risk ratio In epidemiology, risk ratio (RR) or relative risk is the ratio of the probability of an outcome in an exposed group to the probability of an outcome in an unexposed group. It is computed as \\(I_e/I_u\\), where \\(I_{e}\\) is the incidence in the exposed group, and \\(I_{u}\\) is the incidence in the unexposed group. Together with risk difference and odds ratio, risk ratio measures the association between the exposure and the outcome.\nRisk ratio is used in the statistical analysis of the data of experimental, cohort and cross-sectional studies, to estimate the strength of the association between treatments or risk factors, and outcome. For example, it is used to compare the risk of an adverse outcome when receiving a medical treatment versus no treatment (or placebo), or when exposed to an environmental risk factor versus not exposed.\nAssuming the causal effect between the exposure and the outcome, values of RR can be interpreted as follows:\nRR = 1 means that exposure does not affect the outcome;\nRR \u0026lt; 1 means that the risk of the outcome is decreased by the exposure;\nRR \u0026gt; 1 means that the risk of the outcome is increased by the exposure.\n FEV1/FVC ratio The FEV1/FVC ratio, also called Tiffeneau-Pinelli index, is a calculated ratio used in the diagnosis of obstructive and restrictive lung disease. It represents the proportion of a person’s vital capacity that they are able to expire in the first second of forced expiration (FEV1) to the full, forced vital capacity (FVC).\nSee also Wiki: FVC ratio for more metrics of lung capacity.\n## Glossary\n  ","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565740800,"objectID":"f76f0d7b0eacd1b32cc54a23d7d2480a","permalink":"/post/biology/gwas/gwas/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/post/biology/gwas/gwas/","section":"post","summary":"Theory of Genome-Wide Association Studies (GWAS)","tags":["GWAS"],"title":"Genome-Wide Association Studies (GWAS)","type":"post"},{"authors":null,"categories":["Statistics"],"content":"  Simple linear regression How to find coefficients β0 and β1? Matrix form for multiple regression   Simple linear regression Ordinary Least Squares (OLS) function:\n\\(minimize\\left\\{SSE = sum_{i=1}^{n} (y_i - \\hat y_i)^2 \\right\\}\\)\nAssumtions of OLS regression:\n* Linear relationship\n* Multivariathe normality\n* No autocorrelation\n* Homoscedastic (constant variance in residuals)\n* There are more observations (n) than features (p) (n \u0026gt; p)\n* No or little multicollinearity\n\\(\\hat y = \\beta_0 + \\beta_1 x\\)\n\\(\\hat y\\) - expectd value of \\(y\\) given \\(x\\), the same as \\(E(Y|x)\\)\n\\(\\beta_0\\) - intercept\n\\(\\beta_1\\) - slope\n\\(\\hat \\beta_0 = \\bar y - \\hat\\beta_1 \\bar x\\)\n\\(\\hat\\beta_1 = \\frac{\\displaystyle\\sum_{i=1}^{n} (x_i - \\bar x)(y_i-\\bar y)}{\\displaystyle\\sum_{i=1}^{n} (x_i - \\bar x)^2}\\)\n How to find coefficients β0 and β1? There are several ways to extimate coefficients of linear regression. Here we discuss the least squares approach. Other aproaches include maximul likelihood estimation.\nTo estimate \\(\\beta_1\\) and \\(\\beta_0\\) we should find minimum of sum of squared residuals (\\(SSR\\))\nTo find this minimum we should calculate derivatives of \\(SSR\\) with respect to \\(\\beta_0\\) and \\(\\beta_1\\) and set them to 0:\n\\(\\displaystyle\\min_{\\beta_0,\\beta1} : SSR \\implies \\frac{\\partial SSR}{\\partial \\beta_0} = \\frac{\\partial SSR}{\\partial \\beta_1} = 0\\)\nHere is the solution:\n\\(SSR = \\displaystyle\\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1x_i))^2 = \\displaystyle\\sum_{i=1}^{n} (y_i^2 - 2y_i \\beta_0 - 2y_i \\beta_1 x_i + \\beta_0^2 + 2\\beta_0\\beta_1x_i + \\beta_1^2x_i^2)\\)\n\\(\\frac{\\partial SSR}{\\partial\\beta_0} = \\displaystyle\\sum_{i=1}^{n}(-2y_i + 2\\beta_0 + 2\\beta_1x_i)\\)\n\\(\\displaystyle\\sum_{i=1}^{n} (-y_i + \\hat\\beta_0 + \\hat\\beta_1 x_i) = 0\\)\n\\(\\bar y = \\frac{1}{n} \\displaystyle\\sum_{i=1}^{n} y_i\\) and \\(\\bar x = \\frac{1}{n} \\displaystyle\\sum_{i=1}^{n} x_i \\implies -n\\bar y + n \\hat\\beta_0 + \\hat\\beta_1 n \\hat x = 0\\)\n\\(\\hat \\beta_0 = \\bar y - \\hat\\beta_1 \\bar x\\)\n\\(\\frac{\\partial SSR}{\\partial \\beta_1} = \\displaystyle\\sum_{i=1}^{n} (-2x_i y_i + 2\\beta_0 x_i + 2\\beta_1 x_i^2)\\)\n\\(-\\displaystyle\\sum_{i=1}^{n} x_i y_i + \\hat\\beta_0 \\displaystyle\\sum_{i=1}^{n} x_i + \\hat\\beta_1\\displaystyle\\sum_{i=1}^{n} x_i^2 = 0\\)\n\\(-\\displaystyle\\sum_{i=1}^{n} x_i y_i + (\\bar y - \\hat\\beta_1 \\bar x) \\displaystyle\\sum_{i=1}^{n} x_i + \\hat\\beta_1\\displaystyle\\sum_{i=1}^{n} x_i^2 = 0\\)\n\\(\\hat\\beta_1 = \\frac{\\displaystyle\\sum_{i=1}^{n} x_i(y_i-\\bar y)}{\\displaystyle\\sum_{i=1}^{n} x_i (x_i - \\bar x)} = \\frac{\\displaystyle\\sum_{i=1}^{n} (x_i - \\bar x)(y_i-\\bar y)}{\\displaystyle\\sum_{i=1}^{n} (x_i - \\bar x)^2} = \\frac{Cov(x,y)}{Var(x)} = r_{xy} \\frac{s_y}{s_x}\\)\n\\(\\hat x\\) and \\(\\hat y\\) - estimated \\(x\\) and \\(y\\)\n\\(\\bar x\\) and \\(\\bar y\\) - averages of \\(x_i\\) and \\(y_i\\)\n\\(r_{xy}\\) - sample correlation coefficient between \\(x\\) and \\(y\\)\n\\(s_x\\) and \\(s_y\\) - uncorrected sample standard deviations of \\(x\\) and \\(y\\)\n\\(Var\\) and \\(Cov\\) - sample variance and sample covariance.\n Matrix form for multiple regression We can write \\(\\hat y = \\beta_0 + \\beta_1 x\\) in a matrix form:\n\\(Y = \\begin{bmatrix}y_1\\\\y_2\\\\\\vdots\\\\y_n\\end{bmatrix}\\) \\(b = \\begin{bmatrix}\\beta_1\\\\\\beta_2\\\\\\vdots\\\\\\beta_n\\end{bmatrix}\\) \\(X = \\begin{bmatrix}1 \u0026amp; x_{1,1} \u0026amp; x_{1,2} \\dots \u0026amp; x_{1,k}\\\\1 \u0026amp; x_{2,1} \u0026amp; x_{2,2} \\dots \u0026amp; x_{2,k}\\\\\\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots\\\\1 \u0026amp; x_{n,1} \u0026amp; x_{n,2} \\dots \u0026amp; x_{n,k}\\end{bmatrix}\\)\n\\(Y = Xb\\)\n\\(X\u0026#39;Y = X\u0026#39;Xb\\)\n\\((X\u0026#39;X)^{-1}X\u0026#39;Xb = (X\u0026#39;X)^{-1}X\u0026#39;Y\\)\n\\(b = (X\u0026#39;X)^{-1}X\u0026#39;Y\\)\n\\((X\u0026#39;X)^{-1}X\u0026#39;X = I\\) - identity matrix\n ","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565740800,"objectID":"49375f3da816adc5a61702458e6553bc","permalink":"/post/statistics/linear_regression_math/linear_regression_math/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/post/statistics/linear_regression_math/linear_regression_math/","section":"post","summary":"Pure math for linear regression model.","tags":["Math","Statistics","Regression"],"title":"Linear Regression (Math)","type":"post"},{"authors":null,"categories":["Statistics"],"content":"  Principal component analysis of iris dataset Split data Principal component analysis Prediction How to calculate coordinates for test data? Sources    Principal component analysis of iris dataset library(tidyverse) library(factoextra) df \u0026lt;- iris head(df) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa str(df) ## \u0026#39;data.frame\u0026#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels \u0026quot;setosa\u0026quot;,\u0026quot;versicolor\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... library(caret) trainIndex \u0026lt;- caret::createDataPartition(df[,5], p=.8, list = FALSE, times = 1) # split data train \u0026lt;- df[trainIndex, ] test \u0026lt;- df[-trainIndex, ] # PCA pca \u0026lt;- prcomp(train[,-5], scale=TRUE) # eigenvalues on \u0026#39;scree plot\u0026#39; # percentage of variances explained by each principal component. factoextra::fviz_eig(pca, addlabels = TRUE) # Graph of individuals factoextra::fviz_pca_ind(pca, geom = \u0026#39;point\u0026#39;) groups \u0026lt;- as.factor(train[,5]) fviz_pca_ind(pca, col.ind = groups, # color by groups palette = c(\u0026#39;red\u0026#39;, \u0026#39;green\u0026#39;, \u0026#39;blue\u0026#39;), addEllipses = TRUE, # Concentration ellipses ellipse.type = \u0026quot;confidence\u0026quot;, legend.title = \u0026quot;Groups\u0026quot;, repel = TRUE) # Graph of variables fviz_pca_var(pca, col.var = \u0026#39;contrib\u0026#39;, # color by contributions to the PC gradient.cols = c(\u0026#39;#00AFBB\u0026#39;, \u0026#39;#E7B800\u0026#39;, \u0026#39;#FC4E07\u0026#39;), repel = T) # avoid text overlapping # Graph of individuals and variables fviz_pca_biplot(pca, repel = TRUE, geom = \u0026#39;point\u0026#39;, col.var = \u0026#39;red\u0026#39;, # Variables color col.ind = \u0026#39;grey\u0026#39; # Individuals color ) library(tidyverse) library(factoextra) library(mlbench) # dataset data(PimaIndiansDiabetes) df \u0026lt;- PimaIndiansDiabetes head(df) ## pregnant glucose pressure triceps insulin mass pedigree age diabetes ## 1 6 148 72 35 0 33.6 0.627 50 pos ## 2 1 85 66 29 0 26.6 0.351 31 neg ## 3 8 183 64 0 0 23.3 0.672 32 pos ## 4 1 89 66 23 94 28.1 0.167 21 neg ## 5 0 137 40 35 168 43.1 2.288 33 pos ## 6 5 116 74 0 0 25.6 0.201 30 neg str(df) ## \u0026#39;data.frame\u0026#39;: 768 obs. of 9 variables: ## $ pregnant: num 6 1 8 1 0 5 3 10 2 8 ... ## $ glucose : num 148 85 183 89 137 116 78 115 197 125 ... ## $ pressure: num 72 66 64 66 40 74 50 0 70 96 ... ## $ triceps : num 35 29 0 23 35 0 32 0 45 0 ... ## $ insulin : num 0 0 0 94 168 0 88 0 543 0 ... ## $ mass : num 33.6 26.6 23.3 28.1 43.1 25.6 31 35.3 30.5 0 ... ## $ pedigree: num 0.627 0.351 0.672 0.167 2.288 ... ## $ age : num 50 31 32 21 33 30 26 29 53 54 ... ## $ diabetes: Factor w/ 2 levels \u0026quot;neg\u0026quot;,\u0026quot;pos\u0026quot;: 2 1 2 1 2 1 2 1 2 2 ... Split data trainIndex \u0026lt;- sample(nrow(df), nrow(df)*0.8) # split data train \u0026lt;- df[trainIndex, -ncol(df)] test \u0026lt;- df[-trainIndex, -ncol(df)]  Principal component analysis pca \u0026lt;- prcomp(train, scale=T) # eigenvalues on \u0026#39;scree plot\u0026#39; # percentage of variances explained by each principal component. fviz_eig(pca, addlabels = T) # Graph of individuals fviz_pca_ind(pca, geom = \u0026#39;point\u0026#39;, col.ind = \u0026quot;cos2\u0026quot;, # Color by the quality of representation gradient.cols = c(\u0026quot;#00AFBB\u0026quot;, \u0026quot;#E7B800\u0026quot;, \u0026quot;#FC4E07\u0026quot;)) # Graph of variables fviz_pca_var(pca, col.var = \u0026#39;contrib\u0026#39;, # color by contributions to the PC gradient.cols = c(\u0026#39;#00AFBB\u0026#39;, \u0026#39;#E7B800\u0026#39;, \u0026#39;#FC4E07\u0026#39;), repel = T) # avoid text overlapping # Graph of individuals and variables fviz_pca_biplot(pca, repel = TRUE, geom = \u0026#39;point\u0026#39;, col.var = \u0026#39;red\u0026#39;, # Variables color col.ind = \u0026#39;grey\u0026#39; # Individuals color ) # Eigenvalues get_eigenvalue(pca) ## eigenvalue variance.percent cumulative.variance.percent ## Dim.1 2.1484632 26.855791 26.85579 ## Dim.2 1.7117918 21.397397 48.25319 ## Dim.3 1.0560307 13.200383 61.45357 ## Dim.4 0.8577135 10.721419 72.17499 ## Dim.5 0.7827308 9.784135 81.95912 ## Dim.6 0.6490472 8.113090 90.07222 ## Dim.7 0.4246541 5.308177 95.38039 ## Dim.8 0.3695686 4.619608 100.00000 # Results for Variables pca.var \u0026lt;- get_pca_var(pca) pca.var$coord # Coordinates ## Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 ## pregnant -0.2613362 0.7555770 -0.03411457 0.164119419 0.38787318 ## glucose -0.5550955 0.1915545 0.53183488 -0.415232060 -0.33421050 ## pressure -0.5730576 0.1955417 -0.49970031 0.001917723 -0.21716148 ## triceps -0.6082808 -0.4930349 -0.28918361 0.106775060 0.36003451 ## insulin -0.5953969 -0.3664227 0.32796053 -0.298678115 0.43293986 ## mass -0.6682581 -0.1438707 -0.33060384 -0.014819512 -0.35047480 ## pedigree -0.3747603 -0.2310328 0.46492858 0.741684330 -0.16588066 ## age -0.3634878 0.7839270 0.07533503 0.086210125 0.07758022 ## Dim.6 Dim.7 Dim.8 ## pregnant 0.17425048 -0.36905378 0.12529249 ## glucose 0.01137266 -0.20644065 -0.21322789 ## pressure -0.57136625 -0.07659026 0.06465123 ## triceps 0.09518077 -0.04786257 -0.38846870 ## insulin -0.14977159 0.13459416 0.29408539 ## mass 0.50046583 0.06735177 0.21302317 ## pedigree -0.09760457 -0.02509684 0.04744906 ## age 0.02473193 0.46303034 -0.13856443 pca.var$contrib # Contributions to the PCs ## Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 ## pregnant 3.178860 33.350823 0.1102055 3.140347407 19.2206059 ## glucose 14.341925 2.143551 26.7841028 20.102011066 14.2701239 ## pressure 15.285112 2.233715 23.6451844 0.000428775 6.0249461 ## triceps 17.221867 14.200527 7.9190087 1.329221633 16.5605910 ## insulin 16.500048 7.843569 10.1851313 10.400747539 23.9465371 ## mass 20.785505 1.209188 10.3499741 0.025605047 15.6928259 ## pedigree 6.537012 3.118145 20.4689687 64.135127098 3.5154352 ## age 6.149670 35.900481 0.5374244 0.866511436 0.7689349 ## Dim.6 Dim.7 Dim.8 ## pregnant 4.67812337 32.0733237 4.2477111 ## glucose 0.01992727 10.0358717 12.3024868 ## pressure 50.29824867 1.3813755 1.1309892 ## triceps 1.39579650 0.5394567 40.8335307 ## insulin 3.45607012 4.2659628 23.4019348 ## mass 38.58980271 1.0682250 12.2788742 ## pedigree 1.46779037 0.1483210 0.6092002 ## age 0.09424099 50.4874634 5.1952730 pca.var$cos2 # Quality of representation  ## Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 ## pregnant 0.06829663 0.57089665 0.001163804 2.693518e-02 0.15044560 ## glucose 0.30813100 0.03669313 0.282848341 1.724177e-01 0.11169666 ## pressure 0.32839502 0.03823655 0.249700400 3.677661e-06 0.04715911 ## triceps 0.37000549 0.24308346 0.083627161 1.140091e-02 0.12962485 ## insulin 0.35449746 0.13426556 0.107558111 8.920862e-02 0.18743692 ## mass 0.44656894 0.02069878 0.109298901 2.196179e-04 0.12283258 ## pedigree 0.14044530 0.05337616 0.216158588 5.500956e-01 0.02751639 ## age 0.13212341 0.61454148 0.005675366 7.432186e-03 0.00601869 ## Dim.6 Dim.7 Dim.8 ## pregnant 0.0303632306 0.1362006927 0.015698209 ## glucose 0.0001293374 0.0426177435 0.045466134 ## pressure 0.3264593946 0.0058660682 0.004179781 ## triceps 0.0090593787 0.0022908253 0.150907928 ## insulin 0.0224315277 0.0181155871 0.086486214 ## mass 0.2504660493 0.0045362616 0.045378869 ## pedigree 0.0095266529 0.0006298513 0.002251413 ## age 0.0006116685 0.2143970968 0.019200100 # Results for individuals pca.ind \u0026lt;- get_pca_ind(pca) pca.ind$coord %\u0026gt;% head() # Coordinates ## Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 Dim.6 ## 72 -0.3669946 -0.56840272 0.3011660 -0.47509324 0.7579541 0.007199364 ## 752 -0.6216923 -1.11858347 -1.1355107 -0.57113207 -0.3016049 0.252958078 ## 551 0.9874862 -0.93348528 -0.6800219 -0.54448283 -0.1871258 -0.330097027 ## 531 0.4210481 -1.06864263 0.6917994 0.24507935 -0.2124609 -0.145927479 ## 170 0.2644303 -0.02563326 -0.3747252 0.03149040 -0.3520414 -1.156386621 ## 764 -1.7515825 1.74941478 -1.0179324 0.02774283 2.3889854 0.431141870 ## Dim.7 Dim.8 ## 72 -0.7799567 -0.5294728 ## 752 0.1103204 -0.5529107 ## 551 -0.4434670 -0.7824037 ## 531 -0.3403009 0.2020358 ## 170 -0.1597602 0.4188356 ## 764 1.0485653 -0.7088242 pca.ind$contrib %\u0026gt;% head() # Contributions to the PCs ## Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 ## 72 0.010209939 3.073923e-02 0.01398836 0.0428594975 0.119537749 ## 752 0.029299124 1.190467e-01 0.19885548 0.0619387561 0.018927615 ## 551 0.073920645 8.290785e-02 0.07131829 0.0562934359 0.007285954 ## 531 0.013439008 1.086540e-01 0.07381004 0.0114051979 0.009392415 ## 170 0.005300614 6.251561e-05 0.02165614 0.0001882980 0.025787350 ## 764 0.232575992 2.911832e-01 0.15980597 0.0001461473 1.187534409 ## Dim.6 Dim.7 Dim.8 ## 72 0.000013006 0.233312081 0.12354459 ## 752 0.016056557 0.004667754 0.13472450 ## 551 0.027342521 0.075425638 0.26977286 ## 531 0.005343545 0.044414265 0.01798843 ## 170 0.335553241 0.009788896 0.07730788 ## 764 0.046643993 0.421684107 0.22141831 pca.ind$cos2 %\u0026gt;% head() # Quality of representation  ## Dim.1 Dim.2 Dim.3 Dim.4 Dim.5 ## 72 0.06019710 0.1444004010 0.04053852 1.008820e-01 0.256768635 ## 752 0.10372696 0.3357975532 0.34603753 8.754147e-02 0.024412791 ## 551 0.27405014 0.2448966943 0.12996122 8.331752e-02 0.009840908 ## 531 0.08519110 0.5487767729 0.22998053 2.886318e-02 0.021691462 ## 170 0.03731036 0.0003506023 0.07492606 5.291311e-04 0.066129394 ## 764 0.20927254 0.2087548895 0.07067877 5.249919e-05 0.389294326 ## Dim.6 Dim.7 Dim.8 ## 72 2.316564e-05 0.271892389 0.12529777 ## 752 1.717267e-02 0.003266272 0.08204475 ## 551 3.062324e-02 0.055270156 0.17204011 ## 531 1.023305e-02 0.055648944 0.01961496 ## 170 7.135315e-01 0.013618977 0.09360397 ## 764 1.267919e-02 0.074996678 0.03427110  Prediction p \u0026lt;- predict(pca, newdata = test) # Plot prediction plot \u0026lt;- fviz_pca_ind(pca, repel=T, geom = \u0026#39;point\u0026#39;) fviz_add(plot, p, color=\u0026#39;red\u0026#39;)  How to calculate coordinates for test data?  Sources  http://www.sthda.com    ","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565740800,"objectID":"dca9b47611686c54b936dc19b0762ab9","permalink":"/post/statistics/pca/pca/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/post/statistics/pca/pca/","section":"post","summary":"Principal component analysis.","tags":["R","Statistics","PCA"],"title":"Principal component analysis","type":"post"},{"authors":null,"categories":["Bioinformatics"],"content":"  Alignment using Get fastq from SRA database   Alignment using library(Rsubread) # sequence alignment fastq.files \u0026lt;- list.files(pattern = \u0026quot;.fastq.gz$\u0026quot;, full.names = TRUE) fastq.files # build index buildindex(basename=\u0026quot;mm10\u0026quot;, reference=\u0026quot;mm10.fa\u0026quot;) # alignment align(fastq.files, index=\u0026quot;chr1_mm10\u0026quot;) # parameters args(align) # check result bam.files \u0026lt;- list.files(pattern = \u0026quot;.BAM$\u0026quot;, full.names = TRUE) bam.files # properties of BAM files propmapped(files=bam.files)  Get fastq from SRA database library(SRAdb) # SRA database dir.create(\u0026quot;~/SRA\u0026quot;) setwd(\u0026quot;~/SRA\u0026quot;) ## Create database sqlfile \u0026lt;-\u0026#39;SRAmetadb.sqlite\u0026#39; # load database [~3 Gb, 60 Gb on disc!!!] if(!file.exists(\u0026#39;SRAmetadb.sqlite\u0026#39;)) sqlfile \u0026lt;\u0026lt;- getSRAdbFile() # connect database sra_con \u0026lt;- dbConnect(SQLite(), sqlfile) ## get info for SRA from created database sraInf \u0026lt;- getSRAinfo(\u0026quot;SRP045534\u0026quot;, sra_con, sraType=\u0026quot;sra\u0026quot;) sraInf # download SRA # get SRA using SRA info [~ 5 Gb] sapply(sraInf$run, function(x) try(getSRAfile(x, sra_con, fileType=\u0026quot;sra\u0026quot;), silent=TRUE)) ## extract FASTQ from SRA cd ~/SRA for sra in *.sra do fastq-dump $sra done  ","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565740800,"objectID":"f42730e676f53609b06edf102073b067","permalink":"/post/bioinformatics/processing_sequencing_data_in_r/rna_seq_analysis_in_r/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/post/bioinformatics/processing_sequencing_data_in_r/rna_seq_analysis_in_r/","section":"post","summary":"Processing of sequencing data in R.","tags":["R","RNA-seq"],"title":"Processing of sequencing data in R","type":"post"},{"authors":null,"categories":["Bioinformatics"],"content":" library(edgeR) library(limma) library(Glimma) library(gplots) library(org.Mm.eg.db) library(RColorBrewer) # Preprocessing of expression data # dataset: https://figshare.com/s/1d788fd384d33e913a2a setwd(\u0026quot;~/data/\u0026quot;) # load data sampleinfo \u0026lt;- read.delim(\u0026quot;SampleInfo.txt\u0026quot;) seqdata \u0026lt;- read.delim(\u0026quot;GSE60450_Lactation-GenewiseCounts.txt\u0026quot;, stringsAsFactors = FALSE) head(seqdata) # format data # Remove first two columns from seqdata exp \u0026lt;- seqdata[,-(1:2)] # Store EntrezGeneID as rownames rownames(exp) \u0026lt;- seqdata[,1] head(exp) # make shorter sample names colnames(exp) \u0026lt;- substr(colnames(exp), start=1, stop=7) head(exp) # Counts per Million (CPM) myCPM \u0026lt;- edgeR::cpm(exp) # Have a look at the output head(myCPM) # TRUE if CPM \u0026gt; 0.5 thresh \u0026lt;- myCPM \u0026gt; 0.5 # logical matrix head(thresh) # How many genes with CPM \u0026gt; 0.5 table(rowSums(thresh)) # keep genes that have at least 2 TRUES in each row of thresh fltr \u0026lt;- rowSums(thresh) \u0026gt;= 2 # Subset the rows of countdata to keep the more highly expressed genes exp.fltr \u0026lt;- exp[fltr,] summary(exp.fltr) dim(exp.fltr) # We will look at the first sample plot(myCPM[,1],countdata[,1]) # Let us limit the x and y-axis so we can actually look to see what is happening at the smaller counts plot(myCPM[,1],countdata[,1],ylim=c(0,50),xlim=c(0,3)) # Add a vertical line at 0.5 CPM abline(v=0.5) ### Digital Gene Expression List from edgeR # Convert counts to DGEList object dgeObj \u0026lt;- edge::DGEList(exp.fltr) # have a look at dgeObj dgeObj # See what slots are stored in dgeObj names(dgeObj) # Library size information is stored in the samples slot dgeObj$samples # Library sizes and distribution plot dgeObj$samples$lib.size # The names argument tells the barplot to use the sample names on the x-axis # The las argument rotates the axis names barplot(dgeObj$samples$lib.size, names=colnames(dgeObj), las=2, main=\u0026quot;Barplot of library sizes\u0026quot;) # Get log2 counts per million logcounts \u0026lt;- cpm(dgeObj,log=TRUE) # Check distributions of samples using boxplots boxplot(logcounts, xlab=\u0026quot;\u0026quot;, ylab=\u0026quot;Log2 counts per million\u0026quot;, las=2, main = \u0026quot;Boxplots of logCPMs (unnormalised)\u0026quot;) # Let\u0026#39;s add a blue horizontal line that corresponds to the median logCPM abline(h=median(logcounts),col=\u0026quot;blue\u0026quot;) plotMDS(dgeObj) # We specify the option to let us plot two plots side-by-sde par(mfrow=c(1,2)) # Let\u0026#39;s set up colour schemes for CellType # How many cell types and in what order are they stored? levels(sampleinfo$CellType) ## Let\u0026#39;s choose purple for basal and orange for luminal col.cell \u0026lt;- c(\u0026quot;purple\u0026quot;,\u0026quot;orange\u0026quot;)[sampleinfo$CellType] data.frame(sampleinfo$CellType, col.cell) # Redo the MDS with cell type colouring plotMDS(dgeObj,col=col.cell) # Let\u0026#39;s add a legend to the plot so we know which colours correspond to which cell type legend(\u0026quot;topleft\u0026quot;, fill=c(\u0026quot;purple\u0026quot;,\u0026quot;orange\u0026quot;), legend=levels(sampleinfo$CellType)) # Add a title title(\u0026quot;Cell type\u0026quot;) # Similarly for status levels(sampleinfo$Status) col.status \u0026lt;- c(\u0026quot;blue\u0026quot;,\u0026quot;red\u0026quot;,\u0026quot;dark green\u0026quot;)[sampleinfo$Status] col.status plotMDS(dgeObj, col=col.status) legend(\u0026quot;topleft\u0026quot;, fill=c(\u0026quot;blue\u0026quot;,\u0026quot;red\u0026quot;,\u0026quot;dark green\u0026quot;), legend=levels(sampleinfo$Status), cex=0.8) title(\u0026quot;Status\u0026quot;) # There is a sample info corrected file in your data directory # Old sampleinfo sampleinfo # I\u0026#39;m going to write over the sampleinfo object with the corrected sample info sampleinfo \u0026lt;- read.delim(\u0026quot;data/SampleInfo_Corrected.txt\u0026quot;) sampleinfo # Redo the MDSplot with corrected information par(mfrow=c(1,2)) col.cell \u0026lt;- c(\u0026quot;purple\u0026quot;,\u0026quot;orange\u0026quot;)[sampleinfo$CellType] col.status \u0026lt;- c(\u0026quot;blue\u0026quot;,\u0026quot;red\u0026quot;,\u0026quot;dark green\u0026quot;)[sampleinfo$Status] plotMDS(dgeObj,col=col.cell) legend(\u0026quot;topleft\u0026quot;,fill=c(\u0026quot;purple\u0026quot;,\u0026quot;orange\u0026quot;), legend=levels(sampleinfo$CellType)) title(\u0026quot;Cell type\u0026quot;) plotMDS(dgeObj,col=col.status) legend(\u0026quot;topleft\u0026quot;,fill=c(\u0026quot;blue\u0026quot;,\u0026quot;red\u0026quot;,\u0026quot;dark green\u0026quot;), legend=levels(sampleinfo$Status), cex=0.8) title(\u0026quot;Status\u0026quot;) # Dimension 3 appears to separate pregnant samples from the rest. Dim4? plotMDS(dgeObj,dim=c(3,4)) labels \u0026lt;- paste(sampleinfo$SampleName, sampleinfo$CellType, sampleinfo$Status) group \u0026lt;- paste(sampleinfo$CellType,sampleinfo$Status,sep=\u0026quot;.\u0026quot;) group \u0026lt;- factor(group) glMDSPlot(dgeObj, labels=labels, groups=group, folder=\u0026quot;mds\u0026quot;) # Hierarchical clustering with heatmaps # We estimate the variance for each row in the logcounts matrix var_genes \u0026lt;- apply(logcounts, 1, var) head(var_genes) # Get the gene names for the top 500 most variable genes select_var \u0026lt;- names(sort(var_genes, decreasing=TRUE))[1:500] head(select_var) # Subset logcounts matrix highly_variable_lcpm \u0026lt;- logcounts[select_var,] dim(highly_variable_lcpm) head(highly_variable_lcpm) ## Get some nicer colours mypalette \u0026lt;- brewer.pal(11,\u0026quot;RdYlBu\u0026quot;) morecols \u0026lt;- colorRampPalette(mypalette) # Set up colour vector for celltype variable col.cell \u0026lt;- c(\u0026quot;purple\u0026quot;,\u0026quot;orange\u0026quot;)[sampleinfo$CellType] # Plot the heatmap heatmap.2(highly_variable_lcpm, col=rev(morecols(50)), trace=\u0026quot;column\u0026quot;, main=\u0026quot;Top 500 most variable genes across samples\u0026quot;, ColSideColors=col.cell,scale=\u0026quot;row\u0026quot;) # Save the heatmap png(file=\u0026quot;High_var_genes.heatmap.png\u0026quot;) heatmap.2(highly_variable_lcpm,col=rev(morecols(50)), trace=\u0026quot;none\u0026quot;, main=\u0026quot;Top 500 most variable genes\\nacross samples\u0026quot;, ColSideColors=col.cell,scale=\u0026quot;row\u0026quot;) dev.off() # Normalization for composition bias # Apply normalisation to DGEList object dgeObj \u0026lt;- calcNormFactors(dgeObj) dgeObj$samples par(mfrow=c(1,2)) plotMD(logcounts,column = 7) abline(h=0,col=\u0026quot;grey\u0026quot;) plotMD(logcounts,column = 11) abline(h=0,col=\u0026quot;grey\u0026quot;) par(mfrow=c(1,2)) plotMD(dgeObj,column = 7) abline(h=0,col=\u0026quot;grey\u0026quot;) plotMD(dgeObj,column = 11) abline(h=0,col=\u0026quot;grey\u0026quot;) save(group,dgeObj,sampleinfo,file=\u0026quot;Robjects/preprocessing.Rdata\u0026quot;) ","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565740800,"objectID":"c767a6a99bb4fcb004526c8ff2c314e6","permalink":"/post/bioinformatics/rna_seq_in_r/rna_seq_analysis_in_r/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/post/bioinformatics/rna_seq_in_r/rna_seq_analysis_in_r/","section":"post","summary":"Analysis of RNA-seq in R.","tags":["R","RNA-seq"],"title":"RNA-seq analysis in R","type":"post"},{"authors":null,"categories":["R"],"content":"  One-way ANOVA Sources   One-way ANOVA variance = SS/df, where SS - sum of squares and df - degree of freedom\n\\(SS = \\displaystyle\\sum_{i=1}^{n}{(x_i - \\mu)^2}\\), where\n\\(\\mu\\) is the sample mean\nn is the sample size\n\\(var(x) = \\frac{1}{n}{\\displaystyle\\sum_{i=1}^{n}{(x_i - \\mu)^2}}\\)\n3 groups of students with scores (1-100):\na = c(82,93,61,74,69,70,53) b = c(71,62,85,94,78,66,71) c = c(64,73,87,91,56,78,87) SST = SSE + SSC = W + B, where\nSST - Total Sum of Squares\nSSE - Error Sum of Squares - within (W)\nSSC - Sum of Squares Columns (treatmens) - between (B)\nC - columns (treatments)\nN - total number of observations\nMean squared of columns - MSC = SSC/df_columns, where df_columns = C-1\nMean squared of error - MSE = SSE/df_error, where df_error = N-C\nSum of squares (total) - SST, where df_total = N-1 F-statistics - F = MSC/MSE\nLet’s calculate degree of freedom for our example:\ndf_columns = 3-1 = 2, MSC = SSC/2\ndf_error = 21-3 = 18, MSE = SSE/18\ndf_total = 21-1 = 20\na = c(82,93,61,74,69,70,53) b = c(71,62,85,94,78,66,71) c = c(64,73,87,91,56,78,87) sq = function(x) { sum((x - mean(x))^2) } sq(a) ## [1] 1039.429 sq(b) ## [1] 751.4286 sq(c) ## [1] 1021.714 Using R packages:\n# data # Number of calories consumed by month: may \u0026lt;- c(2166, 1568, 2233, 1882, 2019) sep \u0026lt;- c(2279, 2075, 2131, 2009, 1793) dec \u0026lt;- c(2226, 2154, 2583, 2010, 2190) d \u0026lt;- stack(list(may=may, sep=sep, dec=dec)) d ## values ind ## 1 2166 may ## 2 1568 may ## 3 2233 may ## 4 1882 may ## 5 2019 may ## 6 2279 sep ## 7 2075 sep ## 8 2131 sep ## 9 2009 sep ## 10 1793 sep ## 11 2226 dec ## 12 2154 dec ## 13 2583 dec ## 14 2010 dec ## 15 2190 dec names(d) ## [1] \u0026quot;values\u0026quot; \u0026quot;ind\u0026quot; oneway.test(values ~ ind, data=d, var.equal=TRUE) ## ## One-way analysis of means ## ## data: values and ind ## F = 1.7862, num df = 2, denom df = 12, p-value = 0.2094 # alternative using aov res \u0026lt;- aov(values ~ ind, data = d) res ## Call: ## aov(formula = values ~ ind, data = d) ## ## Terms: ## ind Residuals ## Sum of Squares 174664.1 586719.6 ## Deg. of Freedom 2 12 ## ## Residual standard error: 221.1183 ## Estimated effects may be unbalanced summary(res) ## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## ind 2 174664 87332 1.786 0.209 ## Residuals 12 586720 48893  Sources Example for one-way ANOVA: youtube by Brandon Foltz\n ","date":1565308800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565308800,"objectID":"445ffc87498b40ec3f1030dc0cadb409","permalink":"/post/statistics/anova/anova/","publishdate":"2019-08-09T00:00:00Z","relpermalink":"/post/statistics/anova/anova/","section":"post","summary":"One-way ANOVA Sources   One-way ANOVA variance = SS/df, where SS - sum of squares and df - degree of freedom\n\\(SS = \\displaystyle\\sum_{i=1}^{n}{(x_i - \\mu)^2}\\), where\n\\(\\mu\\) is the sample mean\nn is the sample size\n\\(var(x) = \\frac{1}{n}{\\displaystyle\\sum_{i=1}^{n}{(x_i - \\mu)^2}}\\)\n3 groups of students with scores (1-100):\na = c(82,93,61,74,69,70,53) b = c(71,62,85,94,78,66,71) c = c(64,73,87,91,56,78,87) SST = SSE + SSC = W + B, where","tags":["R"],"title":"ANOVA","type":"post"},{"authors":null,"categories":["R","Statistics"],"content":"  Elbow method Average silhouette method Gap statistic method Using NbCLust Sources   Elbow method The basic idea is to find minimal the total intra-cluster variation or total Within-cluster Sum ofSquares (WSS).\nPlot number of clusters ~ WSS show how WSS is reduced with increase of number of clusters. The optimal number of clusters is when adding another cluster doesn’t improve much better the total WSS.\nThe optimal number of clusters can be defined as follow:\n1. Compute clustering algorithm (e.g k-means) for different number of clusters (e.g k is from 1 to 10)\n2. For each k calculate the total within-cluster sum of squares (wss).\n3. Plot k ~ wss 4. The location of a bend (knee) in the plot is generally considered as an indicator of the optimal number of clusters.\n Average silhouette method **Average silhouette method determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering.\nThe optimal number of clusters is detected by maximal the average slhouette.\nThe optimal number of clusters can be defined as follow:\n1. Compute clustering algorithm (e.g k-means) for different number of clusters (e.g k is from 1 to 10)\n2. For each k, calculate the average silhouette of observations (aso).\n3. Plot k ~ aso\n4. The location of the maximum is considered as the optimal number of clusters.\n Gap statistic method The gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e, that yields the largest gap statistic).\nlibrary(factoextra) # normalized data df \u0026lt;- scale(iris[, -5]) head(df) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## [1,] -0.8976739 1.01560199 -1.335752 -1.311052 ## [2,] -1.1392005 -0.13153881 -1.335752 -1.311052 ## [3,] -1.3807271 0.32731751 -1.392399 -1.311052 ## [4,] -1.5014904 0.09788935 -1.279104 -1.311052 ## [5,] -1.0184372 1.24503015 -1.335752 -1.311052 ## [6,] -0.5353840 1.93331463 -1.165809 -1.048667 # Elbow method fviz_nbclust(df, kmeans, method = \u0026quot;wss\u0026quot;) + labs(subtitle = \u0026quot;Elbow method\u0026quot;) # Silhouette method fviz_nbclust(df, kmeans, method = \u0026quot;silhouette\u0026quot;)+ labs(subtitle = \u0026quot;Silhouette method\u0026quot;) # Gap statistic set.seed(123) fviz_nbclust(df, kmeans, nstart = 25, method = \u0026quot;gap_stat\u0026quot;, nboot = 50)+ labs(subtitle = \u0026quot;Gap statistic method\u0026quot;) As we can see from these examples, the Elbow method predicts 2-3 clusters, Silhouette method predicts 2 and the Gap statistic method predicts 2 clusters. As we know, iris data contains 3 clusters of different Species and two of them is generally dificult to distinguish. This situation we can see from the shown diagrams. Unfortunately there is no method to exactly predict number of clusters. As researchers we should try to find if number of clusters have natural sense.\n Using NbCLust library(NbClust) res \u0026lt;- NbClust(data = df, # matrix or data frame distance = \u0026quot;euclidean\u0026quot;, # distance method = \u0026quot;kmeans\u0026quot;, # method min.nc = 2, max.nc = 10) # min and max number of clusters to test ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ##  ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 11 proposed 2 as the best number of clusters ## * 10 proposed 3 as the best number of clusters ## * 1 proposed 4 as the best number of clusters ## * 1 proposed 6 as the best number of clusters ## * 1 proposed 10 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 2 ## ## ## ******************************************************************* res$Best.nc[,1:5] ## KL CH Hartigan CCC Scott ## Number_clusters 3.0000 2.0000 3.0000 3.0000 3.0000 ## Value_Index 5.1669 251.3493 54.2213 5.1886 131.6411 # Example with iris data data \u0026lt;- iris[, -5] # distance matrix diss_matrix \u0026lt;- dist(data, method = \u0026quot;euclidean\u0026quot;) # here method is the distance method res \u0026lt;- NbClust(data, # matrix or data frame diss = diss_matrix, # distance matrix distance = NULL, # if set, diss musnt be NULL min.nc = 2, max.nc = 10, # min and max number of clusters to test method = \u0026quot;complete\u0026quot;, # clustering method index = \u0026quot;alllong\u0026quot;) # all indexes; can be selected ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ##  ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 2 proposed 2 as the best number of clusters ## * 15 proposed 3 as the best number of clusters ## * 6 proposed 4 as the best number of clusters ## * 1 proposed 6 as the best number of clusters ## * 3 proposed 10 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 3 ## ## ## ******************************************************************* # indeces first 5 methods head(res$All.index[,1:7]) ## KL CH Hartigan CCC Scott Marriot TrCovW ## 2 1.9652 280.8392 240.7478 30.4441 933.9084 977604.0 6868.5401 ## 3 5.3598 485.9050 68.8363 35.8668 1210.7629 347351.8 304.1791 ## 4 54.0377 495.1816 16.4167 35.6036 1346.7582 249402.3 135.7432 ## 5 0.0263 414.3925 51.1371 33.0698 1387.9419 296129.2 121.5044 ## 6 7.1653 455.4931 16.8076 33.9870 1506.5585 193380.9 96.9908 ## 7 0.5308 423.7198 20.2960 32.9063 1560.0089 184311.4 93.2005 # optimal number of clusters best.result \u0026lt;- t(res$Best.nc) best.result ## Number_clusters Value_Index ## KL 4 54.0377 ## CH 4 495.1816 ## Hartigan 3 171.9115 ## CCC 3 35.8668 ## Scott 3 276.8545 ## Marriot 3 532302.6940 ## TrCovW 3 6564.3610 ## TraceW 3 117.0760 ## Friedman 4 151.3607 ## Rubin 6 -33.9014 ## Cindex 3 0.3163 ## DB 3 0.7025 ## Silhouette 2 0.5160 ## Duda 4 0.5932 ## PseudoT2 4 32.9134 ## Beale 3 1.8840 ## Ratkowsky 3 0.4922 ## Ball 3 87.7349 ## PtBiserial 3 0.7203 ## Gap 3 0.1343 ## Frey 1 NA ## McClain 2 0.4228 ## Gamma 4 0.9261 ## Gplus 10 52.7862 ## Tau 3 2649.8405 ## Dunn 10 0.1543 ## Hubert 0 0.0000 ## SDindex 3 1.6226 ## Dindex 0 0.0000 ## SDbw 10 0.0303 # density plot of all best results library(magrittr) best.result %\u0026gt;% .[,1] %\u0026gt;% na.omit() %\u0026gt;% as.vector() %\u0026gt;% density(.) %\u0026gt;% plot(., xlab=\u0026#39;k, klusters\u0026#39;, main=\u0026#39;Optimal number of clusters\u0026#39;) Es we can see the most of methods predict k=3 for clustering iris data.\nIf you try k-means method, you will get optimal between 2 and 3.\n Sources  Cluster Validataion Essentials by Alboukadel Kassambara\n Tibshirani R at al. Estimating the number of clusters in a data set via the gap statistic. J.R.Statist.Soc.B, 2001\n [NbClust: An R Package for Determining the Relevant Number of Clusters in a Data] Set(https://cran.r-project.org/web/packages/NbClust/NbClust.pdf)   ","date":1565308800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565308800,"objectID":"0f9095ac20afec3ecec4357d57552bec","permalink":"/post/statistics/determin_number_clusters/determin_number_clusters/","publishdate":"2019-08-09T00:00:00Z","relpermalink":"/post/statistics/determin_number_clusters/determin_number_clusters/","section":"post","summary":"Elbow method Average silhouette method Gap statistic method Using NbCLust Sources   Elbow method The basic idea is to find minimal the total intra-cluster variation or total Within-cluster Sum ofSquares (WSS).\nPlot number of clusters ~ WSS show how WSS is reduced with increase of number of clusters. The optimal number of clusters is when adding another cluster doesn’t improve much better the total WSS.\nThe optimal number of clusters can be defined as follow:","tags":["R","Clustering"],"title":"Determining the optimal number of clusters","type":"post"},{"authors":null,"categories":["Statistics"],"content":"  Gene expression analysis of histone deacetylase 1 (HDAC1) knockout mouse. Sources   Gene expression analysis of histone deacetylase 1 (HDAC1) knockout mouse. This short tutorial should help to understand the basic principal of gene expression analysis using simple dataset and nearly basic R.\n Affymetrix microarray Dataset: GSE5583\n Paper: Mol Cell Biol 2006 Nov;26(21):7913-28. PMID: 16940178\n R code: Ahmed Moustafa  # Read the data into R library (RCurl) url = getURL (\u0026quot;http://bit.ly/GSE5583_data\u0026quot;, followlocation = TRUE) data = as.matrix(read.table (text = url, row.names = 1, header = T)) # Check the loaded dataset dim(data) # Dimension of the dataset ## [1] 12488 6 # data shows gene experssion levels in 6 samples: # rows correspond to samples (3 wild type WT and 3 knock-out KO) # columns correspond to genes ids head(data) # First few rows ## WT.GSM130365 WT.GSM130366 WT.GSM130367 KO.GSM130368 KO.GSM130369 ## 100001_at 11.5 5.6 69.1 15.7 36.0 ## 100002_at 20.5 32.4 93.3 31.8 14.4 ## 100003_at 72.4 89.0 79.2 80.5 130.1 ## 100004_at 261.0 226.2 365.1 432.0 447.3 ## 100005_at 1086.2 1555.6 1487.1 1062.2 1365.9 ## 100006_at 49.7 52.9 15.0 25.8 48.8 ## KO.GSM130370 ## 100001_at 42.0 ## 100002_at 22.9 ## 100003_at 86.7 ## 100004_at 288.1 ## 100005_at 1436.2 ## 100006_at 54.8 ################### # Exploratory plots ################### # Check the behavior of the data hist(data, col = \u0026quot;gray\u0026quot;, main=\u0026quot;GSE5583 - Histogram\u0026quot;) # Log2 transformation (why?) data2 = log2(data) # Check the behavior of the data after log-transformation hist(data2, col = \u0026quot;gray\u0026quot;, main=\u0026quot;GSE5583 (log2) - Histogram\u0026quot;) # Boxplot boxplot(data2, col=c(\u0026quot;darkgreen\u0026quot;, \u0026quot;darkgreen\u0026quot;, \u0026quot;darkgreen\u0026quot;, \u0026quot;darkred\u0026quot;, \u0026quot;darkred\u0026quot;, \u0026quot;darkred\u0026quot;), main=\u0026quot;GSE5583 - boxplots\u0026quot;, las=2) # Hierarchical clustering of the \u0026quot;samples\u0026quot; based on # the correlation coefficients of the expression values hc = hclust(as.dist(1-cor(data2))) plot(hc, main=\u0026quot;GSE5583 - Hierarchical Clustering\u0026quot;) ####################################### # Differential expression (DE) analysis ####################################### # Separate the two conditions into two smaller data frames wt = data2[,1:3] ko = data2[,4:6] # Compute the means of the samples of each condition wt.mean = apply(wt, 1, mean) ko.mean = apply(ko, 1, mean) head(wt.mean) ## 100001_at 100002_at 100003_at 100004_at 100005_at 100006_at ## 4.039868 5.306426 6.320360 8.120503 10.408872 5.089087 head(ko.mean) ## 100001_at 100002_at 100003_at 100004_at 100005_at 100006_at ## 4.844978 4.452076 6.597451 8.576804 10.318839 5.358071 # Just get the maximum of all the means limit = max(wt.mean, ko.mean) # Scatter plot plot(ko.mean ~ wt.mean, xlab = \u0026quot;WT\u0026quot;, ylab = \u0026quot;KO\u0026quot;, main = \u0026quot;GSE5583 - Scatter\u0026quot;, xlim = c(0, limit), ylim = c(0, limit)) # Diagonal line abline(0, 1, col = \u0026quot;red\u0026quot;) # Compute fold-change (biological significance) # Difference between the means of the conditions fold = wt.mean - ko.mean # Histogram of the fold differences hist(fold, col = \u0026quot;gray\u0026quot;) # Compute statistical significance (using t-test) pvalue = NULL # Empty list for the p-values tstat = NULL # Empty list of the t test statistics for(i in 1 : nrow(data)) { # For each gene : x = wt[i,] # WT of gene number i y = ko[i,] # KO of gene number i # Compute t-test between the two conditions t = t.test(x, y) # Put the current p-value in the pvalues list pvalue[i] = t$p.value # Put the current t-statistic in the tstats list tstat[i] = t$statistic } head(pvalue) ## [1] 0.5449730 0.3253745 0.3287830 0.1892376 0.6928410 0.7180077 # Histogram of p-values (-log10) hist(-log10(pvalue), col = \u0026quot;gray\u0026quot;) # Volcano: put the biological significance (fold-change) # and statistical significance (p-value) in one plot plot(fold, -log10(pvalue), main = \u0026quot;GSE5583 - Volcano\u0026quot;) fold_cutoff = 2 pvalue_cutoff = 0.01 abline(v = fold_cutoff, col = \u0026quot;blue\u0026quot;, lwd = 3) abline(v = -fold_cutoff, col = \u0026quot;red\u0026quot;, lwd = 3) abline(h = -log10(pvalue_cutoff), col = \u0026quot;green\u0026quot;, lwd = 3) # Screen for the genes that satisfy the filtering criteria # Fold-change filter for \u0026quot;biological\u0026quot; significance filter_by_fold = abs(fold) \u0026gt;= fold_cutoff dim(data2[filter_by_fold, ]) ## [1] 210 6 # P-value filter for \u0026quot;statistical\u0026quot; significance filter_by_pvalue = pvalue \u0026lt;= pvalue_cutoff dim(data2[filter_by_pvalue, ]) ## [1] 429 6 # Combined filter (both biological and statistical) filter_combined = filter_by_fold \u0026amp; filter_by_pvalue filtered = data2[filter_combined,] dim(filtered) ## [1] 42 6 head(filtered) ## WT.GSM130365 WT.GSM130366 WT.GSM130367 KO.GSM130368 ## 100716_at 4.852998 4.906891 5.626439 7.572890 ## 100914_at 10.340852 9.917074 10.250062 12.248787 ## 101368_at 9.937227 10.204693 10.385215 12.270354 ## 101550_at 5.526695 5.439623 6.221104 2.137504 ## 101635_f_at 7.105385 6.722466 6.943687 5.266787 ## 101883_s_at 5.768184 6.127221 5.133399 11.564292 ## KO.GSM130369 KO.GSM130370 ## 100716_at 7.791163 7.299208 ## 100914_at 12.185526 12.127124 ## 101368_at 12.213499 12.078184 ## 101550_at 2.906891 2.035624 ## 101635_f_at 4.842979 4.643856 ## 101883_s_at 11.679568 11.663514 # Let\u0026#39;s generate the volcano plot again, # highlighting the significantly differential expressed genes plot(fold, -log10(pvalue), main = \u0026quot;GSE5583 - Volcano #2\u0026quot;) points (fold[filter_combined], -log10(pvalue[filter_combined]), pch = 16, col = \u0026quot;red\u0026quot;) # Highlighting up-regulated in red and down-regulated in blue plot(fold, -log10(pvalue), main = \u0026quot;GSE5583 - Volcano #3\u0026quot;) points (fold[filter_combined \u0026amp; fold \u0026lt; 0], -log10(pvalue[filter_combined \u0026amp; fold \u0026lt; 0]), pch = 16, col = \u0026quot;red\u0026quot;) points (fold[filter_combined \u0026amp; fold \u0026gt; 0], -log10(pvalue[filter_combined \u0026amp; fold \u0026gt; 0]), pch = 16, col = \u0026quot;blue\u0026quot;) # Cluster the rows (genes) \u0026amp; columns (samples) by correlation rowv = as.dendrogram(hclust(as.dist(1-cor(t(filtered))))) colv = as.dendrogram(hclust(as.dist(1-cor(filtered)))) # Generate a heatmap heatmap(filtered, Rowv=rowv, Colv=colv, cexCol=0.7) library(gplots) # Enhanced heatmap heatmap.2(filtered, Rowv=rowv, Colv=colv, cexCol=0.7, col = rev(redblue(256)), scale = \u0026quot;row\u0026quot;, trace=\u0026quot;none\u0026quot;, density.info=\u0026quot;none\u0026quot;) # Save the heatmap to a PDF file pdf (\u0026quot;GSE5583_DE_Heatmap.pdf\u0026quot;) heatmap.2(filtered, Rowv=rowv, Colv=colv, cexCol=0.7, col = rev(redblue(256)), scale = \u0026quot;row\u0026quot;) dev.off() # Save the DE genes to a text file write.table (filtered, \u0026quot;GSE5583_DE.txt\u0026quot;, sep = \u0026quot;\\t\u0026quot;, quote = FALSE) n = nrow(filtered) cor.table = NULL x = NULL y = NULL cor.val = NULL cor.sig = NULL for (i in 1 : (n-1)) { x_name = rownames(filtered)[i] x_exps = filtered[i, ] for (j in (i+1) : n) { y_name = rownames(filtered)[j] y_exps = filtered[j, ] output = cor.test(x_exps,y_exps) x = c(x, x_name) y = c(y, y_name) cor.val = c(cor.val, output$estimate) cor.sig = c(cor.sig, output$p.value) } } cor.table = data.frame (x, y, cor.val, cor.sig) dim(cor.table) ## [1] 861 4 head(cor.table) ## x y cor.val cor.sig ## 1 100716_at 100914_at 0.9732295 0.0010653980 ## 2 100716_at 101368_at 0.9897688 0.0001564799 ## 3 100716_at 101550_at -0.9060431 0.0128271221 ## 4 100716_at 101635_f_at -0.9433403 0.0047245418 ## 5 100716_at 101883_s_at 0.9508680 0.0035616301 ## 6 100716_at 102712_at 0.9676037 0.0015572795 sig_cutoff = 0.001 cor.filtered = subset (cor.table, cor.sig \u0026lt; sig_cutoff) dim(cor.filtered) ## [1] 314 4 head(cor.filtered) ## x y cor.val cor.sig ## 2 100716_at 101368_at 0.9897688 1.564799e-04 ## 8 100716_at 103088_at -0.9761495 8.464861e-04 ## 10 100716_at 103299_at -0.9991089 1.190632e-06 ## 14 100716_at 104700_at -0.9792543 6.411095e-04 ## 15 100716_at 160172_at 0.9833552 4.132702e-04 ## 16 100716_at 160943_at 0.9814703 5.118449e-04  Sources Ahmed Moustafa githab\n ","date":1565308800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565308800,"objectID":"ecf339ec4949735e89102064b4b02e59","permalink":"/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis/","publishdate":"2019-08-09T00:00:00Z","relpermalink":"/post/bioinformatics/differential_expression_analysis/diferential_expression_analysis/","section":"post","summary":"Here you can find a beautiful example of differential expression analysis using standard R packages.","tags":["R","Statistics","Clustering","Hierarchical"],"title":"Differential expression analysis","type":"post"},{"authors":null,"categories":["Statistics"],"content":" Example 1: Iris Data description: Iris is a famous (Fisher’s or Anderson’s) data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.\n Structure of the data # select numerical columns 1-4 df = iris[1:4] head(df, n=5) # first rows ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3.0 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 tail(df, n=5) # last rows ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 146 6.7 3.0 5.2 2.3 ## 147 6.3 2.5 5.0 1.9 ## 148 6.5 3.0 5.2 2.0 ## 149 6.2 3.4 5.4 2.3 ## 150 5.9 3.0 5.1 1.8 dim(df) # data dimention ## [1] 150 4 nrow(df) # number of rows ## [1] 150 ncol(df) # number of columns ## [1] 4 str(df)  ## \u0026#39;data.frame\u0026#39;: 150 obs. of 4 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...  Correlation between variables Let’s find if data in columns are correlated using corrplot.\nHere you can find how to tune correlogram.\nlibrary(corrplot) ## corrplot 0.84 loaded # build correlation matrix cor.mx \u0026lt;- cor(df) # correlation plot ordered correlation coefficients corrplot::corrplot.mixed(cor.mx, order=\u0026quot;hclust\u0026quot;) print(cor.mx) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 1.0000000 -0.1175698 0.8717538 0.8179411 ## Sepal.Width -0.1175698 1.0000000 -0.4284401 -0.3661259 ## Petal.Length 0.8717538 -0.4284401 1.0000000 0.9628654 ## Petal.Width 0.8179411 -0.3661259 0.9628654 1.0000000 library(caret) # prepare training scheme control \u0026lt;- caret::trainControl(method=\u0026quot;repeatedcv\u0026quot;, number=10, repeats=3) #model \u0026lt;- train(diabetes~., data=PimaIndiansDiabetes, method=\u0026quot;lvq\u0026quot;, preProcess=\u0026quot;scale\u0026quot;, trControl=control) # estimate variable importance #importance \u0026lt;- varImp(model, scale=FALSE) # summarize importance #print(importance) How to show correlation table? Alternatively we can use corrplot with method = \u0026quot;number\u0026quot;.\nMissing data How to remove ‘NA’, ‘Inf’, and ‘0’\ncolSums(sapply(df, is.na)) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 0 0 0 0 # Scatterplot Matrices pairs(iris[, -5], bg = c(\u0026quot;yellow\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;black\u0026quot;)[iris$Species], pch = 21) How to choose variables with the best correlation? Can we sort them by importance?\nCan we exclude data based on low correlation? This can be also done by dimentionality reduction methods such as PCA.\nCan we remove outliers\nDo we need to remove duplicated data (rows, columns)\n Filtering You can see how to filter and subset your data in folloing posts: 1.\n2.\n3.\n ","date":1565308800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565308800,"objectID":"8d45c3f05330dd10138d27ac2a7929be","permalink":"/post/statistics/exploration/exploration/","publishdate":"2019-08-09T00:00:00Z","relpermalink":"/post/statistics/exploration/exploration/","section":"post","summary":"Exploratory analysis in R.","tags":["R","Statistics","Exploratory analysis"],"title":"Exploratory analysis in R","type":"post"},{"authors":null,"categories":["Statistics"],"content":"  Compare Models And Select The Best Using The Caret R Package Bibliography   Compare Models And Select The Best Using The Caret R Package Data: mlbench::PimaIndiansDiabetes. Find the best model to predict diabetes from all given parameters.\nModels:\n* Learning Vector Quantization (LVQ)\n* Gradient Boosted Machine (GBM)\n* Support Vector Machine (SVM)\nEach model is automatically tuned and is evaluated using 3 repeats of 10-fold cross validation.\nThe random number seed is set before each algorithm is trained to ensure that each algorithm gets the same data partitions and repeats.\nThe best models have 30 results (3 repeats of 10-fold cross validation).\nThe objective of comparing results is to compare the accuracy distributions (30 values) between the models.\nThis is done in three ways. The distributions are summarized in terms of the percentiles. The distributions are summarized as box plots and finally the distributions are summarized as dot plots.\nlibrary(mlbench) library(caret) ## Loading required package: lattice ## Loading required package: ggplot2 # load the dataset data(PimaIndiansDiabetes) # training scheme control \u0026lt;- trainControl(method=\u0026quot;repeatedcv\u0026quot;, number=10, repeats=3) # train the LVQ model set.seed(7) modelLvq \u0026lt;- train(diabetes~., data=PimaIndiansDiabetes, method=\u0026quot;lvq\u0026quot;, trControl=control) # train the GBM model set.seed(7) modelGbm \u0026lt;- train(diabetes~., data=PimaIndiansDiabetes, method=\u0026quot;gbm\u0026quot;, trControl=control, verbose=FALSE) # train the SVM model set.seed(7) modelSvm \u0026lt;- train(diabetes~., data=PimaIndiansDiabetes, method=\u0026quot;svmRadial\u0026quot;, trControl=control) # collect resamples results \u0026lt;- resamples(list(LVQ=modelLvq, GBM=modelGbm, SVM=modelSvm)) # summarize the distributions summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: LVQ, GBM, SVM ## Number of resamples: 30 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s ## LVQ 0.5974026 0.6623377 0.7012987 0.6992538 0.7402597 0.7922078 0 ## GBM 0.7012987 0.7402597 0.7662338 0.7678685 0.8045540 0.8552632 0 ## SVM 0.6973684 0.7305195 0.7662338 0.7665243 0.7922078 0.8441558 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA\u0026#39;s ## LVQ 0.04251905 0.2444627 0.3210038 0.3064691 0.3989071 0.5276074 0 ## GBM 0.24798301 0.3770808 0.4441549 0.4563312 0.5264481 0.6814024 0 ## SVM 0.25171233 0.3670435 0.4590164 0.4500126 0.5211405 0.6457055 0 # boxplots of results bwplot(results) # dot plots of results dotplot(results)  Bibliography Compare Models And Select The Best Using The Caret R Package\n ","date":1565222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565222400,"objectID":"de6547e7f4f61cb79fa53ac4adca5d7a","permalink":"/post/statistics/compare_models_caret/compare_models_caret/","publishdate":"2019-08-08T00:00:00Z","relpermalink":"/post/statistics/compare_models_caret/compare_models_caret/","section":"post","summary":"Compare Models And Select The Best Using The Caret R Package.","tags":["R","Statistics","caret","Machine Learning"],"title":"Compare Models And Select The Best Using The Caret R Package","type":"post"},{"authors":null,"categories":["Statistics"],"content":"  Gradient Boosting Machine   Gradient Boosting Machine Type of problem: regression and supervised classification.\n ","date":1565222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565222400,"objectID":"db5c69b707aad5257b9571bce8438619","permalink":"/post/statistics/gbm/gbm/","publishdate":"2019-08-08T00:00:00Z","relpermalink":"/post/statistics/gbm/gbm/","section":"post","summary":"Gradient Boosting Machine","tags":["R","Statistics","Machine Learning"],"title":"Gradient Boosting Machine","type":"post"},{"authors":null,"categories":["Statistics"],"content":" K-means cluster analysis library(datasets) df \u0026lt;- datasets::iris head(df) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa There are 3 species:\nunique(df$Species) ## [1] setosa versicolor virginica ## Levels: setosa versicolor virginica We try to predict them by Petal.Lenghtand Petal.Width variables using k-means clustering.\n# Plot Petal.Length ~ Petal.Width data plot(df$Petal.Length ~ df$Petal.Width) # Find number of clusters using wss wss \u0026lt;- (nrow(df[, 3:4])-1)*sum(apply(df[, 3:4],2,var)) for (i in 2:15) wss[i] \u0026lt;- sum(kmeans(df[, 3:4], i)$withinss) plot(1:15, wss, type=\u0026quot;b\u0026quot;, xlab=\u0026quot;Number of Clusters\u0026quot;, ylab=\u0026quot;Within groups sum of squares\u0026quot;) More than 3 clusters give no obvious advantages.\n# Make k-means with 3 clasters ncl \u0026lt;- 3 cl \u0026lt;- stats::kmeans(df[, 3:4], ncl, nstart = 20) cl ## K-means clustering with 3 clusters of sizes 48, 50, 52 ## ## Cluster means: ## Petal.Length Petal.Width ## 1 5.595833 2.037500 ## 2 1.462000 0.246000 ## 3 4.269231 1.342308 ## ## Clustering vector: ## [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [36] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 ## [71] 3 3 3 3 3 3 3 1 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 ## [106] 1 3 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 3 1 ## [141] 1 1 1 1 1 1 1 1 1 1 ## ## Within cluster sum of squares by cluster: ## [1] 16.29167 2.02200 13.05769 ## (between_SS / total_SS = 94.3 %) ## ## Available components: ## ## [1] \u0026quot;cluster\u0026quot; \u0026quot;centers\u0026quot; \u0026quot;totss\u0026quot; \u0026quot;withinss\u0026quot; ## [5] \u0026quot;tot.withinss\u0026quot; \u0026quot;betweenss\u0026quot; \u0026quot;size\u0026quot; \u0026quot;iter\u0026quot; ## [9] \u0026quot;ifault\u0026quot; # Compair result of clustering with real data (3 species of iris are in analysis) table(cl$cluster, df$Species) ## ## setosa versicolor virginica ## 1 0 2 46 ## 2 50 0 0 ## 3 0 48 4 # Plot data clusters \u0026lt;- split.data.frame(df, cl$cluster) xlim \u0026lt;- c(min(df$Petal.Width), max(df$Petal.Width)) ylim \u0026lt;- c(min(df$Petal.Length), max(df$Petal.Length)) col \u0026lt;- c(\u0026#39;red\u0026#39;, \u0026#39;green\u0026#39;, \u0026#39;blue\u0026#39;) plot(0, xlab=\u0026#39;Petal width\u0026#39;, ylab=\u0026#39;Petal length\u0026#39;, xlim=xlim, ylim=ylim) for ( i in 1:ncl ) { points(clusters[[i]]$Petal.Length ~ clusters[[i]]$Petal.Width, col=col[i], xlim=xlim, ylim=ylim) }  ","date":1565222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565222400,"objectID":"10bf6633e10f93b660ccda445684e3e5","permalink":"/post/statistics/k_means/k_means/","publishdate":"2019-08-08T00:00:00Z","relpermalink":"/post/statistics/k_means/k_means/","section":"post","summary":"Here you can find how to work with k-means cluster analysis.","tags":["R","Statistics","Clustering","k-means"],"title":"k-means","type":"post"},{"authors":null,"categories":["Statistics"],"content":"  Learning Vector Quantization   Learning Vector Quantization Learning Vector Quantiztion (LVQ) is a supervised classification algorithm for binary and multiclass problems. LVQ is a special case of a neural network.\nLVQ model creates codebook vectors by learning training dataset. Codebook vectors represent class regions. They contain elements that placed around the respective class according to their matching level. If the element matches, it comes closer to the target class, if it does not match, it moves farther from it. With this codebooks, the model classifies new data. Here is a nice explanation how it works.\nThere are several versions of LVQ function:\nlvq1(), olvq1(), lvq2(), lvq3(), dlvq().\nlibrary(class) # olvq1() library(caret) # to split data ## Loading required package: lattice ## Loading required package: ggplot2 # generate dataset df \u0026lt;- iris id = caret::createDataPartition(df$Species, p = .8, list = F) train = df[id, ] test = df[-id, ] # initialize an LVQ codebook cb = class::lvqinit(train[1:4], train$Species) # training set in a codebook. build.cb = class::olvq1(train[1:4], train$Species, cb) # classify test set from LVQ Codebook for test data predict = class::lvqtest(build.cb, test[1:4]) # confusion matrix. caret::confusionMatrix(test$Species, predict) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 10 0 0 ## versicolor 0 9 1 ## virginica 0 0 10 ## ## Overall Statistics ## ## Accuracy : 0.9667 ## 95% CI : (0.8278, 0.9992) ## No Information Rate : 0.3667 ## P-Value [Acc \u0026gt; NIR] : 4.476e-12 ## ## Kappa : 0.95 ## ## Mcnemar\u0026#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 1.0000 0.9091 ## Specificity 1.0000 0.9524 1.0000 ## Pos Pred Value 1.0000 0.9000 1.0000 ## Neg Pred Value 1.0000 1.0000 0.9500 ## Prevalence 0.3333 0.3000 0.3667 ## Detection Rate 0.3333 0.3000 0.3333 ## Detection Prevalence 0.3333 0.3333 0.3333 ## Balanced Accuracy 1.0000 0.9762 0.9545  ","date":1565222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565222400,"objectID":"12407455e69a93d4db8a10123ac086d1","permalink":"/post/statistics/lvq/lvq/","publishdate":"2019-08-08T00:00:00Z","relpermalink":"/post/statistics/lvq/lvq/","section":"post","summary":"Learning Vector Quantization using R","tags":["R","Statistics","Machine Learning"],"title":"Learning Vector Quantization","type":"post"},{"authors":null,"categories":["Statistics"],"content":"  Naive Bayes   Naive Bayes \\(P(c|x) = \\frac{P(x|c)(P(c))}{P(x)}\\), where\n\\(P(c|x)\\) - posteriour probability\n\\(P(x|c)\\) - Likelihood\n\\(P(c)\\) - Class Prior Probbility\n\\(P(x)\\) - Predictor Prior Probability\n ","date":1565222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565222400,"objectID":"66d97da449a6c196852a6b95a08917f4","permalink":"/post/statistics/naive_bayes/naive_bayes/","publishdate":"2019-08-08T00:00:00Z","relpermalink":"/post/statistics/naive_bayes/naive_bayes/","section":"post","summary":"Naive Bayes","tags":["R","Statistics","caret","Machine Learning"],"title":"Naive Bayes","type":"post"},{"authors":null,"categories":["Statistics"],"content":"  Support Vector Machine   Support Vector Machine Type of problem: regression, supervised classification.\n ","date":1565222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565222400,"objectID":"3c41de0ec97631d889e165a995417499","permalink":"/post/statistics/svm/svm/","publishdate":"2019-08-08T00:00:00Z","relpermalink":"/post/statistics/svm/svm/","section":"post","summary":"Support Vector Machine.","tags":["R","Statistics","caret","Machine Learning"],"title":"Support Vector Machine","type":"post"},{"authors":null,"categories":["R","Statistics"],"content":" Here you can find several simple approaches to split data into train and test subset to fit and to test parameters of your model. We want to take 0.8 of our initial data to train our model.\nData: datasets::iris.\nFirst approach is to create a vector containing randomly selected row ids and to apply this vector to split data.  inTrain = sample(nrow(iris), nrow(iris)*0.8) # split data train = iris[inTrain, ] test = iris[-inTrain, ] The same idea to split data as before using caret package.\nThe advantage is that createDataPartition function allows to split data many times and use these subsets to estimate parameters of our model.  library(caret) trainIndex \u0026lt;- createDataPartition(iris$Species, p=.8, list = FALSE, # if FALSE - create a vector/matrix, if TRUE - create a list times = 1) # how many subsets # split data train \u0026lt;- iris[trainIndex, ] test \u0026lt;- iris[-trainIndex, ] Another approch is to create a logical vecotor containing randomly distributed true/false and apply this vector to subset data.  inTrain = sample(c(TRUE, FALSE), nrow(iris), replace = T, prob = c(0.8,0.2)) # select data train = iris[inTrain, ] test = iris[!inTrain, ] Using caTools.  library(caTools) inTrain = sample.split(iris, SplitRatio = .8) train = subset(iris, inTrain == TRUE) test = subset(iris, inTrain == FALSE) Using dplyr  library(dplyr) iris$id \u0026lt;- 1:nrow(iris) train \u0026lt;- iris %\u0026gt;% dplyr::sample_frac(.8) test \u0026lt;- dplyr::anti_join(iris, train, by = \u0026#39;id\u0026#39;) ","date":1565136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565136000,"objectID":"6ba07d8f45246b3fd119ee889a1e4e61","permalink":"/post/statistics/split_data_ways/split_data_ways/","publishdate":"2019-08-07T00:00:00Z","relpermalink":"/post/statistics/split_data_ways/split_data_ways/","section":"post","summary":"Here you will learn approaches to split your data into subsets - train and test for your modeling.","tags":["R","Statistics"],"title":"How to split data into train and test subsets?","type":"post"},{"authors":null,"categories":["Statistics"],"content":" Using of neuralnet R package to tran Neural Network for classification.\nlibrary(neuralnet) library(datasets) # data df \u0026lt;- datasets::iris head(df) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa # split data train.ratio = 0.9 n = nrow(df) train.id \u0026lt;- sample(n, size = n*train.ratio) train \u0026lt;- df[train.id,] test \u0026lt;- df[-train.id,] Binary classification nn \u0026lt;- neuralnet(Species == \u0026quot;setosa\u0026quot; ~ Petal.Length + Petal.Width, train, linear.output = FALSE) # Predict for test data pred \u0026lt;- predict(nn, test) # Confusion matrix t \u0026lt;- table(test$Species == \u0026quot;setosa\u0026quot;, pred[, 1] \u0026gt; 0.5) t ## ## FALSE TRUE ## FALSE 13 0 ## TRUE 0 2 # plot NN plot(nn) # Confusion matrix using caret library(caret) ## Loading required package: lattice ## Loading required package: ggplot2 caret::confusionMatrix(t) ## Confusion Matrix and Statistics ## ## ## FALSE TRUE ## FALSE 13 0 ## TRUE 0 2 ## ## Accuracy : 1 ## 95% CI : (0.782, 1) ## No Information Rate : 0.8667 ## P-Value [Acc \u0026gt; NIR] : 0.1169 ## ## Kappa : 1 ## ## Mcnemar\u0026#39;s Test P-Value : NA ## ## Sensitivity : 1.0000 ## Specificity : 1.0000 ## Pos Pred Value : 1.0000 ## Neg Pred Value : 1.0000 ## Prevalence : 0.8667 ## Detection Rate : 0.8667 ## Detection Prevalence : 0.8667 ## Balanced Accuracy : 1.0000 ## ## \u0026#39;Positive\u0026#39; Class : FALSE ##   Multiclass classification nn \u0026lt;- neuralnet((Species == \u0026quot;setosa\u0026quot;) + (Species == \u0026quot;versicolor\u0026quot;) + (Species == \u0026quot;virginica\u0026quot;) ~ Petal.Length + Petal.Width, train, linear.output = FALSE) # Predict for test data pred \u0026lt;- predict(nn, test) table(test$Species, apply(pred, 1, which.max)) ## ## 1 2 3 ## setosa 2 0 0 ## versicolor 0 3 1 ## virginica 0 0 9 # Plot NN plot(nn) # Confusion matrix using \u0026#39;caret\u0026#39; caret::confusionMatrix(t) ## Confusion Matrix and Statistics ## ## ## FALSE TRUE ## FALSE 13 0 ## TRUE 0 2 ## ## Accuracy : 1 ## 95% CI : (0.782, 1) ## No Information Rate : 0.8667 ## P-Value [Acc \u0026gt; NIR] : 0.1169 ## ## Kappa : 1 ## ## Mcnemar\u0026#39;s Test P-Value : NA ## ## Sensitivity : 1.0000 ## Specificity : 1.0000 ## Pos Pred Value : 1.0000 ## Neg Pred Value : 1.0000 ## Prevalence : 0.8667 ## Detection Rate : 0.8667 ## Detection Prevalence : 0.8667 ## Balanced Accuracy : 1.0000 ## ## \u0026#39;Positive\u0026#39; Class : FALSE ##   Neuralnet with 2 hidden neurons library(datasets) library(neuralnet) library(caret) # data df \u0026lt;- datasets::iris head(df) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa # split data train.ratio = 0.9 n = nrow(df) train.id \u0026lt;- sample(n, size = n*train.ratio) train \u0026lt;- df[train.id,] test \u0026lt;- df[-train.id,] nn \u0026lt;- neuralnet(Species == \u0026quot;versicolor\u0026quot; ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, train, hidden = 2, linear.output = FALSE) # Predict for test data pred \u0026lt;- predict(nn, test) # Confusion matrix t \u0026lt;- table(test$Species == \u0026quot;versicolor\u0026quot;, pred[, 1] \u0026gt; 0.5) # Confusion matrix caret::confusionMatrix(t) ## Confusion Matrix and Statistics ## ## ## FALSE TRUE ## FALSE 5 6 ## TRUE 0 4 ## ## Accuracy : 0.6 ## 95% CI : (0.3229, 0.8366) ## No Information Rate : 0.6667 ## P-Value [Acc \u0026gt; NIR] : 0.79696 ## ## Kappa : 0.3077 ## ## Mcnemar\u0026#39;s Test P-Value : 0.04123 ## ## Sensitivity : 1.0000 ## Specificity : 0.4000 ## Pos Pred Value : 0.4545 ## Neg Pred Value : 1.0000 ## Prevalence : 0.3333 ## Detection Rate : 0.3333 ## Detection Prevalence : 0.7333 ## Balanced Accuracy : 0.7000 ## ## \u0026#39;Positive\u0026#39; Class : FALSE ##  # plot NN plot(nn, rep=\u0026#39;best\u0026#39;) # compair predicted and actual values test$predicted \u0026lt;- ifelse(pred\u0026gt;0.5, 1, 0) test ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species predicted ## 12 4.8 3.4 1.6 0.2 setosa 0 ## 19 5.7 3.8 1.7 0.3 setosa 0 ## 25 4.8 3.4 1.9 0.2 setosa 0 ## 40 5.1 3.4 1.5 0.2 setosa 0 ## 42 4.5 2.3 1.3 0.3 setosa 0 ## 64 6.1 2.9 4.7 1.4 versicolor 1 ## 70 5.6 2.5 3.9 1.1 versicolor 1 ## 74 6.1 2.8 4.7 1.2 versicolor 1 ## 90 5.5 2.5 4.0 1.3 versicolor 1 ## 101 6.3 3.3 6.0 2.5 virginica 1 ## 103 7.1 3.0 5.9 2.1 virginica 1 ## 114 5.7 2.5 5.0 2.0 virginica 1 ## 123 7.7 2.8 6.7 2.0 virginica 1 ## 124 6.3 2.7 4.9 1.8 virginica 1 ## 145 6.7 3.3 5.7 2.5 virginica 1  Bibliography  ","date":1565136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565136000,"objectID":"c89681cf2555fc246994b8726d34f200","permalink":"/post/statistics/neuralnet_r/neuralnet_r/","publishdate":"2019-08-07T00:00:00Z","relpermalink":"/post/statistics/neuralnet_r/neuralnet_r/","section":"post","summary":"Examples of Neural Network classification using Neuralnet R package.","tags":["R","Statistics","Classification","Neural Network"],"title":"Neuralnet R","type":"post"},{"authors":null,"categories":["Statistics"],"content":" Uniform Manifold Approximation and Projection (UMAP) library(umap) head(iris, 3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa # select determinative variables and lables dat = iris[, grep(\u0026quot;Sepal|Petal\u0026quot;, colnames(iris))] lbls = iris[, \u0026quot;Species\u0026quot;] # apply UMAP transformation set.seed(123) iris.umap = umap::umap(dat) # layout matrix (coordinates for vis.) mx \u0026lt;- iris.umap$layout head(mx, 5) ## [,1] [,2] ## [1,] 16.34555 1.7888418 ## [2,] 14.74415 0.8264381 ## [3,] 14.58204 1.3219384 ## [4,] 14.55112 1.3763256 ## [5,] 16.64683 2.1515565 # plot UMAP components plot(subset(mx, lbls == \u0026#39;setosa\u0026#39;), col = \u0026#39;red\u0026#39;, xlim=c(min(mx[,1]), max(mx[,1])), ylim=c(min(mx[,2]), max(mx[,2]))) points(subset(mx, lbls == \u0026#39;virginica\u0026#39;), col = \u0026#39;green\u0026#39;) points(subset(mx, lbls == \u0026#39;versicolor\u0026#39;), col = \u0026#39;blue\u0026#39;) # generate test data by adding noise to original data iris.wnoise = dat + matrix(rnorm(150*40, 0, 0.1), ncol=4) colnames(iris.wnoise) = colnames(dat) head(iris.wnoise, 3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 5.043952 3.578774 1.328476 0.3430402 ## 2 4.876982 3.076904 1.324731 0.3046629 ## 3 4.855871 3.233220 1.206146 0.2435289 # predict pmx = predict(iris.umap, iris.wnoise) head(pmx, 3) ## [,1] [,2] ## 1 16.28067 1.5141526 ## 2 14.81119 0.4305712 ## 3 15.15975 1.3219315 # plot generated test data plot(subset(pmx, lbls == \u0026#39;setosa\u0026#39;), col = \u0026#39;red\u0026#39;, xlim=c(min(pmx[,1]), max(pmx[,1])), ylim=c(min(pmx[,2]), max(pmx[,2]))) points(subset(pmx, lbls == \u0026#39;virginica\u0026#39;), col = \u0026#39;green\u0026#39;) points(subset(pmx, lbls == \u0026#39;versicolor\u0026#39;), col = \u0026#39;blue\u0026#39;) There are two strategies for tuning: via configuration objects (1) and via additional arguments (2).\numap.defaults - configuration object.  # set parameters custom.config = umap.defaults custom.config$random_state = 321 umap3 = umap(dat, custom.config) mx2 = iris.umap$layout plot(subset(mx2, lbls == \u0026#39;setosa\u0026#39;), col = \u0026#39;red\u0026#39;, xlim=c(min(mx2[,1]), max(mx2[,1])), ylim=c(min(mx2[,2]), max(mx2[,2])), main=\u0026quot;Another UMAP visualization (different seed)\u0026quot;) points(subset(mx2, lbls == \u0026#39;virginica\u0026#39;), col = \u0026#39;green\u0026#39;) points(subset(mx2, lbls == \u0026#39;versicolor\u0026#39;), col = \u0026#39;blue\u0026#39;) Additional arguments iris.umap.3 = umap(iris.data, random_state=123)   Bibliography UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction\nUniform Manifold Approximation and Projection in R\n ","date":1565136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565136000,"objectID":"5994c41a83d5579790bbacb00611113b","permalink":"/post/statistics/umap/umap/","publishdate":"2019-08-07T00:00:00Z","relpermalink":"/post/statistics/umap/umap/","section":"post","summary":"Unsupervised method for dimentionality reduction - Uniform Manifold Approximation and Projection (UMAP).","tags":["R","Statistics","Dimentionality reduction"],"title":"Uniform Manifold Approximation and Projection (UMAP)","type":"post"},{"authors":null,"categories":["Statistics"],"content":"  Hierarchical cluster analysis Choosing optimal number of clusters Elbow method  Choosing the best clustering model Hierarchical cluster analysis Bibliography   Hierarchical cluster analysis library(tidyverse) Prepare data for analysis datasets::USArrests\nlibrary(datasets) df \u0026lt;- datasets::USArrests head(df) ## Murder Assault UrbanPop Rape ## Alabama 13.2 236 58 21.2 ## Alaska 10.0 263 48 44.5 ## Arizona 8.1 294 80 31.0 ## Arkansas 8.8 190 50 19.5 ## California 9.0 276 91 40.6 ## Colorado 7.9 204 78 38.7 # check if \u0026#39;NA\u0026#39; values present in data any(is.na(df)) ## [1] FALSE # remove \u0026#39;NA\u0026#39; if necessary df \u0026lt;- na.omit(df) # normalize df \u0026lt;- scale(df)  Choosing optimal number of clusters Elbow method plot k ~ wss, where k - is a cluseter number and wss is a total within-cluster sum of square.\nwss \u0026lt;- (nrow(df)-1)*sum(apply(df,2,var)) for (i in 2:15) wss[i] \u0026lt;- sum(kmeans(df, centers=i)$withinss) plot(1:15, wss, type=\u0026quot;b\u0026quot;, xlab=\u0026quot;Number of Clusters\u0026quot;, ylab=\u0026quot;Within groups sum of squares\u0026quot;) This diagram shows that 4 number of clusers is optimal for this dataset.\n  Choosing the best clustering model hc2 \u0026lt;- cluster::agnes(df, method=\u0026#39;complete\u0026#39;) hc2$ac ## [1] 0.8531583 This metric allows to estimate the quality of cluster. Closer to 1 is better.\nUsing this metric we can try several models and choose the best one.\nm \u0026lt;- c( \u0026quot;average\u0026quot;, \u0026quot;single\u0026quot;, \u0026quot;complete\u0026quot;, \u0026quot;ward\u0026quot;) names(m) \u0026lt;- c( \u0026quot;average\u0026quot;, \u0026quot;single\u0026quot;, \u0026quot;complete\u0026quot;, \u0026quot;ward\u0026quot;) # function to compute coefficient ac \u0026lt;- function(x) { cluster::agnes(df, method = x)$ac } purrr::map_dbl(m, ac) ## average single complete ward ## 0.7379371 0.6276128 0.8531583 0.9346210 As we can see the ward method gives the best clustering.\n Hierarchical cluster analysis Let’s first split data into 4 groups using clust::hclust function.\n# calculate distances d \u0026lt;- dist(df, method=\u0026#39;euclidean\u0026#39;) # hierarchical cluster analysis # \u0026#39;ward.D2\u0026#39; method is equivalent of agnes \u0026#39;ward\u0026#39; hc1 \u0026lt;- hclust(d, method=\u0026#39;ward.D2\u0026#39;) # Plot the obtained dendrogram plot(hc1, hang = -1, cex = 0.6) # show 4 clusers rect.hclust(hc1, k=4, border=\u0026quot;blue\u0026quot;) # group data by clusters groups \u0026lt;- cutree(hc1, k=3) names(groups[groups == 1]) ## [1] \u0026quot;Alabama\u0026quot; \u0026quot;Alaska\u0026quot; \u0026quot;Arizona\u0026quot; \u0026quot;California\u0026quot; ## [5] \u0026quot;Colorado\u0026quot; \u0026quot;Florida\u0026quot; \u0026quot;Georgia\u0026quot; \u0026quot;Illinois\u0026quot; ## [9] \u0026quot;Louisiana\u0026quot; \u0026quot;Maryland\u0026quot; \u0026quot;Michigan\u0026quot; \u0026quot;Mississippi\u0026quot; ## [13] \u0026quot;Nevada\u0026quot; \u0026quot;New Mexico\u0026quot; \u0026quot;New York\u0026quot; \u0026quot;North Carolina\u0026quot; ## [17] \u0026quot;South Carolina\u0026quot; \u0026quot;Tennessee\u0026quot; \u0026quot;Texas\u0026quot; # check for cluster metrics names(hc1) ## [1] \u0026quot;merge\u0026quot; \u0026quot;height\u0026quot; \u0026quot;order\u0026quot; \u0026quot;labels\u0026quot; \u0026quot;method\u0026quot; ## [6] \u0026quot;call\u0026quot; \u0026quot;dist.method\u0026quot; Now we can split data into 4 groups using cluster::agnes function.\n# using \u0026#39;agnes\u0026#39; for hierarhical clustering hc3 \u0026lt;- cluster::agnes(df, method=\u0026#39;ward\u0026#39;) # plot slaster cluster::pltree(hc3, hang = -1, cex = 0.6) # split into groups groups \u0026lt;- cutree(as.hclust(hc3), k = 4) groups[groups==1] ## Alabama Georgia Louisiana Mississippi North Carolina ## 1 1 1 1 1 ## South Carolina Tennessee ## 1 1  Bibliography UC Business Analytics R Programming Guide\n ","date":1565049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565049600,"objectID":"05b6af7752c50155c7e794c23a8175b8","permalink":"/post/statistics/hierarchical_cluster/hierarchical_cluser/","publishdate":"2019-08-06T00:00:00Z","relpermalink":"/post/statistics/hierarchical_cluster/hierarchical_cluser/","section":"post","summary":"Here you can find how to work with hierarchical cluster analysis, how choose number of cluseter and how to choose the best model.","tags":["R","Statistics","Clustering","Hierarchical"],"title":"Hierarchical cluster analysis","type":"post"},{"authors":null,"categories":["Statistics"],"content":"  Background Binomial logistic regression Logistic regression Linear Discriminant Analysis (LDA) Quadratic Discriminant Analysis (QDA) ROC-curve for LDA Tasks   Background Logistic regression builds model for binary dependent variables (0/1, True/False).\nLogistic function: \\[Y = \\frac{1}{1+e^l} = \\frac{e^l}{e^l+1}\\] where \\(l\\) is a linear combination of all observations (log-odds): \\(l = \\beta_0 + \\beta_{1}x_{1} + \\beta_{2}x_{2} + ... + \\beta_{p}x_{p} + \\epsilon\\)\nSee also: Sigmoid functions:\n Binomial logistic regression Probability of passing an exam versus hours of study.\nData from wiki describe if students pass exam depending of how many hours they studied.\nWe build logistic regression model to predict if ‘pass’ depending on learning ‘hours’.\n# put data into dataframe hours=c(0.50,.75,1,1.25,1.5,1.75,1.75,2,2.25,2.5,2.75,3,3.25,3.50,4,4.25,4.5,4.75,5,5.5) pass=c(0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1) df = data.frame(hours, pass) # Logistic Regression model model.logit \u0026lt;- glm(pass ~ hours, data = df, family = \u0026#39;binomial\u0026#39;) summary(model.logit) ## ## Call: ## glm(formula = pass ~ hours, family = \u0026quot;binomial\u0026quot;, data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.70557 -0.57357 -0.04654 0.45470 1.82008 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -4.0777 1.7610 -2.316 0.0206 * ## hours 1.5046 0.6287 2.393 0.0167 * ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 27.726 on 19 degrees of freedom ## Residual deviance: 16.060 on 18 degrees of freedom ## AIC: 20.06 ## ## Number of Fisher Scoring iterations: 5 Coefficients Intercept = -4.0777 and Hours = 1.5046 are entered in the logistic regression equation to estimate the odds (probability) of passing the exam: \\(1/(1+e^{-(-4.0777+1.5046\\cdot hours)})\\) Calculate the probability to pass exam if studied 4 hours:\n1/(1+exp(-(-4.0777+1.5046*4))) ## [1] 0.874429 Let’s find a critical point where probability is 0.5:\ncrit = -coef(model.logit)[1]/coef(model.logit)[2] crit ## (Intercept) ## 2.710083 # predict \u0026#39;pass\u0026#39; for given data df$predic.prob \u0026lt;- predict(model.logit, df, type=\u0026quot;response\u0026quot;) df$predic.pass \u0026lt;- ifelse(df$predic.prob \u0026gt; 0.5, 1, 0) df ## hours pass predic.prob predic.pass ## 1 0.50 0 0.03471034 0 ## 2 0.75 0 0.04977295 0 ## 3 1.00 0 0.07089196 0 ## 4 1.25 0 0.10002862 0 ## 5 1.50 0 0.13934447 0 ## 6 1.75 0 0.19083650 0 ## 7 1.75 1 0.19083650 0 ## 8 2.00 0 0.25570318 0 ## 9 2.25 1 0.33353024 0 ## 10 2.50 0 0.42162653 0 ## 11 2.75 1 0.51501086 1 ## 12 3.00 0 0.60735865 1 ## 13 3.25 1 0.69261733 1 ## 14 3.50 0 0.76648084 1 ## 15 4.00 1 0.87444750 1 ## 16 4.25 1 0.91027764 1 ## 17 4.50 1 0.93662366 1 ## 18 4.75 1 0.95561071 1 ## 19 5.00 1 0.96909707 1 ## 20 5.50 1 0.98519444 1 # plot data plot(df$hours, df$pass, pch=19, col=\u0026#39;black\u0026#39;, main=\u0026#39;Probability of Passing Exam vs Hours Studying\u0026#39;, ylab=\u0026#39;Probability of Passing Exam\u0026#39;, xlab=\u0026#39;Hours Studying\u0026#39;) # data frame to build a logistic function curve \u0026#39;hours~pass\u0026#39; df2 \u0026lt;- data.frame(hours=seq(min(df$hours),max(df$hours),0.1), pass=NA) # predict \u0026#39;pass\u0026#39; from our model df2$pass \u0026lt;- predict(model.logit, df2, type=\u0026quot;response\u0026quot;) # draw logistic function for our data sets lines(df2$pass~df2$hours, lwd=2) # critical point abline(h=0.5, col=\u0026#39;green\u0026#39;) # abline(v=crit, col=\u0026#39;red\u0026#39;) # # draw predicted points (-0.02 to avoid overlapping with actual data) points(df$hours, df$predic.pass-0.03, pch=19, col=\u0026#39;red\u0026#39;) legend(\u0026#39;bottomright\u0026#39;, lty=c(1,1,1,1), col = c(\u0026#39;black\u0026#39;, \u0026#39;red\u0026#39;, \u0026#39;black\u0026#39;, \u0026#39;green\u0026#39;, \u0026#39;red\u0026#39;), legend = c(\u0026#39;actual data\u0026#39;, \u0026#39;predicted\u0026#39;, \u0026#39;Logistic function\u0026#39;, \u0026#39;Decision p\u0026#39;, \u0026#39;Decision bound\u0026#39;), lwd=c(NA,NA,1,1,1), pch=c(19,19,NA,NA,NA), bty = \u0026#39;n\u0026#39;)  Logistic regression Data: generated credit card balance Default{ISLR}. 10000 observations for 4 variables:\ndefault – binary variable: Yes, if credit card holder did not return debt;\nstudent – binary variable: Yes, if credit card holder is a student;\nbalance – average month balance on the bank account;\nincome – income of credit card holder.\nlibrary(\u0026#39;ISLR\u0026#39;) head(Default) ## default student balance income ## 1 No No 729.5265 44361.625 ## 2 No Yes 817.1804 12106.135 ## 3 No No 1073.5492 31767.139 ## 4 No No 529.2506 35704.494 ## 5 No No 785.6559 38463.496 ## 6 No Yes 919.5885 7491.559 set.seed(1) # train subset rate is 0.85 inTrain \u0026lt;- sample(seq_along(Default$default), nrow(Default)*0.85) df \u0026lt;- Default[inTrain, ] # logistic regression model \u0026#39;default ~ f(balance)\u0026#39; model.logit \u0026lt;- glm(default ~ balance, data = df, family = \u0026#39;binomial\u0026#39;) summary(model.logit) ## ## Call: ## glm(formula = default ~ balance, family = \u0026quot;binomial\u0026quot;, data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3282 -0.1420 -0.0553 -0.0201 3.7934 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -1.088e+01 4.022e-01 -27.07 \u0026lt;2e-16 *** ## balance 5.657e-03 2.448e-04 23.11 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2509.1 on 8499 degrees of freedom ## Residual deviance: 1336.5 on 8498 degrees of freedom ## AIC: 1340.5 ## ## Number of Fisher Scoring iterations: 8 # Predict \u0026#39;default\u0026#39; by \u0026#39;balance\u0026#39; p.logit \u0026lt;- predict(model.logit, df, type = \u0026#39;response\u0026#39;) predicted \u0026lt;- factor(ifelse(p.logit \u0026gt; 0.5, 2, 1), levels = c(1, 2), labels = c(\u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39;)) # true values for \u0026#39;default\u0026#39; in train data actual \u0026lt;- df$default # confusion matrix conf.m \u0026lt;- table(actual, predicted) conf.m ## predicted ## actual No Yes ## No 8172 41 ## Yes 194 93 # sensitivity conf.m[2, 2] / sum(conf.m[2, ]) ## [1] 0.3240418 # specificity conf.m[1, 1] / sum(conf.m[1, ]) ## [1] 0.9950079 # probability sum(diag(conf.m)) / sum(conf.m) ## [1] 0.9723529  Linear Discriminant Analysis (LDA) #library(\u0026#39;GGally\u0026#39;) library(\u0026#39;MASS\u0026#39;) model.lda \u0026lt;- lda(default ~ balance, data = Default[inTrain, ]) model.lda ## Call: ## lda(default ~ balance, data = Default[inTrain, ]) ## ## Prior probabilities of groups: ## No Yes ## 0.96623529 0.03376471 ## ## Group means: ## balance ## No 801.1297 ## Yes 1757.2025 ## ## Coefficients of linear discriminants: ## LD1 ## balance 0.00220817 # Predict p.lda \u0026lt;- predict(model.lda, df, type = \u0026#39;response\u0026#39;) actual \u0026lt;- factor(ifelse(p.lda$posterior[, \u0026#39;Yes\u0026#39;] \u0026gt; 0.5, 2, 1), levels = c(1, 2), labels = c(\u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39;)) # confusion matrix conf.m \u0026lt;- table(actual, predicted) conf.m ## predicted ## actual No Yes ## No 8366 42 ## Yes 0 92 # sensitivity conf.m[2, 2] / sum(conf.m[2, ]) ## [1] 1 # specificity conf.m[1, 1] / sum(conf.m[1, ]) ## [1] 0.9950048 # true sum(diag(conf.m)) / sum(conf.m) ## [1] 0.9950588  Quadratic Discriminant Analysis (QDA) model.qda \u0026lt;- qda(default ~ balance, data = Default[inTrain, ]) model.qda ## Call: ## qda(default ~ balance, data = Default[inTrain, ]) ## ## Prior probabilities of groups: ## No Yes ## 0.96623529 0.03376471 ## ## Group means: ## balance ## No 801.1297 ## Yes 1757.2025 # predict p.qda \u0026lt;- predict(model.qda, df, type = \u0026#39;response\u0026#39;) predict \u0026lt;- factor(ifelse(p.qda$posterior[, \u0026#39;Yes\u0026#39;] \u0026gt; 0.5, 2, 1), levels = c(1, 2), labels = c(\u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39;)) # confusion matrix conf.m \u0026lt;- table(actual, predict) conf.m ## predict ## actual No Yes ## No 8390 18 ## Yes 0 92 # sensitivity conf.m[2, 2] / sum(conf.m[2, ]) ## [1] 1 # specificity conf.m[1, 1] / sum(conf.m[1, ]) ## [1] 0.9978592 # true sum(diag(conf.m)) / sum(conf.m) ## [1] 0.9978824  ROC-curve for LDA # считаем 1-SPC и TPR для всех вариантов границы отсечения x \u0026lt;- NULL # для (1 - SPC) y \u0026lt;- NULL # для TPR # confusion matrix tbl \u0026lt;- as.data.frame(matrix(rep(0, 4), 2, 2)) rownames(tbl) \u0026lt;- c(\u0026#39;fact.No\u0026#39;, \u0026#39;fact.Yes\u0026#39;) colnames(tbl) \u0026lt;- c(\u0026#39;predict.No\u0026#39;, \u0026#39;predict.Yes\u0026#39;) # probability vector p.vector \u0026lt;- seq(0, 1, length = 501) # цикл по вероятностям отсечения for (p in p.vector){ # prediction prediction \u0026lt;- factor(ifelse(p.lda$posterior[, \u0026#39;Yes\u0026#39;] \u0026gt; p, 2, 1), levels = c(1, 2), labels = c(\u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39;)) # data frame to compare data with prediction df.compare \u0026lt;- data.frame(actual = actual, prediction = prediction) # fill confusion matrix tbl[1, 1] \u0026lt;- nrow(df.compare[df.compare$Факт == \u0026#39;No\u0026#39; \u0026amp; df.compare$Прогноз == \u0026#39;No\u0026#39;, ]) tbl[2, 2] \u0026lt;- nrow(df.compare[df.compare$Факт == \u0026#39;Yes\u0026#39; \u0026amp; df.compare$Прогноз == \u0026#39;Yes\u0026#39;, ]) tbl[1, 2] \u0026lt;- nrow(df.compare[df.compare$Факт == \u0026#39;No\u0026#39; \u0026amp; df.compare$Прогноз == \u0026#39;Yes\u0026#39;, ]) tbl[2, 1] \u0026lt;- nrow(df.compare[df.compare$Факт == \u0026#39;Yes\u0026#39; \u0026amp; df.compare$Прогноз == \u0026#39;No\u0026#39;, ]) # calculate metrix TPR \u0026lt;- tbl[2, 2] / sum(tbl[2, 2] + tbl[2, 1]) y \u0026lt;- c(y, TPR) SPC \u0026lt;- tbl[1, 1] / sum(tbl[1, 1] + tbl[1, 2]) x \u0026lt;- c(x, 1 - SPC) } # ROC-curve par(mar = c(5, 5, 1, 1)) # curve plot(x, y, type = \u0026#39;l\u0026#39;, col = \u0026#39;blue\u0026#39;, lwd = 3, xlab = \u0026#39;(1 - SPC)\u0026#39;, ylab = \u0026#39;TPR\u0026#39;, xlim = c(0, 1), ylim = c(0, 1)) # line of random classifier abline(a = 0, b = 1, lty = 3, lwd = 2) # oint for probability 0.5 points(x[p.vector == 0.5], y[p.vector == 0.5], pch = 16) text(x[p.vector == 0.5], y[p.vector == 0.5], \u0026#39;p = 0.5\u0026#39;, pos = 4) # point for probability 0.2 points(x[p.vector == 0.2], y[p.vector == 0.2], pch = 16) text(x[p.vector == 0.2], y[p.vector == 0.2], \u0026#39;p = 0.2\u0026#39;, pos = 4) predict \u0026lt;- factor(ifelse(p.lda$posterior[, \u0026#39;Yes\u0026#39;] \u0026gt; 0.2, 2, 1), levels = c(1, 2), labels = c(\u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39;)) conf.m \u0026lt;- table(actual, predict) conf.m ## predict ## actual No Yes ## No 8124 284 ## Yes 0 92 # sensitivity conf.m[2, 2] / sum(conf.m[2, ]) ## [1] 1 # specificity conf.m[1, 1] / sum(conf.m[1, ]) ## [1] 0.9662226 # true sum(diag(conf.m)) / sum(conf.m) ## [1] 0.9665882  Tasks  ","date":1565049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565049600,"objectID":"7057618f944b1519cff58b313db0d0e1","permalink":"/post/statistics/logistic_regression/logistic_regression/","publishdate":"2019-08-06T00:00:00Z","relpermalink":"/post/statistics/logistic_regression/logistic_regression/","section":"post","summary":"Logistic regression","tags":["R","Statistics","Logistic regression","Regression"],"title":"Logistic regression","type":"post"},{"authors":null,"categories":["R","Statistics"],"content":"  t-test t-test and normal distribution One-sample t-test Two samples t-test  Summary of R functions for t-tests What is that?  Non-parametric tests Mann-Whitney U Rank Sum Test Wilcoxon test  Tests for categorical variables Chi-squared tests  Multiple testing The Bonferroni correction  Sources   Null hypothesis (H0):\n1. H0: m = μ\n2. H0: m \\(\\leq\\) μ\n3. H0: m \\(\\geq\\) μ\nAlternative hypotheses (Ha): 1. Ha:m ≠ μ (different)\n2. Ha:m \u0026gt; μ (greater)\n3. Ha:m \u0026lt; μ (less)\nNote: Hypothesis 1. are called two-tailed tests and hypotheses 2. \u0026amp; 3. are called one-tailed tests.\nThe p-value is the probability that the observed data could happen, under the condition that the null hypothesis is true.\nNote: p-value is not the probability that the null hypothesis is true.\nNote: Absence of evidence ⧧ evidence of absence.\nCutoffs for hypothesis testing *p \u0026lt; 0.05, **p \u0026lt; 0.01, ***p \u0026lt; 0.001. If p value is less than significance level alpha (0.05), the hull hypothesies is rejected.\n   not rejected (‘negative’) rejected (‘positive’)    H0 true True negative (specificity) False Positive (Type I error)  H0 false False Negative (Type II error) True positive (sensitivity)    Type II errors are usually more dangerous.\nt-test t-test and normal distribution t-distribution assumes that the observations are independent and that they follow a normal distribution. If the data are dependent, then p-values will likely be totally wrong (e.g., for positive correlation, too optimistic). Type II errors?\nIt is good to test if observations are normally distributed. Otherwise we assume that data is normally distributed.\nIndependence of observations is usually not testable, but can be reasonably assumed if the data collection process was random without replacement.\nFIXME: I do not understand this. Deviation data from normalyty will lead to type-I errors. I data is deviated from normal distribution, use Wilcoxon test or permutation tests.\n One-sample t-test One-sample t-test is used to compare the mean of one sample to a known standard (or theoretical/hypothetical) mean (μ).\nt-statistics: \\(t = \\frac{m - \\mu}{s/\\sqrt{n}}\\), where\nm is the sample mean\nn is the sample size\ns is the sample standard deviation with n−1 degrees of freedom\nμ is the theoretical value\nQ: And what should I do with this t-statistics?\nQ: What is the difference between t-test and ANOVA?\nQ: What is the smallest sample size which can be tested by t-test?\nQ: Show diagrams explaining why p-value of one-sided is smaller than two-sided tests.\nR example:\nWe want to test if N is different from given mean μ=0:\nN = c(-0.01, 0.65, -0.17, 1.77, 0.76, -0.16, 0.88, 1.09, 0.96, 0.25) t.test(N, mu = 0, alternative = \u0026quot;less\u0026quot;) ## ## One Sample t-test ## ## data: N ## t = 3.0483, df = 9, p-value = 0.9931 ## alternative hypothesis: true mean is less than 0 ## 95 percent confidence interval: ## -Inf 0.964019 ## sample estimates: ## mean of x ## 0.602 t.test(N, mu = 0, alternative = \u0026quot;two.sided\u0026quot;) ## ## One Sample t-test ## ## data: N ## t = 3.0483, df = 9, p-value = 0.01383 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.1552496 1.0487504 ## sample estimates: ## mean of x ## 0.602 t.test(N, mu = 0, alternative = \u0026quot;greater\u0026quot;) ## ## One Sample t-test ## ## data: N ## t = 3.0483, df = 9, p-value = 0.006916 ## alternative hypothesis: true mean is greater than 0 ## 95 percent confidence interval: ## 0.239981 Inf ## sample estimates: ## mean of x ## 0.602 FIXME: why it accepts all alternatives at the same time (less and greater?)\n Two samples t-test Do two different samples have the same mean?\nH0:\n1. H0: m1 - m2 = 0\n2. H0: m1 - m2 \\(\\leq\\) 0\n3. H0: m1 - m2 \\(\\geq\\) 0\nHa:\n1. Ha: m1 - m2 ≠ 0 (different)\n2. Ha: m1 - m2 \u0026gt; 0 (greater)\n3. Ha: m1 - m2 \u0026lt; 0 (less)\nThe paired sample t-test has four main assumptions:\nThe dependent variable must be continuous (interval/ratio).\n The observations are independent of one another.\n The dependent variable should be approximately normally distributed.\n The dependent variable should not contain any outliers.  Continuous data can take on any value within a range (income, height, weight, etc.). The opposite of continuous data is discrete data, which can only take on a few values (Low, Medium, High, etc.). Occasionally, discrete data can be used to approximate a continuous scale, such as with Likert-type scales.\nt-statistics: \\(t=\\frac{y - x}{SE}\\), where y and x are the samples means. SE is the standard error for the difference. If H0 is correct, test statistic follows a t-distribution with n+m-2 degrees of freedom (n, m the number of observations in each sample).\nTo apply t-test samples must be tested if they have equal variance:\nequal variance (homoscedastic). Type 3 means two samples, unequal variance (heteroscedastic).\n### t-test a = c(175, 168, 168, 190, 156, 181, 182, 175, 174, 179) b = c(185, 169, 173, 173, 188, 186, 175, 174, 179, 180) # test homogeneity of variances using Fisher’s F-test var.test(a,b) ## ## F test to compare two variances ## ## data: a and b ## F = 2.1028, num df = 9, denom df = 9, p-value = 0.2834 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.5223017 8.4657950 ## sample estimates: ## ratio of variances ## 2.102784 # variance is homogene (can use var.equal=T in t.test) # t-test t.test(a,b, var.equal=TRUE, # variance is homogene (tested by var.test(a,b)) paired=FALSE) # samples are independent ## ## Two Sample t-test ## ## data: a and b ## t = -0.94737, df = 18, p-value = 0.356 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -10.93994 4.13994 ## sample estimates: ## mean of x mean of y ## 174.8 178.2   Summary of R functions for t-tests One-sample t-test\nt.test(x, mu = 0, alternative = c(\u0026quot;two.sided\u0026quot;, \u0026quot;less\u0026quot;, \u0026quot;greater\u0026quot;), paired = FALSE, var.equal = FALSE, conf.level = 0.95) What is that? one-way ANOVA or 2-way ANOVA with Bonferroni multiple comparison or Dunnett’s post-test\n  Non-parametric tests Mann-Whitney U Rank Sum Test The dependent variable is ordinal or continuous.\n The data consist of a randomly selected sample of independent observations from two independent groups.\n The dependent variables for the two independent groups share a similar shape.   Wilcoxon test The Wilcoxon is a non-parametric test which works on normal and non-normal data. However, we usually prefer not to use it if we can assume that the data is normally distributed. The non-parametric test comes with less statistical power, this is a price that one has to pay for more flexible assumptions.\n  Tests for categorical variables Categorical variable can take fixed number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property.\nChi-squared tests The chi-squared test is most suited to large datasets. As a general rule, the chi-squared test is appropriate if at least 80% of the cells have an expected frequency of 5 or greater. In addition, none of the cells should have an expected frequency less than 1. If the expected values are very small, categories may be combined (if it makes sense to do so) to create fewer larger categories. Alternatively, Fisher’s exact test can be used.\ndata = rbind(c(83,35), c(92,43)) data ## [,1] [,2] ## [1,] 83 35 ## [2,] 92 43 chisq.test(data, correct=F) ## ## Pearson\u0026#39;s Chi-squared test ## ## data: data ## X-squared = 0.14172, df = 1, p-value = 0.7066 chisq.test(testor,correct=F) ## Fisher’s Exact test R Example:\n  Group TumourShrinkage-No TumourShrinkage-Yes Total    1 Treatment 8 3 11  2 Placebo 9 4 13  3 Total 17 7 24    The null hypothesis is that there is no association between treatment and tumour shrinkage.\nThe alternative hypothesis is that there is some association between treatment group and tumour shrinkage.\ndata = rbind(c(8,3), c(9,4)) data ## [,1] [,2] ## [1,] 8 3 ## [2,] 9 4 fisher.test(data) ## ## Fisher\u0026#39;s Exact Test for Count Data ## ## data: data ## p-value = 1 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.1456912 10.6433317 ## sample estimates: ## odds ratio ## 1.176844 The output Fisher’s exact test tells us that the probability of observing such an extreme combination of frequencies is high, our p-value is 1.000 which is clearly greater than 0.05. In this case, there is no evidence of an association between treatment group and tumour shrinkage.\n  Multiple testing When performing a large number of tests, the type I error is inflated: for α=0.05 and performing n tests, the probability of no false positive result is: 0.095 x 0.95 x … (n-times) \u0026lt;\u0026lt;\u0026lt; 0.095\nThe larger the number of tests performed, the higher the probability of a false rejection!\nMany data analysis approaches in genomics rely on itemby-item (i.e. multiple) testing:\nMicroarray or RNA-Seq expression profiles of “normal” vs “perturbed” samples: gene-by-gene\nChIP-chip: locus-by-locus\nRNAi and chemical compound screens\nGenome-wide association studies: marker-by-marker\nQTL analysis: marker-by-marker and trait-by-trait\nFalse positive rate (FPR) - the proportion of false positives among all resulst.\nFalse discovery rate (FDR) - the proportion of false positives among all significant results.\nExample: 20,000 genes, 100 hits, 10 of them wrong.\nFPR: 0.05%\nFDR: 10%\nThe Bonferroni correction The Bonferroni correction sets the significance cut-off at α/n.\n  Sources One-Sample T-test in R\n ","date":1564876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564876800,"objectID":"1c7501179b55e90be1a51bf7fc314b7c","permalink":"/post/statistics/hypothesis_testing/hypothesis_testing/","publishdate":"2019-08-04T00:00:00Z","relpermalink":"/post/statistics/hypothesis_testing/hypothesis_testing/","section":"post","summary":"Hypothesis testing.","tags":["R","Statistics","Hypothesis testing"],"title":"Hypothesis testing","type":"post"},{"authors":null,"categories":["Statistics"],"content":"  Confusion Matrix Gain and Lift Chart Kolmogorov Smirnov Chart AUC – ROC Gini Coefficient Concordant – Discordant Ratio Root Mean Squared Error Bibliography   check this link\nConfusion Matrix    True Y True N    Predicted Y True Positive (TP) False Positive (FP)  Predicted N False Negative (FN) True Negatives    Common performance metrcs: False Positive Rate = \\(\\frac{FP}{N}\\)\nTrue Positive Rate (sensitivity) = \\(\\frac{TP}{P}\\)\nPrecision = \\(\\frac{TP}{TP+FP}\\)\nAccuracy = \\(\\frac{TP+TN}{P+N}\\)\nSpecificity = \\(\\frac{TN}{FP+TN}\\)\nPrecision (PPV) = \\(\\frac{TP}{TP+FP} = 1 - FDR\\)\nFalse Discovery Rate (FDR) = \\(\\frac{FP}{FP+TP} = 1 - PPV\\)\nSee more on wiki.\n Gain and Lift Chart  Kolmogorov Smirnov Chart  AUC – ROC ROC graphs are two-dimensional graphs in which True Positive rate is plotted on the Y axis and False Positive rate is plotted on the X axis.\nAn ROC graph depicts relative tradeoffs between benefits (true positives) and costs (false positives) (fawcett_introduction_2006).\n Gini Coefficient  Concordant – Discordant Ratio  Root Mean Squared Error  Bibliography  ","date":1564876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564876800,"objectID":"55bef5752ee8fe3b2d7cd7e81e1f56cb","permalink":"/post/statistics/model_evaluation_metrics/model_evaluation_metrics/","publishdate":"2019-08-04T00:00:00Z","relpermalink":"/post/statistics/model_evaluation_metrics/model_evaluation_metrics/","section":"post","summary":"Model evaluation metrics.","tags":["R","Statistics","Machine Learning"],"title":"Model evaluation metrics","type":"post"},{"authors":null,"categories":["Epigenetics"],"content":"  Polycomb group PcG proteins PRC1 PRC2 Pho-complex Mechanisms of inactivation of gene expression TrxG complex components PcG/TrxG response elements Nuclear architecture Open questions Glossary Cited literature   Polycomb group PcG proteins Polycomb group (PcG) proteins generally maintain gene repression, whereas Trithorax group (TrxG) proteins maintain the active gene expression.\nPcG and TrxG proteins were initially isolated in Drosophila as factors involved in maintaining the expression patterns of HOX genes, which encode transcription factors that are important determinants of patterning during embryonic development.\nIn mammals, PRCs are targeted to a subset of CpG islands (CGIs) at the promoters of developmental genes and prevent unscheduled cellular differentiation.\nIn drosophila PcG proteins form Polycomb repressive complex 1 (PRC1), Polycomb repressive complex 2 (PRC2) and Pleiohomeotic (Pho) repressive complex (PhoRC). PRC2 and PRC1 are recruited to chromatin by PhoRC which directly binds polycomb response elements (PREs).\n PRC1 PRC1 ubiquitinylates histone H2AK119 (H2AK118 in Drosophila) and alter chromatin structure whereas PRC2 trimethylates histone H3K27.\nPRC1 is composed of the core components Polycomb (Pc), Polyhomeotic (Ph), Posterior sex combs (Psc) and Sex combs extra.\nPc can bind the H3K27me3 that facilitate anchoring the complex to chromatin.\nPsc and Sce form a heterodimer, which enhances the E3 ubiquitin ligase activity of the complex. This activity is provided by the Sce subunit of\nPRC1, which monoubiquitylates H2AK118 (K119 in mammals). This ubiquitylation event is thought to restrict RNA polymerase II (Pol II) elongation, but was also shown to recruit PRC2 members.\nPh subunit is able to bind itself that promotes spreading of PRC1 complex.\n PRC2 PRC2 core complex is composed of Enhancer of zeste E(z), Suppressor of zeste 12 Su(z)12, Extra sex combs (Esc) and p55 (Nurf55 or Caf1).\nE(z) methylate H3K27.\nEsc bind H3K27me3 and facilitates multimerization of complex.\nPRC1-mediated events are also thought to compact chromatin to limit the access of activating factors and the Psc subunit in particular has been linked to this function.\np55 is present in a number of chromatin remodeling complexes and interacts with Su(z)12, H3 and H4. The loss of loss of p55 appears to have little consequence on PRC2 activity.\n Pho-complex The first complex, referred to as Pho repressive complex (PhoRC), is composed of Pho and Sfmbt. A second Pho-containing complex has also been described (Pho-INO80) that, in addition to Pho, contains the INO80 nucleosome remodeling complex (Klymenko, 2006). Pho binds DNA in a sequence-specific manner and help to recruit PcG complexes to their response elements (Grossniklaus and Paro, 2014).\n Mechanisms of inactivation of gene expression After the initial recruitment of PRC2 and PRC1 by Pho protein, Enhancer of zeste E(z), a member of PRC2, methylate H3 histone (H3K27me3) at both PREs and along the gene body. This modification is then recognized by Pc, a member of PRC1, which, in turn, ubiquitylates H2A119 via Sex combs extra (Sce), another PRC1 member, and stabilizes PRC2. The accumulation of PRC1 and PRC2 within gene bodies results in the compaction of local nucleosomes and the further silencing of the inactive genes.\n TrxG complex components TrxG include the COMPASS, COMPASS-like, TAC1 and ASH1 complexes, and SET domain HMTs.\nCommon subunits for COMPASS and COMPASS-like complexes include Ash2, Dpy30 (Dpy-30L1), Hcf1 (Hcf), Rbbp5 and Wds.\n PcG/TrxG response elements In Drosophila, the genomic nucleation sites of PcG- and TrxG-mediated epigenetic memory have been referred to as PcG/TrxG response elements (PRE/TREs).\n Nuclear architecture Nuclear architecture by PRC1 complex is disscussed in the recent review (Illingworth 2019)\n Open questions Although some PRE/TRE-like elements have been identified, it is unclear if all of the properties of the Drosophila PRE/TRE (e.g. epigenetic memory) are maintained in the mammalian system.\n How PcG and TrxG proteins are recruited to these elements?\nWe currently lack a clear understanding of the hierarchical recruitment of PcG and TrxG proteins to PRE/TREs, and elucidating these recruitment mechanisms is thus an area of active research.\n What determines the active (TRE) versus repressed (PRE) state?\n What factors d which PRC1 targets will physically interact?\n What is the impact of stoichiometry of PRC1 subunits during development?\n  This page summarizes several recent reviews (Geisler and Paro 2015) to aggregate information in this actively studied area of research.\n Glossary PRC1 - Polycomb Repressive Complex 1\nPRC2 - Polycomb Repressive Complex 2\nPho - Pleiohomeotic\nPhoRC - Pleiohomeotic Repressive Complex\nPREs - Polycomb Response Elements\nPsc - Posterior Sex Combs\nE(z) - Enhancer of zeste\nSu(z)12 - Suppressor of zeste 12\nEsc - Extra sex combs\n Cited literature Geisler, Sarah J., and Renato Paro. 2015. “Trithorax and Polycomb Group-Dependent Regulation: A Tale of Opposing Activities.” Development 142 (17): 2876–87. doi:10.1242/dev.120030.\n Illingworth, Robert S. 2019. “Chromatin Folding and Nuclear Architecture: PRC1 Function in 3d.” Current Opinion in Genetics \u0026amp; Development 55 (July): 82–90. doi:10.1016/j.gde.2019.06.006.\n   ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"31008be030b2907bb4e1c1d8c0525ca9","permalink":"/post/biology/pcg_trx/pcg_trx/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/post/biology/pcg_trx/pcg_trx/","section":"post","summary":"Polycomb group PcG proteins PRC1 PRC2 Pho-complex Mechanisms of inactivation of gene expression TrxG complex components PcG/TrxG response elements Nuclear architecture Open questions Glossary Cited literature   Polycomb group PcG proteins Polycomb group (PcG) proteins generally maintain gene repression, whereas Trithorax group (TrxG) proteins maintain the active gene expression.\nPcG and TrxG proteins were initially isolated in Drosophila as factors involved in maintaining the expression patterns of HOX genes, which encode transcription factors that are important determinants of patterning during embryonic development.","tags":["Epigenetics","PcG","Trx"],"title":"Polycomb and Trithorax group proteins","type":"post"},{"authors":null,"categories":["Statistics","R"],"content":" Here, we will consider a simple example of Markov process with implementation in R.\nThe following example is taken from Bodo Winter website.\nA Markov process is characterized by (1) a finite set of states and (2) fixed transition probabilities between the states.\nLet’s consider an example. Assume you have a classroom, with students who could be either in the state alert or in the state bored. And then, at any given time point, there’s a certain probability of an alert student becoming bored (say 0.2), and there’s a probability of a bored student becoming alert (say 0.25).\nLet’s say there are 20 alert and 80 bored students in a particular class. This is your initial condition at time point \\(t\\). Given the transition probabilities above, what’s the number of alert and bored students at the next point in time, \\(t+1\\)?\nMultiply 20 by 0.2 (=4) and these will be the alert students that turn bored.\nAnd then multiply 80 by 0.25 (=20) and these will be the bored students that turn alert.\nSo, at \\(t+1\\), there’s going to be 20-4+20 alert students. And there’s going to be 80+4-20 bored students. Before, 80% of the students were bored and now, only 64% of the students are bored. Conversely, 36% are alert.\nA handy way of representing this Markov process is by defining a transition probability matrix:\n   A B    A\\(_{t+1}\\) 0.8 0.25  B\\(_{t+1}\\) 0.2 0.75    What this matrix says is: A proportion of 0.8 of the people who are in state A (alert) will also be at state A at time point \\(t+1\\). And, a proportion of 0.25 of the people who are in state B (bored) will switch to alert at t+1. This is what the first row says. The next row is simply one minus the probabilities of the first row, because probabilities (or proportions) have to add up to 1. Now think about multiplying this matrix with the initial proportions of alert and bored students that we had above. 0.8 are bored and 0.2 are alert. In linear algebra this would look the following way:\n \\[ \\begin{bmatrix} 0.8 \u0026amp; 0.25 \\\\ 0.2 \u0026amp; 0.75 \\end{bmatrix}\\times\\begin{bmatrix} 0.2 \\\\ 0.8 \\end{bmatrix} = \\begin{bmatrix} 0.8\\times0.2 + 0.25\\times0.8 \\\\ 0.2\\times0.2 + 0.75\\times0.8 \\end{bmatrix} = \\begin{bmatrix} 0.36 \\\\ 0.64 \\end{bmatrix} \\]\n The results of these calculations are exactly the proportions that we saw above: 36% alert student and 64% bored students.\nNow, you might ask yourself: What happens if this process continues? What happens at \\(t+2\\), \\(t+3\\) etc.? Will it be the case that at one point there are no bored students any more? Let’s simulate this in R and find out! Let’s call this tpm for transition probability matrix:\ntpm = matrix(c(0.8,0.25, 0.2,0.75), nrow=2, byrow=TRUE) colnames(tpm) = c(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;) rownames(tpm) = c(\u0026#39;At+1\u0026#39;, \u0026#39;Bt+1\u0026#39;) tpm ## A B ## At+1 0.8 0.25 ## Bt+1 0.2 0.75 Again this matrix shows that 0.8 students who were in state A at time point t will still be in state A at \\(t+1\\). And 0.25 students who were in state B at time point t will be in state A at \\(t+1\\). The second row has a similar interpretation for alert and bored students becoming bored at \\(t+1\\). Remember that Markov processes assume fixed transition probabilities. This means that in the simulation that we’ll be doing, we leave the transition probability matrix unchanged. However, we will define a vector of the actual proportions – and these are allowed to change. In time, we expect more and more students to become alert, because the transition probability from B to A (which, to remind you, was 0.25) is higher than from A to B (which was 0.2).\nLet’s start our simulation by setting the initial condition as 0.1 students are alert and 0.9 students are bored and define a matrix called sm (short for student matrix):\nsm = as.matrix(c(0.1, 0.9)) rownames(sm)= c(\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;) sm ## [,1] ## A 0.1 ## B 0.9 Now let’s repeat by looping:\nfor(i in 1:10){ sm = tpm %*% sm } Here, we’re looping 10 times and on each iteration, we multiply the matrix tpm with the student matrix sm. We take this result and store it in sm. This means that at the next iteration, our fixed transition probability matrix will be multiplied by a different student matrix, allowing for the proportions to slowly change over time.\nR operator ’%*%’ is used for matrix multiplication\nOutcome of our ten loop iterations:\nsm ## [,1] ## At+1 0.5544017 ## Bt+1 0.4455983 So, after 10 iterations of the Markov process, we now have about 55% alert students and 45% bored ones. What is interesting to me is that even though 80% of the people who are alert at one time point remain alert at the next time point, the process only converged on 55% alert and 45% bored after 10 iterations.\nLet’s reset our initial condition to (0.1 alert and 0.9 bored students) and run a thousand iterations.\nfor(i in 1:1000){ sm = tpm %*% sm } sm ## [,1] ## At+1 0.5555556 ## Bt+1 0.4444444 A 1000 iterations, and we seem to be zoning in onto ~55% and ~44%. This phenomenon is called Markov convergence. You could run even more iterations, and your outcome would get closer and closer to 0.5555 (to infinity). So, the model converges on an equilibrium. However, this is not a fixed equilibrium. It’s not the case that the Markov process comes to a hold or that nobody changes states between alertness and boredness any more. The equilibrium that we’re dealing with here is a statistical equilibrium, where the proportions of alert and bored students remain the same. but there still is constant change (at each time step, 0.2 alert students become bored and 0.25 bored students become alert). Markov models always converge to a statistical equilibrium if the conditions (1) and (2) above are met, and if you can get from any state within your Markov model to any other state (in the case of just two states, that clearly is the case). What’s so cool about this is that it is, at first sight, fairly counterintuitive.\nAt least when I thought about the transition probabilities for the first time, I somehow expected all students to become alert but as we saw, that’s not the case. Moreover, this process is not sensitive to initial conditions. That means that when you start with any proportion of alert or bored students (even extreme ones such as 0.0001 alert students), the process will reach the statistical equilibrium – albeit sometimes a little faster or slower. You can play around with different values for the sm object to explore this property of Markov convergence. Another interesting thing is that the process is impervious to intervention: Say, you introduced something that made more students alert – the Markov model would quickly get back to equilibrium. So Markov processes are essentially ahistorical processes: history doesn’t matter. Even with extreme initial conditions or extreme interventions, the process quickly converges to the equilibrium defined by the transition probabilities. The only way to persistently change the system is to change the transition probabilities. Finally, what I find so cool about Markov processes is their computational simplicity.\nSources Bodo Winter website\n ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"e28695185e4e15861510ad12bc133a6d","permalink":"/post/statistics/markov_process/simple_markov_process/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/post/statistics/markov_process/simple_markov_process/","section":"post","summary":"Here, we will consider a simple example of Markov process with implementation in R.\nThe following example is taken from Bodo Winter website.\nA Markov process is characterized by (1) a finite set of states and (2) fixed transition probabilities between the states.\nLet’s consider an example. Assume you have a classroom, with students who could be either in the state alert or in the state bored. And then, at any given time point, there’s a certain probability of an alert student becoming bored (say 0.","tags":["R","Markov process","Statistics"],"title":"Simple Markov process","type":"post"},{"authors":null,"categories":["R","Statistics"],"content":"  Generate dataset from a given function Split data for train and test Diagram of the given function and generated datasets Build a model using splines Diagram of MSE for train and test data Build optimal model and plot for the model Bibliograpy   In this example we will generate data from a given function and then build a model using splines and estimate quality of the model.\nGenerate dataset from a given function # parameters to generate a dataset n.all \u0026lt;- 100 # number of observations train.percent \u0026lt;- 0.85 # portion of the data for training res.sd \u0026lt;- 1 # standard deviation of noise x.min \u0026lt;- 5 # min limit of the data x.max \u0026lt;- 105 # max limit of the data # generate x set.seed(1) # to get reproducible results by randomizer x \u0026lt;- runif(x.min, x.max, n = n.all) # noise from normal destibution set.seed(1) res \u0026lt;- rnorm(mean = 0, sd = res.sd, n = n.all) # generate y using a given function y.func \u0026lt;- function(x) {4 - 2e-02*x + 5.5e-03*x^2 - 4.9e-05*x^3} # add noise y \u0026lt;- y.func(x) + res  Split data for train and test # split dataset for training and test set.seed(1) # generate vector of chosen x for train data inTrain \u0026lt;- sample(seq_along(x), size = train.percent*n.all) # train data set x.train \u0026lt;- x[inTrain] y.train \u0026lt;- y[inTrain] # test data set x.test \u0026lt;- x[-inTrain] y.test \u0026lt;- y[-inTrain]  Diagram of the given function and generated datasets # lines of generated data for plot x.line \u0026lt;- seq(x.min, x.max, length = n.all) y.line \u0026lt;- y.func(x.line) # PLOT # generate plot by train data par(mar = c(4, 4, 1, 1)) # reduce margins (optional) plot(x.train, y.train, main = \u0026#39;Generated data and original function\u0026#39;, col = grey(0.2), bg = grey(0.2), pch = 21, xlab = \u0026#39;X\u0026#39;, ylab = \u0026#39;Y\u0026#39;, xlim = c(x.min, x.max), ylim = c(min(y), max(y)), cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # add points of test data points(x.test, y.test, col = \u0026#39;red\u0026#39;, bg = \u0026#39;red\u0026#39;, pch = 21) # add the given function lines(x.line, y.line, lwd = 2, lty = 2) # add legend legend(\u0026#39;topleft\u0026#39;, legend = c(\u0026#39;train\u0026#39;, \u0026#39;test\u0026#39;, \u0026#39;f(X)\u0026#39;), pch = c(16, 16, NA), col = c(grey(0.2), \u0026#39;red\u0026#39;, \u0026#39;black\u0026#39;), lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)  Build a model using splines We will compair sevaral models with degree of freedoms (df) from 2 to 40, where 2 correspond to a linear model.\nmax.df \u0026lt;- 40 # max degree of freedom (df) # tbl \u0026lt;- data.frame(df = 2:max.df) # data frame for writing errors tbl$MSE.train \u0026lt;- 0 # column 1: errors of train data tbl$MSE.test \u0026lt;- 0 # сcolumn 2: errors of test data # generate models using for cycle for (i in 2:max.df) { mod \u0026lt;- smooth.spline(x = x.train, y = y.train, df = i) # predicted values for train and test data using built model y.model.train \u0026lt;- predict(mod, data.frame(x = x.train))$y[, 1] y.model.test \u0026lt;- predict(mod, data.frame(x = x.test))$y[, 1] # MSE errors for train and test data MSE \u0026lt;- c(sum((y.train - y.model.train)^2) / length(x.train), sum((y.test - y.model.test)^2) / length(x.test)) # write errors to the previously created data frame tbl[tbl$df == i, c(\u0026#39;MSE.train\u0026#39;, \u0026#39;MSE.test\u0026#39;)] \u0026lt;- MSE } # view first rows of the table head(tbl, 4) ## df MSE.train MSE.test ## 1 2 3.7188566 2.885166 ## 2 3 1.4463925 1.635813 ## 3 4 0.8938817 1.239533 ## 4 5 0.7668250 1.038918  Diagram of MSE for train and test data # plot MSE from our table plot(x = tbl$df, y = tbl$MSE.test, main = \u0026quot;Changes of MSE from degrees of freedom\u0026quot;, type = \u0026#39;l\u0026#39;, col = \u0026#39;red\u0026#39;, lwd = 2, xlab = \u0026#39;spline degree of freedom\u0026#39;, ylab = \u0026#39;MSE\u0026#39;, ylim = c(min(tbl$MSE.train, tbl$MSE.test), max(tbl$MSE.train, tbl$MSE.test)), cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # add points(x = tbl$df, y = tbl$MSE.test, pch = 21, col = \u0026#39;red\u0026#39;, bg = \u0026#39;red\u0026#39;) lines(x = tbl$df, y = tbl$MSE.train, col = grey(0.3), lwd = 2) # minimal MSE abline(h = res.sd, lty = 2, col = grey(0.4), lwd = 2) # add legend legend(\u0026#39;topright\u0026#39;, legend = c(\u0026#39;train\u0026#39;, \u0026#39;test\u0026#39;), pch = c(NA, 16), col = c(grey(0.2), \u0026#39;red\u0026#39;), lty = c(1, 1), lwd = c(2, 2), cex = 1.2) # df of minimal MSE for test data min.MSE.test \u0026lt;- min(tbl$MSE.test) df.min.MSE.test \u0026lt;- tbl[tbl$MSE.test == min.MSE.test, \u0026#39;df\u0026#39;] # optimal df for precise model and maximal simplicity df.my.MSE.test \u0026lt;- 6 my.MSE.test \u0026lt;- tbl[tbl$df == df.my.MSE.test, \u0026#39;MSE.test\u0026#39;] # show the optimal solution abline(v = df.my.MSE.test, lty = 2, lwd = 2) points(x = df.my.MSE.test, y = my.MSE.test, pch = 15, col = \u0026#39;blue\u0026#39;) mtext(df.my.MSE.test, side = 1, line = -1, at = df.my.MSE.test, col = \u0026#39;blue\u0026#39;, cex = 1.2)  Build optimal model and plot for the model mod.MSE.test \u0026lt;- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test) # predict data for 250 x\u0026#39;s to get smoothed curve x.model.plot \u0026lt;- seq(x.min, x.max, length = 250) y.model.plot \u0026lt;- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1] # plot train data par(mar = c(4, 4, 1, 1)) plot(x.train, y.train, main = \u0026quot;Initial data and the best fit model\u0026quot;, col = grey(0.2), bg = grey(0.2), pch = 21, xlab = \u0026#39;X\u0026#39;, ylab = \u0026#39;Y\u0026#39;, xlim = c(x.min, x.max), ylim = c(min(y), max(y)), cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # add test data points(x.test, y.test, col = \u0026#39;red\u0026#39;, bg = \u0026#39;red\u0026#39;, pch = 21) # function lines(x.line, y.line,lwd = 2, lty = 2) # add model lines(x.model.plot, y.model.plot, lwd = 2, col = \u0026#39;blue\u0026#39;) # legend legend(\u0026#39;topleft\u0026#39;, legend = c(\u0026#39;train\u0026#39;, \u0026#39;test\u0026#39;, \u0026#39;f(X)\u0026#39;, \u0026#39;model\u0026#39;), pch = c(16, 16, NA, NA), col = c(grey(0.2), \u0026#39;red\u0026#39;, \u0026#39;black\u0026#39;, \u0026#39;blue\u0026#39;), lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)  Bibliograpy An Introduction to Statistical Learning by Gareth James\n ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"7c2bdd460840ef71fbf1420e7ca61d85","permalink":"/post/statistics/splines/splines/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/post/statistics/splines/splines/","section":"post","summary":"Practical example showing how to generate data set using given function, how to split data, buld spline model on train data and how to use test data to find optimal parameters of the model.","tags":["R","Splines","Regression"],"title":"Spline model","type":"post"}]