[{"authors":["admin"],"categories":null,"content":"Mark Goldberg is a researcher of epigenetic processes in invertebrates at Oxford Brookes University. His research interests include epigenetics, computational biology and single-cell technologies.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Mark Goldberg is a researcher of epigenetic processes in invertebrates at Oxford Brookes University. His research interests include epigenetics, computational biology and single-cell technologies.","tags":null,"title":"Mark Goldberg","type":"authors"},{"authors":null,"categories":["Statistics"],"content":"  Simple linear regression How to find coefficients β0 and β1? Matrix form for multiple regression   Simple linear regression \\(\\hat y = \\beta_0 + \\beta_1 x\\)\n\\(\\hat y\\) - expectd value of \\(y\\) given \\(x\\), the same as \\(E(Y|x)\\)\n\\(\\beta_0\\) - intercept\n\\(\\beta_1\\) - slope\n\\(\\hat \\beta_0 = \\bar y - \\hat\\beta_1 \\bar x\\)\n\\(\\hat\\beta_1 = \\frac{\\displaystyle\\sum_{i=1}^{n} (x_i - \\bar x)(y_i-\\bar y)}{\\displaystyle\\sum_{i=1}^{n} (x_i - \\bar x)^2}\\)\n How to find coefficients β0 and β1? There are several ways to extimate coefficients of linear regression. Here we discuss the least squares approach. Other aproaches include maximul likelihood estimation.\nTo estimate \\(\\beta_1\\) and \\(\\beta_0\\) we should find minimum of sum of squared residuals (\\(SSR\\))\nTo find this minimum we should calculate derivatives of \\(SSR\\) with respect to \\(\\beta_0\\) and \\(\\beta_1\\) and set them to 0:\n\\(\\displaystyle\\min_{\\beta_0,\\beta1} : SSR \\implies \\frac{\\partial SSR}{\\partial \\beta_0} = \\frac{\\partial SSR}{\\partial \\beta_1} = 0\\)\nHere is the solution:\n\\(SSR = \\displaystyle\\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1x_i))^2 = \\displaystyle\\sum_{i=1}^{n} (y_i^2 - 2y_i \\beta_0 - 2y_i \\beta_1 x_i + \\beta_0^2 + 2\\beta_0\\beta_1x_i + \\beta_1^2x_i^2)\\)\n\\(\\frac{\\partial SSR}{\\partial\\beta_0} = \\displaystyle\\sum_{i=1}^{n}(-2y_i + 2\\beta_0 + 2\\beta_1x_i)\\)\n\\(\\displaystyle\\sum_{i=1}^{n} (-y_i + \\hat\\beta_0 + \\hat\\beta_1 x_i) = 0\\)\n\\(\\bar y = \\frac{1}{n} \\displaystyle\\sum_{i=1}^{n} y_i\\) and \\(\\bar x = \\frac{1}{n} \\displaystyle\\sum_{i=1}^{n} x_i \\implies -n\\bar y + n \\hat\\beta_0 + \\hat\\beta_1 n \\hat x = 0\\)\n\\(\\hat \\beta_0 = \\bar y - \\hat\\beta_1 \\bar x\\)\n\\(\\frac{\\partial SSR}{\\partial \\beta_1} = \\displaystyle\\sum_{i=1}^{n} (-2x_i y_i + 2\\beta_0 x_i + 2\\beta_1 x_i^2)\\)\n\\(-\\displaystyle\\sum_{i=1}^{n} x_i y_i + \\hat\\beta_0 \\displaystyle\\sum_{i=1}^{n} x_i + \\hat\\beta_1\\displaystyle\\sum_{i=1}^{n} x_i^2 = 0\\)\n\\(-\\displaystyle\\sum_{i=1}^{n} x_i y_i + (\\bar y - \\hat\\beta_1 \\bar x) \\displaystyle\\sum_{i=1}^{n} x_i + \\hat\\beta_1\\displaystyle\\sum_{i=1}^{n} x_i^2 = 0\\)\n\\(\\hat\\beta_1 = \\frac{\\displaystyle\\sum_{i=1}^{n} x_i(y_i-\\bar y)}{\\displaystyle\\sum_{i=1}^{n} x_i (x_i - \\bar x)} = \\frac{\\displaystyle\\sum_{i=1}^{n} (x_i - \\bar x)(y_i-\\bar y)}{\\displaystyle\\sum_{i=1}^{n} (x_i - \\bar x)^2} = \\frac{Cov(x,y)}{Var(x)} = r_{xy} \\frac{s_y}{s_x}\\)\n\\(\\hat x\\) and \\(\\hat y\\) - estimated \\(x\\) and \\(y\\)\n\\(\\bar x\\) and \\(\\bar y\\) - averages of \\(x_i\\) and \\(y_i\\)\n\\(r_{xy}\\) - sample correlation coefficient between \\(x\\) and \\(y\\)\n\\(s_x\\) and \\(s_y\\) - uncorrected sample standard deviations of \\(x\\) and \\(y\\)\n\\(Var\\) and \\(Cov\\) - sample variance and sample covariance.\n Matrix form for multiple regression We can write \\(\\hat y = \\beta_0 + \\beta_1 x\\) in a matrix form:\n\\(Y = \\begin{bmatrix}y_1\\\\y_2\\\\\\vdots\\\\y_n\\end{bmatrix}\\) \\(b = \\begin{bmatrix}\\beta_1\\\\\\beta_2\\\\\\vdots\\\\\\beta_n\\end{bmatrix}\\) \\(X = \\begin{bmatrix}1 \u0026amp; x_{1,1} \u0026amp; x_{1,2} \\dots \u0026amp; x_{1,k}\\\\1 \u0026amp; x_{2,1} \u0026amp; x_{2,2} \\dots \u0026amp; x_{2,k}\\\\\\vdots \u0026amp; \\vdots \u0026amp; \\vdots \u0026amp; \\vdots\\\\1 \u0026amp; x_{n,1} \u0026amp; x_{n,2} \\dots \u0026amp; x_{n,k}\\end{bmatrix}\\)\n\\(Y = Xb\\)\n\\(X\u0026#39;Y = X\u0026#39;Xb\\)\n\\((X\u0026#39;X)^-1X\u0026#39;Xb = (X\u0026#39;X)^-1X\u0026#39;Y\\)\n\\(b = (X\u0026#39;X)^-1X\u0026#39;Y\\)\n\\((X\u0026#39;X)^-1X\u0026#39;X = I\\) - identity matrix\n ","date":1565049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565049600,"objectID":"49375f3da816adc5a61702458e6553bc","permalink":"/post/statistics/linear_regression_math/linear_regression_math/","publishdate":"2019-08-06T00:00:00Z","relpermalink":"/post/statistics/linear_regression_math/linear_regression_math/","section":"post","summary":"Pure math for linear regression model.","tags":["Math","Statistics","Regression"],"title":"Linear Regression (Math)","type":"post"},{"authors":null,"categories":["Statistics"],"content":"  Background Binomial logistic regression Logistic regression Linear Discriminant Analysis (LDA) Quadratic Discriminant Analysis (QDA) ROC-curve for LDA Tasks   Background Logistic regression builds model for binary dependent variables (0/1, True/False).\nLogistic function: \\[Y = \\frac{1}{1+e^l} = \\frac{e^l}{e^l+1}\\] where \\(l\\) is a linear combination of all observations (log-odds): \\(l = \\beta_0 + \\beta_{1}x_{1} + \\beta_{2}x_{2} + ... + \\beta_{p}x_{p} + \\epsilon\\)\nSee also: Sigmoid functions:\n Binomial logistic regression Probability of passing an exam versus hours of study.\nData from wiki describe if students pass exam depending of how many hours they studied.\nWe build logistic regression model to predict if ‘pass’ depending on learning ‘hours’.\n# put data into dataframe hours=c(0.50,.75,1,1.25,1.5,1.75,1.75,2,2.25,2.5,2.75,3,3.25,3.50,4,4.25,4.5,4.75,5,5.5) pass=c(0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1) df = data.frame(hours, pass) # Logistic Regression model model.logit \u0026lt;- glm(pass ~ hours, data = df, family = \u0026#39;binomial\u0026#39;) summary(model.logit) ## ## Call: ## glm(formula = pass ~ hours, family = \u0026quot;binomial\u0026quot;, data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.70557 -0.57357 -0.04654 0.45470 1.82008 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -4.0777 1.7610 -2.316 0.0206 * ## hours 1.5046 0.6287 2.393 0.0167 * ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 27.726 on 19 degrees of freedom ## Residual deviance: 16.060 on 18 degrees of freedom ## AIC: 20.06 ## ## Number of Fisher Scoring iterations: 5 Coefficients Intercept = -4.0777 and Hours = 1.5046 are entered in the logistic regression equation to estimate the odds (probability) of passing the exam: \\(1/(1+e^{-(-4.0777+1.5046\\cdot hours)})\\) Calculate the probability to pass exam if studied 4 hours:\n1/(1+exp(-(-4.0777+1.5046*4))) ## [1] 0.874429 Let’s find a critical point where probability is 0.5:\ncrit = -coef(model.logit)[1]/coef(model.logit)[2] crit ## (Intercept) ## 2.710083 # predict \u0026#39;pass\u0026#39; for given data df$predic.prob \u0026lt;- predict(model.logit, df, type=\u0026quot;response\u0026quot;) df$predic.pass \u0026lt;- ifelse(df$predic.prob \u0026gt; 0.5, 1, 0) df ## hours pass predic.prob predic.pass ## 1 0.50 0 0.03471034 0 ## 2 0.75 0 0.04977295 0 ## 3 1.00 0 0.07089196 0 ## 4 1.25 0 0.10002862 0 ## 5 1.50 0 0.13934447 0 ## 6 1.75 0 0.19083650 0 ## 7 1.75 1 0.19083650 0 ## 8 2.00 0 0.25570318 0 ## 9 2.25 1 0.33353024 0 ## 10 2.50 0 0.42162653 0 ## 11 2.75 1 0.51501086 1 ## 12 3.00 0 0.60735865 1 ## 13 3.25 1 0.69261733 1 ## 14 3.50 0 0.76648084 1 ## 15 4.00 1 0.87444750 1 ## 16 4.25 1 0.91027764 1 ## 17 4.50 1 0.93662366 1 ## 18 4.75 1 0.95561071 1 ## 19 5.00 1 0.96909707 1 ## 20 5.50 1 0.98519444 1 # plot data plot(df$hours, df$pass, pch=19, col=\u0026#39;black\u0026#39;, main=\u0026#39;Probability of Passing Exam vs Hours Studying\u0026#39;, ylab=\u0026#39;Probability of Passing Exam\u0026#39;, xlab=\u0026#39;Hours Studying\u0026#39;) # data frame to build a logistic function curve \u0026#39;hours~pass\u0026#39; df2 \u0026lt;- data.frame(hours=seq(min(df$hours),max(df$hours),0.1), pass=NA) # predict \u0026#39;pass\u0026#39; from our model df2$pass \u0026lt;- predict(model.logit, df2, type=\u0026quot;response\u0026quot;) # draw logistic function for our data sets lines(df2$pass~df2$hours, lwd=2) # critical point abline(h=0.5, col=\u0026#39;green\u0026#39;) # abline(v=crit, col=\u0026#39;red\u0026#39;) # # draw predicted points (-0.02 to avoid overlapping with actual data) points(df$hours, df$predic.pass-0.03, pch=19, col=\u0026#39;red\u0026#39;) legend(\u0026#39;bottomright\u0026#39;, lty=c(1,1,1,1), col = c(\u0026#39;black\u0026#39;, \u0026#39;red\u0026#39;, \u0026#39;black\u0026#39;, \u0026#39;green\u0026#39;, \u0026#39;red\u0026#39;), legend = c(\u0026#39;actual data\u0026#39;, \u0026#39;predicted\u0026#39;, \u0026#39;Logistic function\u0026#39;, \u0026#39;Decision p\u0026#39;, \u0026#39;Decision bound\u0026#39;), lwd=c(NA,NA,1,1,1), pch=c(19,19,NA,NA,NA), bty = \u0026#39;n\u0026#39;)  Logistic regression Data: generated credit card balance Default{ISLR}. 10000 observations for 4 variables:\ndefault – binary variable: Yes, if credit card holder did not return debt;\nstudent – binary variable: Yes, if credit card holder is a student;\nbalance – average month balance on the bank account;\nincome – income of credit card holder.\nlibrary(\u0026#39;ISLR\u0026#39;) head(Default) ## default student balance income ## 1 No No 729.5265 44361.625 ## 2 No Yes 817.1804 12106.135 ## 3 No No 1073.5492 31767.139 ## 4 No No 529.2506 35704.494 ## 5 No No 785.6559 38463.496 ## 6 No Yes 919.5885 7491.559 set.seed(1) # train subset rate is 0.85 inTrain \u0026lt;- sample(seq_along(Default$default), nrow(Default)*0.85) df \u0026lt;- Default[inTrain, ] # logistic regression model \u0026#39;default ~ f(balance)\u0026#39; model.logit \u0026lt;- glm(default ~ balance, data = df, family = \u0026#39;binomial\u0026#39;) summary(model.logit) ## ## Call: ## glm(formula = default ~ balance, family = \u0026quot;binomial\u0026quot;, data = df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3282 -0.1420 -0.0553 -0.0201 3.7934 ## ## Coefficients: ## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -1.088e+01 4.022e-01 -27.07 \u0026lt;2e-16 *** ## balance 5.657e-03 2.448e-04 23.11 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2509.1 on 8499 degrees of freedom ## Residual deviance: 1336.5 on 8498 degrees of freedom ## AIC: 1340.5 ## ## Number of Fisher Scoring iterations: 8 # Predict \u0026#39;default\u0026#39; by \u0026#39;balance\u0026#39; p.logit \u0026lt;- predict(model.logit, df, type = \u0026#39;response\u0026#39;) predicted \u0026lt;- factor(ifelse(p.logit \u0026gt; 0.5, 2, 1), levels = c(1, 2), labels = c(\u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39;)) # true values for \u0026#39;default\u0026#39; in train data actual \u0026lt;- df$default # confusion matrix conf.m \u0026lt;- table(actual, predicted) conf.m ## predicted ## actual No Yes ## No 8172 41 ## Yes 194 93 # sensitivity conf.m[2, 2] / sum(conf.m[2, ]) ## [1] 0.3240418 # specificity conf.m[1, 1] / sum(conf.m[1, ]) ## [1] 0.9950079 # probability sum(diag(conf.m)) / sum(conf.m) ## [1] 0.9723529  Linear Discriminant Analysis (LDA) #library(\u0026#39;GGally\u0026#39;) library(\u0026#39;MASS\u0026#39;) model.lda \u0026lt;- lda(default ~ balance, data = Default[inTrain, ]) model.lda ## Call: ## lda(default ~ balance, data = Default[inTrain, ]) ## ## Prior probabilities of groups: ## No Yes ## 0.96623529 0.03376471 ## ## Group means: ## balance ## No 801.1297 ## Yes 1757.2025 ## ## Coefficients of linear discriminants: ## LD1 ## balance 0.00220817 # Predict p.lda \u0026lt;- predict(model.lda, df, type = \u0026#39;response\u0026#39;) actual \u0026lt;- factor(ifelse(p.lda$posterior[, \u0026#39;Yes\u0026#39;] \u0026gt; 0.5, 2, 1), levels = c(1, 2), labels = c(\u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39;)) # confusion matrix conf.m \u0026lt;- table(actual, predicted) conf.m ## predicted ## actual No Yes ## No 8366 42 ## Yes 0 92 # sensitivity conf.m[2, 2] / sum(conf.m[2, ]) ## [1] 1 # specificity conf.m[1, 1] / sum(conf.m[1, ]) ## [1] 0.9950048 # true sum(diag(conf.m)) / sum(conf.m) ## [1] 0.9950588  Quadratic Discriminant Analysis (QDA) model.qda \u0026lt;- qda(default ~ balance, data = Default[inTrain, ]) model.qda ## Call: ## qda(default ~ balance, data = Default[inTrain, ]) ## ## Prior probabilities of groups: ## No Yes ## 0.96623529 0.03376471 ## ## Group means: ## balance ## No 801.1297 ## Yes 1757.2025 # predict p.qda \u0026lt;- predict(model.qda, df, type = \u0026#39;response\u0026#39;) predict \u0026lt;- factor(ifelse(p.qda$posterior[, \u0026#39;Yes\u0026#39;] \u0026gt; 0.5, 2, 1), levels = c(1, 2), labels = c(\u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39;)) # confusion matrix conf.m \u0026lt;- table(actual, predict) conf.m ## predict ## actual No Yes ## No 8390 18 ## Yes 0 92 # sensitivity conf.m[2, 2] / sum(conf.m[2, ]) ## [1] 1 # specificity conf.m[1, 1] / sum(conf.m[1, ]) ## [1] 0.9978592 # true sum(diag(conf.m)) / sum(conf.m) ## [1] 0.9978824  ROC-curve for LDA # считаем 1-SPC и TPR для всех вариантов границы отсечения x \u0026lt;- NULL # для (1 - SPC) y \u0026lt;- NULL # для TPR # confusion matrix tbl \u0026lt;- as.data.frame(matrix(rep(0, 4), 2, 2)) rownames(tbl) \u0026lt;- c(\u0026#39;fact.No\u0026#39;, \u0026#39;fact.Yes\u0026#39;) colnames(tbl) \u0026lt;- c(\u0026#39;predict.No\u0026#39;, \u0026#39;predict.Yes\u0026#39;) # probability vector p.vector \u0026lt;- seq(0, 1, length = 501) # цикл по вероятностям отсечения for (p in p.vector){ # prediction prediction \u0026lt;- factor(ifelse(p.lda$posterior[, \u0026#39;Yes\u0026#39;] \u0026gt; p, 2, 1), levels = c(1, 2), labels = c(\u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39;)) # data frame to compare data with prediction df.compare \u0026lt;- data.frame(actual = actual, prediction = prediction) # fill confusion matrix tbl[1, 1] \u0026lt;- nrow(df.compare[df.compare$Факт == \u0026#39;No\u0026#39; \u0026amp; df.compare$Прогноз == \u0026#39;No\u0026#39;, ]) tbl[2, 2] \u0026lt;- nrow(df.compare[df.compare$Факт == \u0026#39;Yes\u0026#39; \u0026amp; df.compare$Прогноз == \u0026#39;Yes\u0026#39;, ]) tbl[1, 2] \u0026lt;- nrow(df.compare[df.compare$Факт == \u0026#39;No\u0026#39; \u0026amp; df.compare$Прогноз == \u0026#39;Yes\u0026#39;, ]) tbl[2, 1] \u0026lt;- nrow(df.compare[df.compare$Факт == \u0026#39;Yes\u0026#39; \u0026amp; df.compare$Прогноз == \u0026#39;No\u0026#39;, ]) # calculate metrix TPR \u0026lt;- tbl[2, 2] / sum(tbl[2, 2] + tbl[2, 1]) y \u0026lt;- c(y, TPR) SPC \u0026lt;- tbl[1, 1] / sum(tbl[1, 1] + tbl[1, 2]) x \u0026lt;- c(x, 1 - SPC) } # ROC-curve par(mar = c(5, 5, 1, 1)) # curve plot(x, y, type = \u0026#39;l\u0026#39;, col = \u0026#39;blue\u0026#39;, lwd = 3, xlab = \u0026#39;(1 - SPC)\u0026#39;, ylab = \u0026#39;TPR\u0026#39;, xlim = c(0, 1), ylim = c(0, 1)) # line of random classifier abline(a = 0, b = 1, lty = 3, lwd = 2) # oint for probability 0.5 points(x[p.vector == 0.5], y[p.vector == 0.5], pch = 16) text(x[p.vector == 0.5], y[p.vector == 0.5], \u0026#39;p = 0.5\u0026#39;, pos = 4) # point for probability 0.2 points(x[p.vector == 0.2], y[p.vector == 0.2], pch = 16) text(x[p.vector == 0.2], y[p.vector == 0.2], \u0026#39;p = 0.2\u0026#39;, pos = 4) predict \u0026lt;- factor(ifelse(p.lda$posterior[, \u0026#39;Yes\u0026#39;] \u0026gt; 0.2, 2, 1), levels = c(1, 2), labels = c(\u0026#39;No\u0026#39;, \u0026#39;Yes\u0026#39;)) conf.m \u0026lt;- table(actual, predict) conf.m ## predict ## actual No Yes ## No 8124 284 ## Yes 0 92 # sensitivity conf.m[2, 2] / sum(conf.m[2, ]) ## [1] 1 # specificity conf.m[1, 1] / sum(conf.m[1, ]) ## [1] 0.9662226 # true sum(diag(conf.m)) / sum(conf.m) ## [1] 0.9665882  Tasks  ","date":1565049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565049600,"objectID":"7057618f944b1519cff58b313db0d0e1","permalink":"/post/statistics/logistic_regression/logistic_regression/","publishdate":"2019-08-06T00:00:00Z","relpermalink":"/post/statistics/logistic_regression/logistic_regression/","section":"post","summary":"Logistic regression","tags":["R","Statistics","Logistic regression","Regression"],"title":"Logistic regression","type":"post"},{"authors":null,"categories":["R","Statistics"],"content":"  t-test t-test and normal distribution One-sample t-test Two samples t-test  Summary of R functions for t-tests What is that?  Non-parametric tests Mann-Whitney U Rank Sum Test Wilcoxon test  Tests for categorical variables Chi-squared tests  Multiple testing The Bonferroni correction  Sources   Null hypothesis (H0):\n1. H0: m = μ\n2. H0: m \\(\\leq\\) μ\n3. H0: m \\(\\geq\\) μ\nAlternative hypotheses (Ha): 1. Ha:m ≠ μ (different)\n2. Ha:m \u0026gt; μ (greater)\n3. Ha:m \u0026lt; μ (less)\nNote: Hypothesis 1. are called two-tailed tests and hypotheses 2. \u0026amp; 3. are called one-tailed tests.\nThe p-value is the probability that the observed data could happen, under the condition that the null hypothesis is true.\nNote: p-value is not the probability that the null hypothesis is true.\nNote: Absence of evidence ⧧ evidence of absence.\nCutoffs for hypothesis testing *p \u0026lt; 0.05, **p \u0026lt; 0.01, ***p \u0026lt; 0.001. If p value is less than significance level alpha (0.05), the hull hypothesies is rejected.\n   not rejected (‘negative’) rejected (‘positive’)    H0 true True negative (specificity) False Positive (Type I error)  H0 false False Negative (Type II error) True positive (sensitivity)    Type II errors are usually more dangerous.\nt-test t-test and normal distribution t-distribution assumes that the observations are independent and that they follow a normal distribution. If the data are dependent, then p-values will likely be totally wrong (e.g., for positive correlation, too optimistic). Type II errors?\nIt is good to test if observations are normally distributed. Otherwise we assume that data is normally distributed.\nIndependence of observations is usually not testable, but can be reasonably assumed if the data collection process was random without replacement.\nFIXME: I do not understand this. Deviation data from normalyty will lead to type-I errors. I data is deviated from normal distribution, use Wilcoxon test or permutation tests.\n One-sample t-test One-sample t-test is used to compare the mean of one sample to a known standard (or theoretical/hypothetical) mean (μ).\nt-statistics: \\(t = \\frac{m - \\mu}{s/\\sqrt{n}}\\), where\nm is the sample mean\nn is the sample size\ns is the sample standard deviation with n−1 degrees of freedom\nμ is the theoretical value\nQ: And what should I do with this t-statistics?\nQ: What is the difference between t-test and ANOVA?\nQ: What is the smallest sample size which can be tested by t-test?\nQ: Show diagrams explaining why p-value of one-sided is smaller than two-sided tests.\nR example:\nWe want to test if N is different from given mean μ=0:\nN = c(-0.01, 0.65, -0.17, 1.77, 0.76, -0.16, 0.88, 1.09, 0.96, 0.25) t.test(N, mu = 0, alternative = \u0026quot;less\u0026quot;) ## ## One Sample t-test ## ## data: N ## t = 3.0483, df = 9, p-value = 0.9931 ## alternative hypothesis: true mean is less than 0 ## 95 percent confidence interval: ## -Inf 0.964019 ## sample estimates: ## mean of x ## 0.602 t.test(N, mu = 0, alternative = \u0026quot;two.sided\u0026quot;) ## ## One Sample t-test ## ## data: N ## t = 3.0483, df = 9, p-value = 0.01383 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.1552496 1.0487504 ## sample estimates: ## mean of x ## 0.602 t.test(N, mu = 0, alternative = \u0026quot;greater\u0026quot;) ## ## One Sample t-test ## ## data: N ## t = 3.0483, df = 9, p-value = 0.006916 ## alternative hypothesis: true mean is greater than 0 ## 95 percent confidence interval: ## 0.239981 Inf ## sample estimates: ## mean of x ## 0.602 FIXME: why it accepts all alternatives at the same time (less and greater?)\n Two samples t-test Do two different samples have the same mean?\nH0:\n1. H0: m1 - m2 = 0\n2. H0: m1 - m2 \\(\\leq\\) 0\n3. H0: m1 - m2 \\(\\geq\\) 0\nHa:\n1. Ha: m1 - m2 ≠ 0 (different)\n2. Ha: m1 - m2 \u0026gt; 0 (greater)\n3. Ha: m1 - m2 \u0026lt; 0 (less)\nThe paired sample t-test has four main assumptions:\nThe dependent variable must be continuous (interval/ratio).\n The observations are independent of one another.\n The dependent variable should be approximately normally distributed.\n The dependent variable should not contain any outliers.  Continuous data can take on any value within a range (income, height, weight, etc.). The opposite of continuous data is discrete data, which can only take on a few values (Low, Medium, High, etc.). Occasionally, discrete data can be used to approximate a continuous scale, such as with Likert-type scales.\nt-statistics: \\(t=\\frac{y - x}{SE}\\), where y and x are the samples means. SE is the standard error for the difference. If H0 is correct, test statistic follows a t-distribution with n+m-2 degrees of freedom (n, m the number of observations in each sample).\nTo apply t-test samples must be tested if they have equal variance:\nequal variance (homoscedastic). Type 3 means two samples, unequal variance (heteroscedastic).\n### t-test a = c(175, 168, 168, 190, 156, 181, 182, 175, 174, 179) b = c(185, 169, 173, 173, 188, 186, 175, 174, 179, 180) # test homogeneity of variances using Fisher’s F-test var.test(a,b) ## ## F test to compare two variances ## ## data: a and b ## F = 2.1028, num df = 9, denom df = 9, p-value = 0.2834 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.5223017 8.4657950 ## sample estimates: ## ratio of variances ## 2.102784 # variance is homogene (can use var.equal=T in t.test) # t-test t.test(a,b, var.equal=TRUE, # variance is homogene (tested by var.test(a,b)) paired=FALSE) # samples are independent ## ## Two Sample t-test ## ## data: a and b ## t = -0.94737, df = 18, p-value = 0.356 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -10.93994 4.13994 ## sample estimates: ## mean of x mean of y ## 174.8 178.2   Summary of R functions for t-tests One-sample t-test\nt.test(x, mu = 0, alternative = c(\u0026quot;two.sided\u0026quot;, \u0026quot;less\u0026quot;, \u0026quot;greater\u0026quot;), paired = FALSE, var.equal = FALSE, conf.level = 0.95) What is that? one-way ANOVA or 2-way ANOVA with Bonferroni multiple comparison or Dunnett’s post-test\n  Non-parametric tests Mann-Whitney U Rank Sum Test The dependent variable is ordinal or continuous.\n The data consist of a randomly selected sample of independent observations from two independent groups.\n The dependent variables for the two independent groups share a similar shape.   Wilcoxon test The Wilcoxon is a non-parametric test which works on normal and non-normal data. However, we usually prefer not to use it if we can assume that the data is normally distributed. The non-parametric test comes with less statistical power, this is a price that one has to pay for more flexible assumptions.\n  Tests for categorical variables Categorical variable can take fixed number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property.\nChi-squared tests The chi-squared test is most suited to large datasets. As a general rule, the chi-squared test is appropriate if at least 80% of the cells have an expected frequency of 5 or greater. In addition, none of the cells should have an expected frequency less than 1. If the expected values are very small, categories may be combined (if it makes sense to do so) to create fewer larger categories. Alternatively, Fisher’s exact test can be used.\ndata = rbind(c(83,35), c(92,43)) data ## [,1] [,2] ## [1,] 83 35 ## [2,] 92 43 chisq.test(data, correct=F) ## ## Pearson\u0026#39;s Chi-squared test ## ## data: data ## X-squared = 0.14172, df = 1, p-value = 0.7066 chisq.test(testor,correct=F) ## Fisher’s Exact test R Example:\n  Group TumourShrinkage-No TumourShrinkage-Yes Total    1 Treatment 8 3 11  2 Placebo 9 4 13  3 Total 17 7 24    The null hypothesis is that there is no association between treatment and tumour shrinkage.\nThe alternative hypothesis is that there is some association between treatment group and tumour shrinkage.\ndata = rbind(c(8,3), c(9,4)) data ## [,1] [,2] ## [1,] 8 3 ## [2,] 9 4 fisher.test(data) ## ## Fisher\u0026#39;s Exact Test for Count Data ## ## data: data ## p-value = 1 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.1456912 10.6433317 ## sample estimates: ## odds ratio ## 1.176844 The output Fisher’s exact test tells us that the probability of observing such an extreme combination of frequencies is high, our p-value is 1.000 which is clearly greater than 0.05. In this case, there is no evidence of an association between treatment group and tumour shrinkage.\n  Multiple testing When performing a large number of tests, the type I error is inflated: for α=0.05 and performing n tests, the probability of no false positive result is: 0.095 x 0.95 x … (n-times) \u0026lt;\u0026lt;\u0026lt; 0.095\nThe larger the number of tests performed, the higher the probability of a false rejection!\nMany data analysis approaches in genomics rely on itemby-item (i.e. multiple) testing:\nMicroarray or RNA-Seq expression profiles of “normal” vs “perturbed” samples: gene-by-gene\nChIP-chip: locus-by-locus\nRNAi and chemical compound screens\nGenome-wide association studies: marker-by-marker\nQTL analysis: marker-by-marker and trait-by-trait\nFalse positive rate (FPR) - the proportion of false positives among all resulst.\nFalse discovery rate (FDR) - the proportion of false positives among all significant results.\nExample: 20,000 genes, 100 hits, 10 of them wrong.\nFPR: 0.05%\nFDR: 10%\nThe Bonferroni correction The Bonferroni correction sets the significance cut-off at α/n.\n  Sources One-Sample T-test in R\n ","date":1564876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564876800,"objectID":"1c7501179b55e90be1a51bf7fc314b7c","permalink":"/post/statistics/hypothesis_testing/hypothesis_testing/","publishdate":"2019-08-04T00:00:00Z","relpermalink":"/post/statistics/hypothesis_testing/hypothesis_testing/","section":"post","summary":"Hypothesis testing.","tags":["R","Statistics","Hypothesis testing"],"title":"Hypothesis testing","type":"post"},{"authors":null,"categories":["Statistics"],"content":"  Confusion Matrix Gain and Lift Chart Kolmogorov Smirnov Chart AUC – ROC Gini Coefficient Concordant – Discordant Ratio Root Mean Squared Error Bibliography   check this link\nConfusion Matrix    True Y True N    Predicted Y True Positive (TP) False Positive (FP)  Predicted N False Negative (FN) True Negatives    Common performance metrcs: False Positive Rate = \\(\\frac{FP}{N}\\)\nTrue Positive Rate (sensitivity) = \\(\\frac{TP}{P}\\)\nPrecision = \\(\\frac{TP}{TP+FP}\\)\nAccuracy = \\(\\frac{TP+TN}{P+N}\\)\nSpecificity = \\(\\frac{TN}{FP+TN}\\)\nPrecision (PPV) = \\(\\frac{TP}{TP+FP} = 1 - FDR\\)\nFalse Discovery Rate (FDR) = \\(\\frac{FP}{FP+TP} = 1 - PPV\\)\nSee more on wiki.\n Gain and Lift Chart  Kolmogorov Smirnov Chart  AUC – ROC ROC graphs are two-dimensional graphs in which True Positive rate is plotted on the Y axis and False Positive rate is plotted on the X axis.\nAn ROC graph depicts relative tradeoffs between benefits (true positives) and costs (false positives) (fawcett_introduction_2006).\n Gini Coefficient  Concordant – Discordant Ratio  Root Mean Squared Error  Bibliography  ","date":1564876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564876800,"objectID":"55bef5752ee8fe3b2d7cd7e81e1f56cb","permalink":"/post/statistics/model_evaluation_metrics/model_evaluation_metrics/","publishdate":"2019-08-04T00:00:00Z","relpermalink":"/post/statistics/model_evaluation_metrics/model_evaluation_metrics/","section":"post","summary":"Model evaluation metrics.","tags":["R","Statistics","Machine Learning"],"title":"Model evaluation metrics","type":"post"},{"authors":null,"categories":["R","Statistics"],"content":" Here you can find several simple approaches to split data into train and test subset to fit and to test parameters of your model. We want to split our data:\ntrain percent - 0.7\ntest percent - 0.3\nLet us create a data frame filled with generated data.\ndf = data.frame(x=seq(.1,1,.1), y=seq(1,10), z=letters[seq(1,10)]) df ## x y z ## 1 0.1 1 a ## 2 0.2 2 b ## 3 0.3 3 c ## 4 0.4 4 d ## 5 0.5 5 e ## 6 0.6 6 f ## 7 0.7 7 g ## 8 0.8 8 h ## 9 0.9 9 i ## 10 1.0 10 j First approach is to create a vector filled with selected IDs of rows and then apply this vector to subset data.  set.seed(1) # set state of random number generator tv1 = sample(nrow(df), nrow(df)*0.7) tv1 ## [1] 3 4 5 7 2 8 9 # Select train and test data for column y using our vector train = df$y[tv1] test = df$y[-tv1] # the same as test = df$y[!tv1] train ## [1] 3 4 5 7 2 8 9 test ## [1] 1 6 10 Another approch is to create a vecotr filled with logical true/false for each row of dataset and apply this vector to subset data.  set.seed(1) # set state of random number generator tv2 = sample(c(TRUE, FALSE), nrow(df), replace = T, prob = c(0.7,0.3)) tv2 ## [1] TRUE TRUE TRUE FALSE TRUE FALSE FALSE TRUE TRUE TRUE # Select train and test data for column y using our vector train = df$y[tv2] test = df$y[!tv2] train ## [1] 1 2 3 5 8 9 10 test ## [1] 4 6 7 Using caTools.  library(caTools) set.seed(1) sample = sample.split(df, SplitRatio = .7) train = subset(df$y, sample == TRUE) test = subset(df$y, sample == FALSE) train ## [1] 1 2 4 5 7 8 10 test ## [1] 3 6 9 Using dplyr  library(dplyr) df$id \u0026lt;- 1:nrow(df) train \u0026lt;- df %\u0026gt;% dplyr::sample_frac(.7) test \u0026lt;- dplyr::anti_join(df, train, by = \u0026#39;id\u0026#39;) train$y ## [1] 10 2 8 7 4 6 1 test$y ## [1] 3 5 9 ","date":1564790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564790400,"objectID":"6ba07d8f45246b3fd119ee889a1e4e61","permalink":"/post/statistics/split_data_ways/split_data_ways/","publishdate":"2019-08-03T00:00:00Z","relpermalink":"/post/statistics/split_data_ways/split_data_ways/","section":"post","summary":"Here you will learn approaches to split your data into subsets - train and test for your modeling.","tags":["R","Statistics"],"title":"How to split data into train and test subsets?","type":"post"},{"authors":null,"categories":["Epigenetics"],"content":"  Polycomb group PcG proteins PRC1 PRC2 Pho-complex Mechanisms of inactivation of gene expression TrxG complex components PcG/TrxG response elements Nuclear architecture Open questions Glossary Cited literature   Polycomb group PcG proteins Polycomb group (PcG) proteins generally maintain gene repression, whereas Trithorax group (TrxG) proteins maintain the active gene expression.\nPcG and TrxG proteins were initially isolated in Drosophila as factors involved in maintaining the expression patterns of HOX genes, which encode transcription factors that are important determinants of patterning during embryonic development.\nIn mammals, PRCs are targeted to a subset of CpG islands (CGIs) at the promoters of developmental genes and prevent unscheduled cellular differentiation.\nIn drosophila PcG proteins form Polycomb repressive complex 1 (PRC1), Polycomb repressive complex 2 (PRC2) and Pleiohomeotic (Pho) repressive complex (PhoRC). PRC2 and PRC1 are recruited to chromatin by PhoRC which directly binds polycomb response elements (PREs).\n PRC1 PRC1 ubiquitinylates histone H2AK119 (H2AK118 in Drosophila) and alter chromatin structure whereas PRC2 trimethylates histone H3K27.\nPRC1 is composed of the core components Polycomb (Pc), Polyhomeotic (Ph), Posterior sex combs (Psc) and Sex combs extra.\nPc can bind the H3K27me3 that facilitate anchoring the complex to chromatin.\nPsc and Sce form a heterodimer, which enhances the E3 ubiquitin ligase activity of the complex. This activity is provided by the Sce subunit of\nPRC1, which monoubiquitylates H2AK118 (K119 in mammals). This ubiquitylation event is thought to restrict RNA polymerase II (Pol II) elongation, but was also shown to recruit PRC2 members.\nPh subunit is able to bind itself that promotes spreading of PRC1 complex.\n PRC2 PRC2 core complex is composed of Enhancer of zeste E(z), Suppressor of zeste 12 Su(z)12, Extra sex combs (Esc) and p55 (Nurf55 or Caf1).\nE(z) methylate H3K27.\nEsc bind H3K27me3 and facilitates multimerization of complex.\nPRC1-mediated events are also thought to compact chromatin to limit the access of activating factors and the Psc subunit in particular has been linked to this function.\np55 is present in a number of chromatin remodeling complexes and interacts with Su(z)12, H3 and H4. The loss of loss of p55 appears to have little consequence on PRC2 activity.\n Pho-complex The first complex, referred to as Pho repressive complex (PhoRC), is composed of Pho and Sfmbt. A second Pho-containing complex has also been described (Pho-INO80) that, in addition to Pho, contains the INO80 nucleosome remodeling complex (Klymenko, 2006). Pho binds DNA in a sequence-specific manner and help to recruit PcG complexes to their response elements (Grossniklaus and Paro, 2014).\n Mechanisms of inactivation of gene expression After the initial recruitment of PRC2 and PRC1 by Pho protein, Enhancer of zeste E(z), a member of PRC2, methylate H3 histone (H3K27me3) at both PREs and along the gene body. This modification is then recognized by Pc, a member of PRC1, which, in turn, ubiquitylates H2A119 via Sex combs extra (Sce), another PRC1 member, and stabilizes PRC2. The accumulation of PRC1 and PRC2 within gene bodies results in the compaction of local nucleosomes and the further silencing of the inactive genes.\n TrxG complex components TrxG include the COMPASS, COMPASS-like, TAC1 and ASH1 complexes, and SET domain HMTs.\nCommon subunits for COMPASS and COMPASS-like complexes include Ash2, Dpy30 (Dpy-30L1), Hcf1 (Hcf), Rbbp5 and Wds.\n PcG/TrxG response elements In Drosophila, the genomic nucleation sites of PcG- and TrxG-mediated epigenetic memory have been referred to as PcG/TrxG response elements (PRE/TREs).\n Nuclear architecture Nuclear architecture by PRC1 complex is disscussed in the recent review (Illingworth 2019)\n Open questions Although some PRE/TRE-like elements have been identified, it is unclear if all of the properties of the Drosophila PRE/TRE (e.g. epigenetic memory) are maintained in the mammalian system.\n How PcG and TrxG proteins are recruited to these elements?\nWe currently lack a clear understanding of the hierarchical recruitment of PcG and TrxG proteins to PRE/TREs, and elucidating these recruitment mechanisms is thus an area of active research.\n What determines the active (TRE) versus repressed (PRE) state?\n What factors d which PRC1 targets will physically interact?\n What is the impact of stoichiometry of PRC1 subunits during development?\n  This page summarizes several recent reviews (Geisler and Paro 2015) to aggregate information in this actively studied area of research.\n Glossary PRC1 - Polycomb Repressive Complex 1\nPRC2 - Polycomb Repressive Complex 2\nPho - Pleiohomeotic\nPhoRC - Pleiohomeotic Repressive Complex\nPREs - Polycomb Response Elements\nPsc - Posterior Sex Combs\nE(z) - Enhancer of zeste\nSu(z)12 - Suppressor of zeste 12\nEsc - Extra sex combs\n Cited literature Geisler, Sarah J., and Renato Paro. 2015. “Trithorax and Polycomb Group-Dependent Regulation: A Tale of Opposing Activities.” Development 142 (17): 2876–87. doi:10.1242/dev.120030.\n Illingworth, Robert S. 2019. “Chromatin Folding and Nuclear Architecture: PRC1 Function in 3d.” Current Opinion in Genetics \u0026amp; Development 55 (July): 82–90. doi:10.1016/j.gde.2019.06.006.\n   ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"31008be030b2907bb4e1c1d8c0525ca9","permalink":"/post/biology/pcg_trx/pcg_trx/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/post/biology/pcg_trx/pcg_trx/","section":"post","summary":"Polycomb group PcG proteins PRC1 PRC2 Pho-complex Mechanisms of inactivation of gene expression TrxG complex components PcG/TrxG response elements Nuclear architecture Open questions Glossary Cited literature   Polycomb group PcG proteins Polycomb group (PcG) proteins generally maintain gene repression, whereas Trithorax group (TrxG) proteins maintain the active gene expression.\nPcG and TrxG proteins were initially isolated in Drosophila as factors involved in maintaining the expression patterns of HOX genes, which encode transcription factors that are important determinants of patterning during embryonic development.","tags":["Epigenetics","PcG","Trx"],"title":"Polycomb and Trithorax group proteins","type":"post"},{"authors":null,"categories":["R"],"content":" Here you can find some simple interesting concepts for coding in R.\nHow to select all numeric columns in a data frame? df = data.frame(x = c(1:3), y = c(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;), z = c(0.1, 0.2, 0.3)) df ## x y z ## 1 1 A 0.1 ## 2 2 B 0.2 ## 3 3 C 0.3 # Select all numeric columns df[sapply(df,is.numeric)] ## x z ## 1 1 0.1 ## 2 2 0.2 ## 3 3 0.3  ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"5621f0671b7114034878c191aff0f0b5","permalink":"/post/programming/r_tips_tricks/r_tips_tricks/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/post/programming/r_tips_tricks/r_tips_tricks/","section":"post","summary":"Here you can find some simple interesting concepts for coding in R.\nHow to select all numeric columns in a data frame? df = data.frame(x = c(1:3), y = c(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;), z = c(0.1, 0.2, 0.3)) df ## x y z ## 1 1 A 0.1 ## 2 2 B 0.2 ## 3 3 C 0.3 # Select all numeric columns df[sapply(df,is.numeric)] ## x z ## 1 1 0.1 ## 2 2 0.","tags":["R"],"title":"R tips and coding tricks","type":"post"},{"authors":null,"categories":["Statistics","R"],"content":" Here, we will consider a simple example of Markov process with implementation in R.\nThe following example is taken from Bodo Winter website.\nA Markov process is characterized by (1) a finite set of states and (2) fixed transition probabilities between the states.\nLet’s consider an example. Assume you have a classroom, with students who could be either in the state alert or in the state bored. And then, at any given time point, there’s a certain probability of an alert student becoming bored (say 0.2), and there’s a probability of a bored student becoming alert (say 0.25).\nLet’s say there are 20 alert and 80 bored students in a particular class. This is your initial condition at time point \\(t\\). Given the transition probabilities above, what’s the number of alert and bored students at the next point in time, \\(t+1\\)?\nMultiply 20 by 0.2 (=4) and these will be the alert students that turn bored.\nAnd then multiply 80 by 0.25 (=20) and these will be the bored students that turn alert.\nSo, at \\(t+1\\), there’s going to be 20-4+20 alert students. And there’s going to be 80+4-20 bored students. Before, 80% of the students were bored and now, only 64% of the students are bored. Conversely, 36% are alert.\nA handy way of representing this Markov process is by defining a transition probability matrix:\n   A B    A\\(_{t+1}\\) 0.8 0.25  B\\(_{t+1}\\) 0.2 0.75    What this matrix says is: A proportion of 0.8 of the people who are in state A (alert) will also be at state A at time point \\(t+1\\). And, a proportion of 0.25 of the people who are in state B (bored) will switch to alert at t+1. This is what the first row says. The next row is simply one minus the probabilities of the first row, because probabilities (or proportions) have to add up to 1. Now think about multiplying this matrix with the initial proportions of alert and bored students that we had above. 0.8 are bored and 0.2 are alert. In linear algebra this would look the following way:\n \\[ \\begin{bmatrix} 0.8 \u0026amp; 0.25 \\\\ 0.2 \u0026amp; 0.75 \\end{bmatrix}\\times\\begin{bmatrix} 0.2 \\\\ 0.8 \\end{bmatrix} = \\begin{bmatrix} 0.8\\times0.2 + 0.25\\times0.8 \\\\ 0.2\\times0.2 + 0.75\\times0.8 \\end{bmatrix} = \\begin{bmatrix} 0.36 \\\\ 0.64 \\end{bmatrix} \\]\n The results of these calculations are exactly the proportions that we saw above: 36% alert student and 64% bored students.\nNow, you might ask yourself: What happens if this process continues? What happens at \\(t+2\\), \\(t+3\\) etc.? Will it be the case that at one point there are no bored students any more? Let’s simulate this in R and find out! Let’s call this tpm for transition probability matrix:\ntpm = matrix(c(0.8,0.25, 0.2,0.75), nrow=2, byrow=TRUE) colnames(tpm) = c(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;) rownames(tpm) = c(\u0026#39;At+1\u0026#39;, \u0026#39;Bt+1\u0026#39;) tpm ## A B ## At+1 0.8 0.25 ## Bt+1 0.2 0.75 Again this matrix shows that 0.8 students who were in state A at time point t will still be in state A at \\(t+1\\). And 0.25 students who were in state B at time point t will be in state A at \\(t+1\\). The second row has a similar interpretation for alert and bored students becoming bored at \\(t+1\\). Remember that Markov processes assume fixed transition probabilities. This means that in the simulation that we’ll be doing, we leave the transition probability matrix unchanged. However, we will define a vector of the actual proportions – and these are allowed to change. In time, we expect more and more students to become alert, because the transition probability from B to A (which, to remind you, was 0.25) is higher than from A to B (which was 0.2).\nLet’s start our simulation by setting the initial condition as 0.1 students are alert and 0.9 students are bored and define a matrix called sm (short for student matrix):\nsm = as.matrix(c(0.1, 0.9)) rownames(sm)= c(\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;) sm ## [,1] ## A 0.1 ## B 0.9 Now let’s repeat by looping:\nfor(i in 1:10){ sm = tpm %*% sm } Here, we’re looping 10 times and on each iteration, we multiply the matrix tpm with the student matrix sm. We take this result and store it in sm. This means that at the next iteration, our fixed transition probability matrix will be multiplied by a different student matrix, allowing for the proportions to slowly change over time.\nR operator ’%*%’ is used for matrix multiplication\nOutcome of our ten loop iterations:\nsm ## [,1] ## At+1 0.5544017 ## Bt+1 0.4455983 So, after 10 iterations of the Markov process, we now have about 55% alert students and 45% bored ones. What is interesting to me is that even though 80% of the people who are alert at one time point remain alert at the next time point, the process only converged on 55% alert and 45% bored after 10 iterations.\nLet’s reset our initial condition to (0.1 alert and 0.9 bored students) and run a thousand iterations.\nfor(i in 1:1000){ sm = tpm %*% sm } sm ## [,1] ## At+1 0.5555556 ## Bt+1 0.4444444 A 1000 iterations, and we seem to be zoning in onto ~55% and ~44%. This phenomenon is called Markov convergence. You could run even more iterations, and your outcome would get closer and closer to 0.5555 (to infinity). So, the model converges on an equilibrium. However, this is not a fixed equilibrium. It’s not the case that the Markov process comes to a hold or that nobody changes states between alertness and boredness any more. The equilibrium that we’re dealing with here is a statistical equilibrium, where the proportions of alert and bored students remain the same. but there still is constant change (at each time step, 0.2 alert students become bored and 0.25 bored students become alert). Markov models always converge to a statistical equilibrium if the conditions (1) and (2) above are met, and if you can get from any state within your Markov model to any other state (in the case of just two states, that clearly is the case). What’s so cool about this is that it is, at first sight, fairly counterintuitive.\nAt least when I thought about the transition probabilities for the first time, I somehow expected all students to become alert but as we saw, that’s not the case. Moreover, this process is not sensitive to initial conditions. That means that when you start with any proportion of alert or bored students (even extreme ones such as 0.0001 alert students), the process will reach the statistical equilibrium – albeit sometimes a little faster or slower. You can play around with different values for the sm object to explore this property of Markov convergence. Another interesting thing is that the process is impervious to intervention: Say, you introduced something that made more students alert – the Markov model would quickly get back to equilibrium. So Markov processes are essentially ahistorical processes: history doesn’t matter. Even with extreme initial conditions or extreme interventions, the process quickly converges to the equilibrium defined by the transition probabilities. The only way to persistently change the system is to change the transition probabilities. Finally, what I find so cool about Markov processes is their computational simplicity.\nSources Bodo Winter website\n ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"e28695185e4e15861510ad12bc133a6d","permalink":"/post/statistics/markov_process/simple_markov_process/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/post/statistics/markov_process/simple_markov_process/","section":"post","summary":"Here, we will consider a simple example of Markov process with implementation in R.\nThe following example is taken from Bodo Winter website.\nA Markov process is characterized by (1) a finite set of states and (2) fixed transition probabilities between the states.\nLet’s consider an example. Assume you have a classroom, with students who could be either in the state alert or in the state bored. And then, at any given time point, there’s a certain probability of an alert student becoming bored (say 0.","tags":["R","Markov process","Statistics"],"title":"Simple Markov process","type":"post"},{"authors":null,"categories":["R","Statistics"],"content":"  Generate dataset from a given function Split data for train and test Diagram of the given function and generated datasets Build a model using splines Diagram of MSE for train and test data Build optimal model and plot for the model Bibliograpy   In this example we will generate data from a given function and then build a model using splines and estimate quality of the model.\nGenerate dataset from a given function # parameters to generate a dataset n.all \u0026lt;- 100 # number of observations train.percent \u0026lt;- 0.85 # portion of the data for training res.sd \u0026lt;- 1 # standard deviation of noise x.min \u0026lt;- 5 # min limit of the data x.max \u0026lt;- 105 # max limit of the data # generate x set.seed(1) # to get reproducible results by randomizer x \u0026lt;- runif(x.min, x.max, n = n.all) # noise from normal destibution set.seed(1) res \u0026lt;- rnorm(mean = 0, sd = res.sd, n = n.all) # generate y using a given function y.func \u0026lt;- function(x) {4 - 2e-02*x + 5.5e-03*x^2 - 4.9e-05*x^3} # add noise y \u0026lt;- y.func(x) + res  Split data for train and test # split dataset for training and test set.seed(1) # generate vector of chosen x for train data inTrain \u0026lt;- sample(seq_along(x), size = train.percent*n.all) # train data set x.train \u0026lt;- x[inTrain] y.train \u0026lt;- y[inTrain] # test data set x.test \u0026lt;- x[-inTrain] y.test \u0026lt;- y[-inTrain]  Diagram of the given function and generated datasets # lines of generated data for plot x.line \u0026lt;- seq(x.min, x.max, length = n.all) y.line \u0026lt;- y.func(x.line) # PLOT # generate plot by train data par(mar = c(4, 4, 1, 1)) # reduce margins (optional) plot(x.train, y.train, main = \u0026#39;Generated data and original function\u0026#39;, col = grey(0.2), bg = grey(0.2), pch = 21, xlab = \u0026#39;X\u0026#39;, ylab = \u0026#39;Y\u0026#39;, xlim = c(x.min, x.max), ylim = c(min(y), max(y)), cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # add points of test data points(x.test, y.test, col = \u0026#39;red\u0026#39;, bg = \u0026#39;red\u0026#39;, pch = 21) # add the given function lines(x.line, y.line, lwd = 2, lty = 2) # add legend legend(\u0026#39;topleft\u0026#39;, legend = c(\u0026#39;train\u0026#39;, \u0026#39;test\u0026#39;, \u0026#39;f(X)\u0026#39;), pch = c(16, 16, NA), col = c(grey(0.2), \u0026#39;red\u0026#39;, \u0026#39;black\u0026#39;), lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)  Build a model using splines We will compair sevaral models with degree of freedoms (df) from 2 to 40, where 2 correspond to a linear model.\nmax.df \u0026lt;- 40 # max degree of freedom (df) # tbl \u0026lt;- data.frame(df = 2:max.df) # data frame for writing errors tbl$MSE.train \u0026lt;- 0 # column 1: errors of train data tbl$MSE.test \u0026lt;- 0 # сcolumn 2: errors of test data # generate models using for cycle for (i in 2:max.df) { mod \u0026lt;- smooth.spline(x = x.train, y = y.train, df = i) # predicted values for train and test data using built model y.model.train \u0026lt;- predict(mod, data.frame(x = x.train))$y[, 1] y.model.test \u0026lt;- predict(mod, data.frame(x = x.test))$y[, 1] # MSE errors for train and test data MSE \u0026lt;- c(sum((y.train - y.model.train)^2) / length(x.train), sum((y.test - y.model.test)^2) / length(x.test)) # write errors to the previously created data frame tbl[tbl$df == i, c(\u0026#39;MSE.train\u0026#39;, \u0026#39;MSE.test\u0026#39;)] \u0026lt;- MSE } # view first rows of the table head(tbl, 4) ## df MSE.train MSE.test ## 1 2 3.7188566 2.885166 ## 2 3 1.4463925 1.635813 ## 3 4 0.8938817 1.239533 ## 4 5 0.7668250 1.038918  Diagram of MSE for train and test data # plot MSE from our table plot(x = tbl$df, y = tbl$MSE.test, main = \u0026quot;Changes of MSE from degrees of freedom\u0026quot;, type = \u0026#39;l\u0026#39;, col = \u0026#39;red\u0026#39;, lwd = 2, xlab = \u0026#39;spline degree of freedom\u0026#39;, ylab = \u0026#39;MSE\u0026#39;, ylim = c(min(tbl$MSE.train, tbl$MSE.test), max(tbl$MSE.train, tbl$MSE.test)), cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # add points(x = tbl$df, y = tbl$MSE.test, pch = 21, col = \u0026#39;red\u0026#39;, bg = \u0026#39;red\u0026#39;) lines(x = tbl$df, y = tbl$MSE.train, col = grey(0.3), lwd = 2) # minimal MSE abline(h = res.sd, lty = 2, col = grey(0.4), lwd = 2) # add legend legend(\u0026#39;topright\u0026#39;, legend = c(\u0026#39;train\u0026#39;, \u0026#39;test\u0026#39;), pch = c(NA, 16), col = c(grey(0.2), \u0026#39;red\u0026#39;), lty = c(1, 1), lwd = c(2, 2), cex = 1.2) # df of minimal MSE for test data min.MSE.test \u0026lt;- min(tbl$MSE.test) df.min.MSE.test \u0026lt;- tbl[tbl$MSE.test == min.MSE.test, \u0026#39;df\u0026#39;] # optimal df for precise model and maximal simplicity df.my.MSE.test \u0026lt;- 6 my.MSE.test \u0026lt;- tbl[tbl$df == df.my.MSE.test, \u0026#39;MSE.test\u0026#39;] # show the optimal solution abline(v = df.my.MSE.test, lty = 2, lwd = 2) points(x = df.my.MSE.test, y = my.MSE.test, pch = 15, col = \u0026#39;blue\u0026#39;) mtext(df.my.MSE.test, side = 1, line = -1, at = df.my.MSE.test, col = \u0026#39;blue\u0026#39;, cex = 1.2)  Build optimal model and plot for the model mod.MSE.test \u0026lt;- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test) # predict data for 250 x\u0026#39;s to get smoothed curve x.model.plot \u0026lt;- seq(x.min, x.max, length = 250) y.model.plot \u0026lt;- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1] # plot train data par(mar = c(4, 4, 1, 1)) plot(x.train, y.train, main = \u0026quot;Initial data and the best fit model\u0026quot;, col = grey(0.2), bg = grey(0.2), pch = 21, xlab = \u0026#39;X\u0026#39;, ylab = \u0026#39;Y\u0026#39;, xlim = c(x.min, x.max), ylim = c(min(y), max(y)), cex = 1.2, cex.lab = 1.2, cex.axis = 1.2) # add test data points(x.test, y.test, col = \u0026#39;red\u0026#39;, bg = \u0026#39;red\u0026#39;, pch = 21) # function lines(x.line, y.line,lwd = 2, lty = 2) # add model lines(x.model.plot, y.model.plot, lwd = 2, col = \u0026#39;blue\u0026#39;) # legend legend(\u0026#39;topleft\u0026#39;, legend = c(\u0026#39;train\u0026#39;, \u0026#39;test\u0026#39;, \u0026#39;f(X)\u0026#39;, \u0026#39;model\u0026#39;), pch = c(16, 16, NA, NA), col = c(grey(0.2), \u0026#39;red\u0026#39;, \u0026#39;black\u0026#39;, \u0026#39;blue\u0026#39;), lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)  Bibliograpy An Introduction to Statistical Learning by Gareth James\n ","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"7c2bdd460840ef71fbf1420e7ca61d85","permalink":"/post/statistics/splines/splines/","publishdate":"2019-08-01T00:00:00Z","relpermalink":"/post/statistics/splines/splines/","section":"post","summary":"Practical example showing how to generate data set using given function, how to split data, buld spline model on train data and how to use test data to find optimal parameters of the model.","tags":["R","Splines","Regression"],"title":"Spline model","type":"post"}]