<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mark Goldberg</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Mark Goldberg</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019</copyright><lastBuildDate>Tue, 06 Aug 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/logo.png</url>
      <title>Mark Goldberg</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Statistical learning - topics</title>
      <link>/post/statistics/statistical_learning_topics/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/statistical_learning_topics/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#statistics&#34;&gt;Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variation-analysis&#34;&gt;Variation analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#unsupervised-learning&#34;&gt;Unsupervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#supervised-regression&#34;&gt;Supervised Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#supervised-classification&#34;&gt;Supervised Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimation-of-model-parameters&#34;&gt;Estimation of model parameters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#time-series&#34;&gt;Time Series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deep-learning&#34;&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;statistics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Statistics&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Correlation&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/post/statistics/hypothesis_testing/hypothesis_testing/&#34;&gt;Hypothesis testing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;variation-analysis&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Variation analysis&lt;/h1&gt;
&lt;/div&gt;
&lt;div id=&#34;unsupervised-learning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Unsupervised Learning&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/post/statistics/hierarchical_cluster/hierarchical_cluser/&#34;&gt;Hierarchical Cluster Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/post/statistics/k_means/k_means/&#34;&gt;K-means Cluster Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Principal Component Analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;supervised-regression&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Supervised Regression&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Linear Regression&lt;/li&gt;
&lt;li&gt;Multiple regression&lt;/li&gt;
&lt;li&gt;Linear Model Selection&lt;/li&gt;
&lt;li&gt;Polynomial Regression&lt;/li&gt;
&lt;li&gt;Stepwise Regression&lt;/li&gt;
&lt;li&gt;Regularized Regression&lt;/li&gt;
&lt;li&gt;Regression Trees &amp;amp; Bagging&lt;/li&gt;
&lt;li&gt;Random Forests&lt;/li&gt;
&lt;li&gt;Imprecise Regression&lt;/li&gt;
&lt;li&gt;Lasso regression&lt;/li&gt;
&lt;li&gt;Ridge regression&lt;/li&gt;
&lt;li&gt;ElasticNet regression&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;supervised-classification&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Supervised Classification&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Naïve Bayes&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/post/statistics/logistic_regression/logistic_regression&#34;&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Multinomial logistic regression&lt;/li&gt;
&lt;li&gt;Ordinal logistic regression&lt;/li&gt;
&lt;li&gt;Linear &amp;amp; Quadratic Discriminant Analysis&lt;/li&gt;
&lt;li&gt;Support Vector Machines&lt;/li&gt;
&lt;li&gt;Random Forests and Boosting&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation-of-model-parameters&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimation of model parameters&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;/post/statistics/model_evaluation_metrics/model_evaluation_metrics&#34;&gt;Model evaluation metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/post/statistics/resampling_methods/resampling_methods&#34;&gt;Resampling Methods&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;time-series&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Time Series&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Exploring &amp;amp; Visualizing Times Series&lt;/li&gt;
&lt;li&gt;Benchmark Methods &amp;amp; Forecast Accuracy&lt;/li&gt;
&lt;li&gt;Moving Averages&lt;/li&gt;
&lt;li&gt;Exponential Smoothing&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;deep-learning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Deep Learning&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Neural Network Fundamentals&lt;/li&gt;
&lt;li&gt;Neural Network for Regression&lt;/li&gt;
&lt;li&gt;Neural Network for Classification&lt;/li&gt;
&lt;li&gt;Feedforward Deep Learning with Keras &amp;amp; Tensorflow&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;/post/statistics/hopfield_network/hopfield_network&#34;&gt;Hopfield Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hierarchical cluster analysis</title>
      <link>/post/statistics/hierarchical_cluster/hierarchical_cluser/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/hierarchical_cluster/hierarchical_cluser/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#hierarchical-cluster-analysis&#34;&gt;Hierarchical cluster analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#choosing-optimal-number-of-clusters&#34;&gt;Choosing optimal number of clusters&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#elbow-method&#34;&gt;Elbow method&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#choosing-the-best-clustering-model&#34;&gt;Choosing the best clustering model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hierarchical-cluster-analysis-1&#34;&gt;Hierarchical cluster analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bibliography&#34;&gt;Bibliography&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;hierarchical-cluster-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical cluster analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Prepare data for analysis &lt;code&gt;datasets::USArrests&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(datasets)
df &amp;lt;- datasets::USArrests
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            Murder Assault UrbanPop Rape
## Alabama      13.2     236       58 21.2
## Alaska       10.0     263       48 44.5
## Arizona       8.1     294       80 31.0
## Arkansas      8.8     190       50 19.5
## California    9.0     276       91 40.6
## Colorado      7.9     204       78 38.7&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# check if &amp;#39;NA&amp;#39; values present in data
any(is.na(df))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# remove &amp;#39;NA&amp;#39; if necessary
df &amp;lt;- na.omit(df)
# normalize
df &amp;lt;- scale(df)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-optimal-number-of-clusters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing optimal number of clusters&lt;/h2&gt;
&lt;div id=&#34;elbow-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Elbow method&lt;/h3&gt;
&lt;p&gt;plot &lt;strong&gt;k&lt;/strong&gt; ~ &lt;strong&gt;wss&lt;/strong&gt;, where &lt;strong&gt;k&lt;/strong&gt; - is a cluseter number and &lt;strong&gt;wss&lt;/strong&gt; is a total within-cluster sum of square.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wss &amp;lt;- (nrow(df)-1)*sum(apply(df,2,var))
for (i in 2:15) wss[i] &amp;lt;- sum(kmeans(df, centers=i)$withinss)
plot(1:15, wss, type=&amp;quot;b&amp;quot;,
     xlab=&amp;quot;Number of Clusters&amp;quot;,
     ylab=&amp;quot;Within groups sum of squares&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/hierarchical_cluster/hierarchical_cluser_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt; This diagram shows that 4 number of clusers is optimal for this dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-the-best-clustering-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing the best clustering model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hc2 &amp;lt;- cluster::agnes(df, method=&amp;#39;complete&amp;#39;)
hc2$ac&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8531583&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This metric allows to estimate the quality of cluster. Closer to 1 is better.&lt;br /&gt;
Using this metric we can try several models and choose the best one.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- c( &amp;quot;average&amp;quot;, &amp;quot;single&amp;quot;, &amp;quot;complete&amp;quot;, &amp;quot;ward&amp;quot;)
names(m) &amp;lt;- c( &amp;quot;average&amp;quot;, &amp;quot;single&amp;quot;, &amp;quot;complete&amp;quot;, &amp;quot;ward&amp;quot;)
# function to compute coefficient
ac &amp;lt;- function(x) { cluster::agnes(df, method = x)$ac }

purrr::map_dbl(m, ac)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   average    single  complete      ward 
## 0.7379371 0.6276128 0.8531583 0.9346210&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see the &lt;strong&gt;ward method&lt;/strong&gt; gives the best clustering.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-cluster-analysis-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hierarchical cluster analysis&lt;/h2&gt;
&lt;p&gt;Let’s first split data into 4 groups using &lt;code&gt;clust::hclust&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# calculate distances
d &amp;lt;- dist(df, method=&amp;#39;euclidean&amp;#39;)

# hierarchical cluster analysis
# &amp;#39;ward.D2&amp;#39; method is equivalent of agnes &amp;#39;ward&amp;#39;
hc1 &amp;lt;- hclust(d, method=&amp;#39;ward.D2&amp;#39;)

# Plot the obtained dendrogram
plot(hc1, hang = -1, cex = 0.6)

# show 4 clusers
rect.hclust(hc1, k=4, border=&amp;quot;blue&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/hierarchical_cluster/hierarchical_cluser_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# group data by clusters
groups &amp;lt;- cutree(hc1, k=3)
names(groups[groups == 1])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Alabama&amp;quot;        &amp;quot;Alaska&amp;quot;         &amp;quot;Arizona&amp;quot;        &amp;quot;California&amp;quot;    
##  [5] &amp;quot;Colorado&amp;quot;       &amp;quot;Florida&amp;quot;        &amp;quot;Georgia&amp;quot;        &amp;quot;Illinois&amp;quot;      
##  [9] &amp;quot;Louisiana&amp;quot;      &amp;quot;Maryland&amp;quot;       &amp;quot;Michigan&amp;quot;       &amp;quot;Mississippi&amp;quot;   
## [13] &amp;quot;Nevada&amp;quot;         &amp;quot;New Mexico&amp;quot;     &amp;quot;New York&amp;quot;       &amp;quot;North Carolina&amp;quot;
## [17] &amp;quot;South Carolina&amp;quot; &amp;quot;Tennessee&amp;quot;      &amp;quot;Texas&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# check for cluster metrics
names(hc1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;merge&amp;quot;       &amp;quot;height&amp;quot;      &amp;quot;order&amp;quot;       &amp;quot;labels&amp;quot;      &amp;quot;method&amp;quot;     
## [6] &amp;quot;call&amp;quot;        &amp;quot;dist.method&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can split data into 4 groups using &lt;code&gt;cluster::agnes&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# using &amp;#39;agnes&amp;#39; for hierarhical clustering
hc3 &amp;lt;- cluster::agnes(df, method=&amp;#39;ward&amp;#39;)
# plot slaster
cluster::pltree(hc3, hang = -1, cex = 0.6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/hierarchical_cluster/hierarchical_cluser_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# split into groups
groups &amp;lt;- cutree(as.hclust(hc3), k = 4)
groups[groups==1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        Alabama        Georgia      Louisiana    Mississippi North Carolina 
##              1              1              1              1              1 
## South Carolina      Tennessee 
##              1              1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://uc-r.github.io/hc_clustering&#34;&gt;UC Business Analytics R Programming Guide&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression (Math)</title>
      <link>/post/statistics/linear_regression_math/linear_regression_math/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/linear_regression_math/linear_regression_math/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#simple-linear-regression&#34;&gt;Simple linear regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-find-coefficients-0-and-1&#34;&gt;How to find coefficients β0 and β1?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#matrix-form-for-multiple-regression&#34;&gt;Matrix form for multiple regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;simple-linear-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simple linear regression&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat y = \beta_0 + \beta_1 x\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; - expectd value of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, the same as &lt;span class=&#34;math inline&#34;&gt;\(E(Y|x)\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; - intercept&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; - slope&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat \beta_0 = \bar y - \hat\beta_1 \bar x\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_1 = \frac{\displaystyle\sum_{i=1}^{n} (x_i - \bar x)(y_i-\bar y)}{\displaystyle\sum_{i=1}^{n} (x_i - \bar x)^2}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-find-coefficients-0-and-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to find coefficients β0 and β1?&lt;/h2&gt;
&lt;p&gt;There are several ways to extimate coefficients of linear regression. Here we discuss the least squares approach. Other aproaches include &lt;strong&gt;maximul likelihood estimation&lt;/strong&gt;.&lt;br /&gt;
To estimate &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; we should find &lt;strong&gt;minimum&lt;/strong&gt; of &lt;strong&gt;sum of squared residuals&lt;/strong&gt; (&lt;span class=&#34;math inline&#34;&gt;\(SSR\)&lt;/span&gt;)&lt;br /&gt;
To find this minimum we should calculate derivatives of &lt;span class=&#34;math inline&#34;&gt;\(SSR\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and set them to 0:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle\min_{\beta_0,\beta1} : SSR \implies \frac{\partial SSR}{\partial \beta_0} = \frac{\partial SSR}{\partial \beta_1} = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here is the solution:&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(SSR = \displaystyle\sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1x_i))^2 = \displaystyle\sum_{i=1}^{n} (y_i^2 - 2y_i \beta_0 - 2y_i \beta_1 x_i + \beta_0^2 + 2\beta_0\beta_1x_i + \beta_1^2x_i^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial SSR}{\partial\beta_0} = \displaystyle\sum_{i=1}^{n}(-2y_i + 2\beta_0 + 2\beta_1x_i)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle\sum_{i=1}^{n} (-y_i + \hat\beta_0 + \hat\beta_1 x_i) = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\bar y = \frac{1}{n} \displaystyle\sum_{i=1}^{n} y_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar x = \frac{1}{n} \displaystyle\sum_{i=1}^{n} x_i \implies -n\bar y + n \hat\beta_0 + \hat\beta_1 n \hat x = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat \beta_0 = \bar y - \hat\beta_1 \bar x\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial SSR}{\partial \beta_1} = \displaystyle\sum_{i=1}^{n} (-2x_i y_i + 2\beta_0 x_i + 2\beta_1 x_i^2)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(-\displaystyle\sum_{i=1}^{n} x_i y_i + \hat\beta_0 \displaystyle\sum_{i=1}^{n} x_i + \hat\beta_1\displaystyle\sum_{i=1}^{n} x_i^2 = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(-\displaystyle\sum_{i=1}^{n} x_i y_i + (\bar y - \hat\beta_1 \bar x) \displaystyle\sum_{i=1}^{n} x_i + \hat\beta_1\displaystyle\sum_{i=1}^{n} x_i^2 = 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_1 = \frac{\displaystyle\sum_{i=1}^{n} x_i(y_i-\bar y)}{\displaystyle\sum_{i=1}^{n} x_i (x_i - \bar x)} = \frac{\displaystyle\sum_{i=1}^{n} (x_i - \bar x)(y_i-\bar y)}{\displaystyle\sum_{i=1}^{n} (x_i - \bar x)^2} = \frac{Cov(x,y)}{Var(x)} = r_{xy} \frac{s_y}{s_x}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat y\)&lt;/span&gt; - estimated &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\bar x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar y\)&lt;/span&gt; - averages of &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(r_{xy}\)&lt;/span&gt; - sample correlation coefficient between &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(s_x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s_y\)&lt;/span&gt; - uncorrected sample standard deviations of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(Var\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Cov\)&lt;/span&gt; - sample variance and sample covariance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-form-for-multiple-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Matrix form for multiple regression&lt;/h2&gt;
&lt;p&gt;We can write &lt;span class=&#34;math inline&#34;&gt;\(\hat y = \beta_0 + \beta_1 x\)&lt;/span&gt; in a matrix form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y = \begin{bmatrix}y_1\\y_2\\\vdots\\y_n\end{bmatrix}\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(b = \begin{bmatrix}\beta_1\\\beta_2\\\vdots\\\beta_n\end{bmatrix}\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(X = \begin{bmatrix}1 &amp;amp; x_{1,1} &amp;amp; x_{1,2} \dots &amp;amp; x_{1,k}\\1 &amp;amp; x_{2,1} &amp;amp; x_{2,2} \dots &amp;amp; x_{2,k}\\\vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots\\1 &amp;amp; x_{n,1} &amp;amp; x_{n,2} \dots &amp;amp; x_{n,k}\end{bmatrix}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y = Xb\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(X&amp;#39;Y = X&amp;#39;Xb\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\((X&amp;#39;X)^{-1}X&amp;#39;Xb = (X&amp;#39;X)^{-1}X&amp;#39;Y\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(b = (X&amp;#39;X)^{-1}X&amp;#39;Y\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\((X&amp;#39;X)^{-1}X&amp;#39;X = I\)&lt;/span&gt; - identity matrix&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Logistic regression</title>
      <link>/post/statistics/logistic_regression/logistic_regression/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/logistic_regression/logistic_regression/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#background&#34;&gt;Background&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#binomial-logistic-regression&#34;&gt;Binomial logistic regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression&#34;&gt;Logistic regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#linear-discriminant-analysis-lda&#34;&gt;Linear Discriminant Analysis (LDA)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quadratic-discriminant-analysis-qda&#34;&gt;Quadratic Discriminant Analysis (QDA)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#roc-curve-for-lda&#34;&gt;ROC-curve for LDA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tasks&#34;&gt;Tasks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Logistic regression builds model for binary dependent variables (0/1, True/False).&lt;/p&gt;
&lt;p&gt;Logistic function: &lt;span class=&#34;math display&#34;&gt;\[Y = \frac{1}{1+e^l} = \frac{e^l}{e^l+1}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(l\)&lt;/span&gt; is a linear combination of all observations (log-odds): &lt;span class=&#34;math inline&#34;&gt;\(l = \beta_0 + \beta_{1}x_{1} + \beta_{2}x_{2} + ... + \beta_{p}x_{p} + \epsilon\)&lt;/span&gt;&lt;br /&gt;
See also: &lt;a href=&#34;https://en.wikipedia.org/wiki/Sigmoid_function&#34;&gt;Sigmoid functions&lt;/a&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;binomial-logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Binomial logistic regression&lt;/h2&gt;
&lt;p&gt;Probability of passing an exam versus hours of study.&lt;br /&gt;
Data from &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34;&gt;wiki&lt;/a&gt; describe if students &lt;strong&gt;pass&lt;/strong&gt; exam depending of how many &lt;strong&gt;hours&lt;/strong&gt; they studied.&lt;br /&gt;
We build &lt;strong&gt;logistic regression&lt;/strong&gt; model to predict if ‘pass’ depending on learning ‘hours’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# put data into dataframe
hours=c(0.50,.75,1,1.25,1.5,1.75,1.75,2,2.25,2.5,2.75,3,3.25,3.50,4,4.25,4.5,4.75,5,5.5)
pass=c(0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1)
df = data.frame(hours, pass)

# Logistic Regression model
model.logit &amp;lt;- glm(pass ~ hours, data = df, family = &amp;#39;binomial&amp;#39;)

summary(model.logit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = pass ~ hours, family = &amp;quot;binomial&amp;quot;, data = df)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.70557  -0.57357  -0.04654   0.45470   1.82008  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)  
## (Intercept)  -4.0777     1.7610  -2.316   0.0206 *
## hours         1.5046     0.6287   2.393   0.0167 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 27.726  on 19  degrees of freedom
## Residual deviance: 16.060  on 18  degrees of freedom
## AIC: 20.06
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Coefficients Intercept = -4.0777 and Hours = 1.5046 are entered in the &lt;strong&gt;logistic regression equation&lt;/strong&gt; to estimate the odds (probability) of passing the exam: &lt;span class=&#34;math inline&#34;&gt;\(1/(1+e^{-(-4.0777+1.5046\cdot hours)})\)&lt;/span&gt; Calculate the probability to pass exam if studied 4 hours:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1/(1+exp(-(-4.0777+1.5046*4)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.874429&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s find a critical point where probability is 0.5:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;crit = -coef(model.logit)[1]/coef(model.logit)[2]
crit&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept) 
##    2.710083&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# predict &amp;#39;pass&amp;#39; for given data
df$predic.prob &amp;lt;- predict(model.logit, df, type=&amp;quot;response&amp;quot;)
df$predic.pass &amp;lt;-  ifelse(df$predic.prob &amp;gt; 0.5, 1, 0)
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    hours pass predic.prob predic.pass
## 1   0.50    0  0.03471034           0
## 2   0.75    0  0.04977295           0
## 3   1.00    0  0.07089196           0
## 4   1.25    0  0.10002862           0
## 5   1.50    0  0.13934447           0
## 6   1.75    0  0.19083650           0
## 7   1.75    1  0.19083650           0
## 8   2.00    0  0.25570318           0
## 9   2.25    1  0.33353024           0
## 10  2.50    0  0.42162653           0
## 11  2.75    1  0.51501086           1
## 12  3.00    0  0.60735865           1
## 13  3.25    1  0.69261733           1
## 14  3.50    0  0.76648084           1
## 15  4.00    1  0.87444750           1
## 16  4.25    1  0.91027764           1
## 17  4.50    1  0.93662366           1
## 18  4.75    1  0.95561071           1
## 19  5.00    1  0.96909707           1
## 20  5.50    1  0.98519444           1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot data
plot(df$hours, df$pass, pch=19, col=&amp;#39;black&amp;#39;,
     main=&amp;#39;Probability of Passing Exam vs Hours Studying&amp;#39;,
     ylab=&amp;#39;Probability of Passing Exam&amp;#39;,
     xlab=&amp;#39;Hours Studying&amp;#39;)

# data frame to build a logistic function curve &amp;#39;hours~pass&amp;#39;
df2 &amp;lt;- data.frame(hours=seq(min(df$hours),max(df$hours),0.1), pass=NA)

# predict &amp;#39;pass&amp;#39; from our model
df2$pass &amp;lt;- predict(model.logit, df2, type=&amp;quot;response&amp;quot;)
# draw logistic function for our data sets
lines(df2$pass~df2$hours, lwd=2)
# critical point
abline(h=0.5, col=&amp;#39;green&amp;#39;) # 
abline(v=crit, col=&amp;#39;red&amp;#39;) # 

# draw predicted points (-0.02 to avoid overlapping with actual data)
points(df$hours, df$predic.pass-0.03, pch=19, col=&amp;#39;red&amp;#39;)

legend(&amp;#39;bottomright&amp;#39;, lty=c(1,1,1,1),
       col = c(&amp;#39;black&amp;#39;, &amp;#39;red&amp;#39;, &amp;#39;black&amp;#39;, &amp;#39;green&amp;#39;, &amp;#39;red&amp;#39;),
       legend = c(&amp;#39;actual data&amp;#39;, &amp;#39;predicted&amp;#39;, &amp;#39;Logistic function&amp;#39;, &amp;#39;Decision p&amp;#39;, &amp;#39;Decision bound&amp;#39;),
       lwd=c(NA,NA,1,1,1), pch=c(19,19,NA,NA,NA), bty = &amp;#39;n&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/logistic_regression/logistic_regression_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Logistic regression&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Data&lt;/em&gt;: generated credit card balance &lt;code&gt;Default{ISLR}&lt;/code&gt;. 10000 observations for 4 variables:&lt;br /&gt;
&lt;strong&gt;default&lt;/strong&gt; – binary variable: Yes, if credit card holder did not return debt;&lt;br /&gt;
&lt;strong&gt;student&lt;/strong&gt; – binary variable: Yes, if credit card holder is a student;&lt;br /&gt;
&lt;strong&gt;balance&lt;/strong&gt; – average month balance on the bank account;&lt;br /&gt;
&lt;strong&gt;income&lt;/strong&gt; – income of credit card holder.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;#39;ISLR&amp;#39;)
head(Default)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   default student   balance    income
## 1      No      No  729.5265 44361.625
## 2      No     Yes  817.1804 12106.135
## 3      No      No 1073.5492 31767.139
## 4      No      No  529.2506 35704.494
## 5      No      No  785.6559 38463.496
## 6      No     Yes  919.5885  7491.559&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
# train subset rate is 0.85
inTrain &amp;lt;- sample(seq_along(Default$default), nrow(Default)*0.85)
df &amp;lt;- Default[inTrain, ]

# logistic regression model &amp;#39;default ~ f(balance)&amp;#39;
model.logit &amp;lt;- glm(default ~ balance, data = df, family = &amp;#39;binomial&amp;#39;)
summary(model.logit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = default ~ balance, family = &amp;quot;binomial&amp;quot;, data = df)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.3282  -0.1420  -0.0553  -0.0201   3.7934  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -1.088e+01  4.022e-01  -27.07   &amp;lt;2e-16 ***
## balance      5.657e-03  2.448e-04   23.11   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2509.1  on 8499  degrees of freedom
## Residual deviance: 1336.5  on 8498  degrees of freedom
## AIC: 1340.5
## 
## Number of Fisher Scoring iterations: 8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Predict &amp;#39;default&amp;#39; by &amp;#39;balance&amp;#39;
p.logit &amp;lt;- predict(model.logit, df, type = &amp;#39;response&amp;#39;)
predicted &amp;lt;- factor(ifelse(p.logit &amp;gt; 0.5, 2, 1),
                  levels = c(1, 2),
                  labels = c(&amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39;))

# true values for &amp;#39;default&amp;#39; in train data
actual &amp;lt;- df$default

# confusion matrix
conf.m &amp;lt;- table(actual, predicted)
conf.m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       predicted
## actual   No  Yes
##    No  8172   41
##    Yes  194   93&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sensitivity
conf.m[2, 2] / sum(conf.m[2, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3240418&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# specificity
conf.m[1, 1] / sum(conf.m[1, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9950079&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# probability
sum(diag(conf.m)) / sum(conf.m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9723529&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;linear-discriminant-analysis-lda&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear Discriminant Analysis (LDA)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#library(&amp;#39;GGally&amp;#39;)
library(&amp;#39;MASS&amp;#39;)
model.lda &amp;lt;- lda(default ~ balance, data = Default[inTrain, ])
model.lda&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## lda(default ~ balance, data = Default[inTrain, ])
## 
## Prior probabilities of groups:
##         No        Yes 
## 0.96623529 0.03376471 
## 
## Group means:
##       balance
## No   801.1297
## Yes 1757.2025
## 
## Coefficients of linear discriminants:
##                LD1
## balance 0.00220817&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Predict
p.lda &amp;lt;- predict(model.lda, df, type = &amp;#39;response&amp;#39;)
actual &amp;lt;- factor(ifelse(p.lda$posterior[, &amp;#39;Yes&amp;#39;] &amp;gt; 0.5, 
                         2, 1),
                  levels = c(1, 2),
                  labels = c(&amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39;))

# confusion matrix
conf.m &amp;lt;- table(actual, predicted)
conf.m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       predicted
## actual   No  Yes
##    No  8366   42
##    Yes    0   92&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sensitivity
conf.m[2, 2] / sum(conf.m[2, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# specificity
conf.m[1, 1] / sum(conf.m[1, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9950048&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# true
sum(diag(conf.m)) / sum(conf.m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9950588&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;quadratic-discriminant-analysis-qda&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quadratic Discriminant Analysis (QDA)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.qda &amp;lt;- qda(default ~ balance, data = Default[inTrain, ])
model.qda&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
## qda(default ~ balance, data = Default[inTrain, ])
## 
## Prior probabilities of groups:
##         No        Yes 
## 0.96623529 0.03376471 
## 
## Group means:
##       balance
## No   801.1297
## Yes 1757.2025&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# predict
p.qda &amp;lt;- predict(model.qda, df, type = &amp;#39;response&amp;#39;)
predict &amp;lt;- factor(ifelse(p.qda$posterior[, &amp;#39;Yes&amp;#39;] &amp;gt; 0.5, 
                         2, 1),
                  levels = c(1, 2),
                  labels = c(&amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39;))

# confusion matrix
conf.m &amp;lt;- table(actual, predict)
conf.m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       predict
## actual   No  Yes
##    No  8390   18
##    Yes    0   92&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sensitivity
conf.m[2, 2] / sum(conf.m[2, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# specificity
conf.m[1, 1] / sum(conf.m[1, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9978592&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# true
sum(diag(conf.m)) / sum(conf.m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9978824&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;roc-curve-for-lda&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ROC-curve for LDA&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# считаем 1-SPC и TPR для всех вариантов границы отсечения
x &amp;lt;- NULL    # для (1 - SPC)
y &amp;lt;- NULL    # для TPR

# confusion matrix
tbl &amp;lt;- as.data.frame(matrix(rep(0, 4), 2, 2))
rownames(tbl) &amp;lt;- c(&amp;#39;fact.No&amp;#39;, &amp;#39;fact.Yes&amp;#39;)
colnames(tbl) &amp;lt;- c(&amp;#39;predict.No&amp;#39;, &amp;#39;predict.Yes&amp;#39;)

# probability vector
p.vector &amp;lt;- seq(0, 1, length = 501)

# цикл по вероятностям отсечения
for (p in p.vector){
    # prediction
    prediction &amp;lt;- factor(ifelse(p.lda$posterior[, &amp;#39;Yes&amp;#39;] &amp;gt; p, 
                             2, 1),
                      levels = c(1, 2),
                      labels = c(&amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39;))
    
    # data frame to compare data with prediction
    df.compare &amp;lt;- data.frame(actual = actual, prediction = prediction)
    
    # fill confusion matrix
    tbl[1, 1] &amp;lt;- nrow(df.compare[df.compare$Факт == &amp;#39;No&amp;#39; &amp;amp; df.compare$Прогноз == &amp;#39;No&amp;#39;, ])
    tbl[2, 2] &amp;lt;- nrow(df.compare[df.compare$Факт == &amp;#39;Yes&amp;#39; &amp;amp; df.compare$Прогноз == &amp;#39;Yes&amp;#39;, ])
    tbl[1, 2] &amp;lt;- nrow(df.compare[df.compare$Факт == &amp;#39;No&amp;#39; &amp;amp; df.compare$Прогноз == &amp;#39;Yes&amp;#39;, ])
    tbl[2, 1] &amp;lt;- nrow(df.compare[df.compare$Факт == &amp;#39;Yes&amp;#39; &amp;amp; df.compare$Прогноз == &amp;#39;No&amp;#39;, ])
    
    # calculate metrix
    TPR &amp;lt;- tbl[2, 2] / sum(tbl[2, 2] + tbl[2, 1])
    y &amp;lt;- c(y, TPR)
    SPC &amp;lt;- tbl[1, 1] / sum(tbl[1, 1] + tbl[1, 2])
    x &amp;lt;- c(x, 1 - SPC)
}

# ROC-curve
par(mar = c(5, 5, 1, 1))
# curve
plot(x, y, type = &amp;#39;l&amp;#39;, col = &amp;#39;blue&amp;#39;, lwd = 3,
     xlab = &amp;#39;(1 - SPC)&amp;#39;, ylab = &amp;#39;TPR&amp;#39;, 
     xlim = c(0, 1), ylim = c(0, 1))
# line of random classifier
abline(a = 0, b = 1, lty = 3, lwd = 2)

# oint for probability 0.5
points(x[p.vector == 0.5], y[p.vector == 0.5], pch = 16)
text(x[p.vector == 0.5], y[p.vector == 0.5], &amp;#39;p = 0.5&amp;#39;, pos = 4)
# point for probability 0.2
points(x[p.vector == 0.2], y[p.vector == 0.2], pch = 16)
text(x[p.vector == 0.2], y[p.vector == 0.2], &amp;#39;p = 0.2&amp;#39;, pos = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/logistic_regression/logistic_regression_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;predict &amp;lt;- factor(ifelse(p.lda$posterior[, &amp;#39;Yes&amp;#39;] &amp;gt; 0.2, 2, 1),
                      levels = c(1, 2),
                      labels = c(&amp;#39;No&amp;#39;, &amp;#39;Yes&amp;#39;))

conf.m &amp;lt;- table(actual, predict)
conf.m&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       predict
## actual   No  Yes
##    No  8124  284
##    Yes    0   92&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# sensitivity
conf.m[2, 2] / sum(conf.m[2, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# specificity
conf.m[1, 1] / sum(conf.m[1, ])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9662226&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# true
sum(diag(conf.m)) / sum(conf.m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9665882&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tasks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tasks&lt;/h2&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>k-means</title>
      <link>/post/statistics/k_means/k_means/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/k_means/k_means/</guid>
      <description>


&lt;div id=&#34;k-means-cluster-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;K-means cluster analysis&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(datasets)
df &amp;lt;- datasets::iris
head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 3 species:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unique(df$Species)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] setosa     versicolor virginica 
## Levels: setosa versicolor virginica&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We try to predict them by &lt;em&gt;Petal.Lenght&lt;/em&gt;and &lt;em&gt;Petal.Width&lt;/em&gt; variables using k-means clustering.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Plot Petal.Length ~ Petal.Width data
plot(df$Petal.Length ~ df$Petal.Width)

# Find number of clusters using wss
wss &amp;lt;- (nrow(df[, 3:4])-1)*sum(apply(df[, 3:4],2,var))
for (i in 2:15) wss[i] &amp;lt;- sum(kmeans(df[, 3:4], i)$withinss)
plot(1:15, wss, type=&amp;quot;b&amp;quot;, xlab=&amp;quot;Number of Clusters&amp;quot;, ylab=&amp;quot;Within groups sum of squares&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;More than 3 clusters give no obvious advantages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Make k-means with 3 clasters
ncl &amp;lt;- 3
cl &amp;lt;- stats::kmeans(df[, 3:4], ncl, nstart = 20)
cl&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## K-means clustering with 3 clusters of sizes 50, 48, 52
## 
## Cluster means:
##   Petal.Length Petal.Width
## 1     1.462000    0.246000
## 2     5.595833    2.037500
## 3     4.269231    1.342308
## 
## Clustering vector:
##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
##  [71] 3 3 3 3 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2
## [106] 2 3 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 3 2
## [141] 2 2 2 2 2 2 2 2 2 2
## 
## Within cluster sum of squares by cluster:
## [1]  2.02200 16.29167 13.05769
##  (between_SS / total_SS =  94.3 %)
## 
## Available components:
## 
## [1] &amp;quot;cluster&amp;quot;      &amp;quot;centers&amp;quot;      &amp;quot;totss&amp;quot;        &amp;quot;withinss&amp;quot;    
## [5] &amp;quot;tot.withinss&amp;quot; &amp;quot;betweenss&amp;quot;    &amp;quot;size&amp;quot;         &amp;quot;iter&amp;quot;        
## [9] &amp;quot;ifault&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compair result of clustering with real data (3 species of iris are in analysis)
table(cl$cluster, df$Species)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    
##     setosa versicolor virginica
##   1     50          0         0
##   2      0          2        46
##   3      0         48         4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot data
clusters &amp;lt;- split.data.frame(df, cl$cluster)
xlim &amp;lt;- c(min(df$Petal.Width), max(df$Petal.Width))
ylim &amp;lt;- c(min(df$Petal.Length), max(df$Petal.Length))
col &amp;lt;- c(&amp;#39;red&amp;#39;, &amp;#39;green&amp;#39;, &amp;#39;blue&amp;#39;)
plot(0, xlab=&amp;#39;Petal width&amp;#39;, ylab=&amp;#39;Petal length&amp;#39;, xlim=xlim, ylim=ylim)
for ( i in 1:ncl ) {
  points(clusters[[i]]$Petal.Length ~ clusters[[i]]$Petal.Width, col=col[i], xlim=xlim, ylim=ylim)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/k_means/k_means_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hypothesis testing</title>
      <link>/post/statistics/hypothesis_testing/hypothesis_testing/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/hypothesis_testing/hypothesis_testing/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#t-test&#34;&gt;t-test&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#t-test-and-normal-distribution&#34;&gt;t-test and normal distribution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#one-sample-t-test&#34;&gt;One-sample t-test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#two-samples-t-test&#34;&gt;Two samples t-test&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-of-r-functions-for-t-tests&#34;&gt;Summary of R functions for t-tests&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-that&#34;&gt;What is that?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#non-parametric-tests&#34;&gt;Non-parametric tests&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#mann-whitney-u-rank-sum-test&#34;&gt;Mann-Whitney U Rank Sum Test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wilcoxon-test&#34;&gt;Wilcoxon test&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tests-for-categorical-variables&#34;&gt;Tests for categorical variables&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#chi-squared-tests&#34;&gt;Chi-squared tests&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-testing&#34;&gt;Multiple testing&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#the-bonferroni-correction&#34;&gt;The Bonferroni correction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sources&#34;&gt;Sources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Null hypothesis (H&lt;sub&gt;0&lt;/sub&gt;):&lt;br /&gt;
1. H&lt;sub&gt;0&lt;/sub&gt;: m = μ&lt;br /&gt;
2. H&lt;sub&gt;0&lt;/sub&gt;: m &lt;span class=&#34;math inline&#34;&gt;\(\leq\)&lt;/span&gt; μ&lt;br /&gt;
3. H&lt;sub&gt;0&lt;/sub&gt;: m &lt;span class=&#34;math inline&#34;&gt;\(\geq\)&lt;/span&gt; μ&lt;/p&gt;
&lt;p&gt;Alternative hypotheses (H&lt;sub&gt;a&lt;/sub&gt;): 1. H&lt;sub&gt;a&lt;/sub&gt;:m ≠ μ (different)&lt;br /&gt;
2. H&lt;sub&gt;a&lt;/sub&gt;:m &amp;gt; μ (greater)&lt;br /&gt;
3. H&lt;sub&gt;a&lt;/sub&gt;:m &amp;lt; μ (less)&lt;/p&gt;
&lt;p&gt;Note: Hypothesis 1. are called &lt;strong&gt;two-tailed tests&lt;/strong&gt; and hypotheses 2. &amp;amp; 3. are called &lt;strong&gt;one-tailed tests&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The p-value is the probability that the observed data could happen, under the condition that the null hypothesis is true.&lt;/p&gt;
&lt;p&gt;Note: p-value is not the probability that the null hypothesis is true.&lt;br /&gt;
Note: Absence of evidence ⧧ evidence of absence.&lt;/p&gt;
&lt;p&gt;Cutoffs for hypothesis testing *p &amp;lt; 0.05, **p &amp;lt; 0.01, ***p &amp;lt; 0.001. If p value is less than significance level alpha (0.05), the hull hypothesies is rejected.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;not rejected (‘negative’)&lt;/th&gt;
&lt;th&gt;rejected (‘positive’)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;H&lt;sub&gt;0&lt;/sub&gt; true&lt;/td&gt;
&lt;td&gt;True negative (specificity)&lt;/td&gt;
&lt;td&gt;False Positive (Type I error)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;H&lt;sub&gt;0&lt;/sub&gt; false&lt;/td&gt;
&lt;td&gt;False Negative (Type II error)&lt;/td&gt;
&lt;td&gt;True positive (sensitivity)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Type II errors are usually more dangerous.&lt;/p&gt;
&lt;div id=&#34;t-test&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;t-test&lt;/h1&gt;
&lt;div id=&#34;t-test-and-normal-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;t-test and normal distribution&lt;/h2&gt;
&lt;p&gt;t-distribution assumes that the observations are &lt;strong&gt;independent&lt;/strong&gt; and that they follow a &lt;strong&gt;normal distribution&lt;/strong&gt;. If the data are &lt;strong&gt;dependent&lt;/strong&gt;, then p-values will likely be totally wrong (e.g., for positive correlation, too optimistic). Type II errors?&lt;br /&gt;
It is good to test if observations are normally distributed. Otherwise we assume that data is normally distributed.&lt;br /&gt;
Independence of observations is usually not testable, but can be reasonably assumed if the data collection process was random without replacement.&lt;/p&gt;
&lt;p&gt;FIXME: I do not understand this. Deviation data from normalyty will lead to type-I errors. I data is deviated from normal distribution, use &lt;strong&gt;Wilcoxon test&lt;/strong&gt; or &lt;strong&gt;permutation tests&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;one-sample-t-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;One-sample t-test&lt;/h2&gt;
&lt;p&gt;One-sample t-test is used to compare the mean of one sample to a known standard (or theoretical/hypothetical) mean (μ).&lt;br /&gt;
t-statistics: &lt;span class=&#34;math inline&#34;&gt;\(t = \frac{m - \mu}{s/\sqrt{n}}\)&lt;/span&gt;, where&lt;br /&gt;
&lt;strong&gt;m&lt;/strong&gt; is the sample &lt;strong&gt;mean&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;n&lt;/strong&gt; is the sample &lt;strong&gt;size&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;s&lt;/strong&gt; is the sample &lt;strong&gt;standard deviation&lt;/strong&gt; with n−1 degrees of freedom&lt;br /&gt;
&lt;strong&gt;μ&lt;/strong&gt; is the &lt;strong&gt;theoretical value&lt;/strong&gt;&lt;br /&gt;
Q: And what should I do with this t-statistics?&lt;br /&gt;
Q: What is the difference between t-test and ANOVA?&lt;br /&gt;
Q: What is the smallest sample size which can be tested by t-test?&lt;br /&gt;
Q: Show diagrams explaining why p-value of one-sided is smaller than two-sided tests.&lt;/p&gt;
&lt;p&gt;R example:&lt;br /&gt;
We want to test if N is different from given mean μ=0:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;N = c(-0.01, 0.65, -0.17, 1.77, 0.76, -0.16, 0.88, 1.09, 0.96, 0.25)
t.test(N, mu = 0, alternative = &amp;quot;less&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  N
## t = 3.0483, df = 9, p-value = 0.9931
## alternative hypothesis: true mean is less than 0
## 95 percent confidence interval:
##      -Inf 0.964019
## sample estimates:
## mean of x 
##     0.602&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(N, mu = 0, alternative = &amp;quot;two.sided&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  N
## t = 3.0483, df = 9, p-value = 0.01383
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  0.1552496 1.0487504
## sample estimates:
## mean of x 
##     0.602&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(N, mu = 0, alternative = &amp;quot;greater&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One Sample t-test
## 
## data:  N
## t = 3.0483, df = 9, p-value = 0.006916
## alternative hypothesis: true mean is greater than 0
## 95 percent confidence interval:
##  0.239981      Inf
## sample estimates:
## mean of x 
##     0.602&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;FIXME: why it accepts all alternatives at the same time (less and greater?)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-samples-t-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Two samples t-test&lt;/h2&gt;
&lt;p&gt;Do two different samples have the same mean?&lt;br /&gt;
H&lt;sub&gt;0&lt;/sub&gt;:&lt;br /&gt;
1. H&lt;sub&gt;0&lt;/sub&gt;: m&lt;sub&gt;1&lt;/sub&gt; - m&lt;sub&gt;2&lt;/sub&gt; = 0&lt;br /&gt;
2. H&lt;sub&gt;0&lt;/sub&gt;: m&lt;sub&gt;1&lt;/sub&gt; - m&lt;sub&gt;2&lt;/sub&gt; &lt;span class=&#34;math inline&#34;&gt;\(\leq\)&lt;/span&gt; 0&lt;br /&gt;
3. H&lt;sub&gt;0&lt;/sub&gt;: m&lt;sub&gt;1&lt;/sub&gt; - m&lt;sub&gt;2&lt;/sub&gt; &lt;span class=&#34;math inline&#34;&gt;\(\geq\)&lt;/span&gt; 0&lt;/p&gt;
&lt;p&gt;H&lt;sub&gt;a&lt;/sub&gt;:&lt;br /&gt;
1. H&lt;sub&gt;a&lt;/sub&gt;: m&lt;sub&gt;1&lt;/sub&gt; - m&lt;sub&gt;2&lt;/sub&gt; ≠ 0 (different)&lt;br /&gt;
2. H&lt;sub&gt;a&lt;/sub&gt;: m&lt;sub&gt;1&lt;/sub&gt; - m&lt;sub&gt;2&lt;/sub&gt; &amp;gt; 0 (greater)&lt;br /&gt;
3. H&lt;sub&gt;a&lt;/sub&gt;: m&lt;sub&gt;1&lt;/sub&gt; - m&lt;sub&gt;2&lt;/sub&gt; &amp;lt; 0 (less)&lt;/p&gt;
&lt;p&gt;The paired sample t-test has four main assumptions:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The dependent variable must be &lt;strong&gt;continuous&lt;/strong&gt; (interval/ratio).&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;The observations are &lt;strong&gt;independent&lt;/strong&gt; of one another.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;The dependent variable should be approximately &lt;strong&gt;normally distributed&lt;/strong&gt;.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;The dependent variable should not contain any &lt;strong&gt;outliers&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Continuous data can take on any value within a range (income, height, weight, etc.). The opposite of continuous data is discrete data, which can only take on a few values (Low, Medium, High, etc.). Occasionally, discrete data can be used to approximate a continuous scale, such as with &lt;strong&gt;Likert-type scales&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;t-statistics: &lt;span class=&#34;math inline&#34;&gt;\(t=\frac{y - x}{SE}\)&lt;/span&gt;, where y and x are the samples means. SE is the standard error for the difference. If H&lt;sub&gt;0&lt;/sub&gt; is correct, test statistic follows a t-distribution with n+m-2 degrees of freedom (n, m the number of observations in each sample).&lt;/p&gt;
&lt;p&gt;To apply t-test samples must be tested if they have equal variance:&lt;br /&gt;
equal variance (homoscedastic). Type 3 means two samples, unequal variance (heteroscedastic).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### t-test
a = c(175, 168, 168, 190, 156, 181, 182, 175, 174, 179)
b = c(185, 169, 173, 173, 188, 186, 175, 174, 179, 180)

# test homogeneity of variances using Fisher’s F-test
var.test(a,b)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  F test to compare two variances
## 
## data:  a and b
## F = 2.1028, num df = 9, denom df = 9, p-value = 0.2834
## alternative hypothesis: true ratio of variances is not equal to 1
## 95 percent confidence interval:
##  0.5223017 8.4657950
## sample estimates:
## ratio of variances 
##           2.102784&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# variance is homogene (can use var.equal=T in t.test)

# t-test
t.test(a,b, 
       var.equal=TRUE,   # variance is homogene (tested by var.test(a,b)) 
       paired=FALSE)     # samples are independent&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Two Sample t-test
## 
## data:  a and b
## t = -0.94737, df = 18, p-value = 0.356
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -10.93994   4.13994
## sample estimates:
## mean of x mean of y 
##     174.8     178.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-of-r-functions-for-t-tests&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary of R functions for t-tests&lt;/h1&gt;
&lt;p&gt;One-sample t-test&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t.test(x, mu = 0, alternative = c(&amp;quot;two.sided&amp;quot;, &amp;quot;less&amp;quot;, &amp;quot;greater&amp;quot;), paired = FALSE, var.equal = FALSE, conf.level = 0.95)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;what-is-that&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is that?&lt;/h2&gt;
&lt;p&gt;one-way ANOVA or 2-way ANOVA with &lt;strong&gt;Bonferroni multiple comparison&lt;/strong&gt; or &lt;strong&gt;Dunnett’s post-test&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;non-parametric-tests&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Non-parametric tests&lt;/h1&gt;
&lt;div id=&#34;mann-whitney-u-rank-sum-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mann-Whitney U Rank Sum Test&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The dependent variable is ordinal or continuous.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;The data consist of a randomly selected sample of independent observations from two independent groups.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;The dependent variables for the two independent groups share a similar shape.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;wilcoxon-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wilcoxon test&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;Wilcoxon&lt;/strong&gt; is a &lt;strong&gt;non-parametric test&lt;/strong&gt; which works on normal and non-normal data. However, we usually prefer not to use it if we can assume that the data is normally distributed. The non-parametric test comes with less statistical power, this is a price that one has to pay for more flexible assumptions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tests-for-categorical-variables&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tests for categorical variables&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Categorical variable&lt;/strong&gt; can take fixed number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property.&lt;/p&gt;
&lt;div id=&#34;chi-squared-tests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Chi-squared tests&lt;/h2&gt;
&lt;p&gt;The chi-squared test is most suited to large datasets. As a general rule, the chi-squared test is appropriate if at least 80% of the cells have an expected frequency of 5 or greater. In addition, none of the cells should have an expected frequency less than 1. If the expected values are very small, categories may be combined (if it makes sense to do so) to create fewer larger categories. Alternatively, Fisher’s exact test can be used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data = rbind(c(83,35), c(92,43))
data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]   83   35
## [2,]   92   43&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;chisq.test(data, correct=F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Pearson&amp;#39;s Chi-squared test
## 
## data:  data
## X-squared = 0.14172, df = 1, p-value = 0.7066&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;chisq.test(testor,correct=F) ## Fisher’s Exact test R Example:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Group&lt;/th&gt;
&lt;th&gt;TumourShrinkage-No&lt;/th&gt;
&lt;th&gt;TumourShrinkage-Yes&lt;/th&gt;
&lt;th&gt;Total&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1 Treatment&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;2 Placebo&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;3 Total&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;24&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The &lt;strong&gt;null hypothesis&lt;/strong&gt; is that there is &lt;strong&gt;no association&lt;/strong&gt; between treatment and tumour shrinkage.&lt;br /&gt;
The &lt;strong&gt;alternative hypothesis&lt;/strong&gt; is that there is &lt;strong&gt;some association&lt;/strong&gt; between treatment group and tumour shrinkage.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data = rbind(c(8,3), c(9,4))
data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    8    3
## [2,]    9    4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fisher.test(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Fisher&amp;#39;s Exact Test for Count Data
## 
## data:  data
## p-value = 1
## alternative hypothesis: true odds ratio is not equal to 1
## 95 percent confidence interval:
##   0.1456912 10.6433317
## sample estimates:
## odds ratio 
##   1.176844&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output Fisher’s exact test tells us that the probability of observing such an extreme combination of frequencies is high, our p-value is 1.000 which is clearly greater than 0.05. In this case, there is &lt;strong&gt;no evidence of an association&lt;/strong&gt; between treatment group and tumour shrinkage.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-testing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Multiple testing&lt;/h1&gt;
&lt;p&gt;When performing a large number of tests, the type I error is inflated: for α=0.05 and performing n tests, the probability of no false positive result is: 0.095 x 0.95 x … (n-times) &amp;lt;&amp;lt;&amp;lt; 0.095&lt;br /&gt;
The larger the number of tests performed, the higher the probability of a false rejection!&lt;br /&gt;
Many data analysis approaches in genomics rely on itemby-item (i.e. multiple) testing:&lt;br /&gt;
Microarray or RNA-Seq expression profiles of “normal” vs “perturbed” samples: gene-by-gene&lt;br /&gt;
ChIP-chip: locus-by-locus&lt;br /&gt;
RNAi and chemical compound screens&lt;br /&gt;
Genome-wide association studies: marker-by-marker&lt;br /&gt;
QTL analysis: marker-by-marker and trait-by-trait&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;False positive rate&lt;/strong&gt; (FPR) - the proportion of false positives among all resulst.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;False discovery rate&lt;/strong&gt; (FDR) - the proportion of false positives among all significant results.&lt;/p&gt;
&lt;p&gt;Example: 20,000 genes, 100 hits, 10 of them wrong.&lt;br /&gt;
FPR: 0.05%&lt;br /&gt;
FDR: 10%&lt;/p&gt;
&lt;div id=&#34;the-bonferroni-correction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Bonferroni correction&lt;/h2&gt;
&lt;p&gt;The Bonferroni correction sets the significance cut-off at α/n.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sources&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sources&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;http://www.sthda.com/english/wiki/one-sample-t-test-in-r&#34;&gt;One-Sample T-test in R&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Model evaluation metrics</title>
      <link>/post/statistics/model_evaluation_metrics/model_evaluation_metrics/</link>
      <pubDate>Sun, 04 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/model_evaluation_metrics/model_evaluation_metrics/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#confusion-matrix&#34;&gt;Confusion Matrix&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gain-and-lift-chart&#34;&gt;Gain and Lift Chart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kolmogorov-smirnov-chart&#34;&gt;Kolmogorov Smirnov Chart&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#auc-roc&#34;&gt;AUC – ROC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gini-coefficient&#34;&gt;Gini Coefficient&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#concordant-discordant-ratio&#34;&gt;Concordant – Discordant Ratio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#root-mean-squared-error&#34;&gt;Root Mean Squared Error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bibliography&#34;&gt;Bibliography&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://www.analyticsvidhya.com/blog/2016/02/7-important-model-evaluation-error-metrics/&#34;&gt;check this link&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;confusion-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Confusion Matrix&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;True Y&lt;/th&gt;
&lt;th&gt;True N&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Predicted Y&lt;/td&gt;
&lt;td&gt;True Positive (TP)&lt;/td&gt;
&lt;td&gt;False Positive (FP)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Predicted N&lt;/td&gt;
&lt;td&gt;False Negative (FN)&lt;/td&gt;
&lt;td&gt;True Negatives&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Common performance metrcs: &lt;strong&gt;False Positive Rate&lt;/strong&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\frac{FP}{N}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;True Positive Rate (sensitivity)&lt;/strong&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\frac{TP}{P}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Precision&lt;/strong&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\frac{TP}{TP+FP}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Accuracy&lt;/strong&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\frac{TP+TN}{P+N}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Specificity&lt;/strong&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\frac{TN}{FP+TN}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Precision (PPV)&lt;/strong&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\frac{TP}{TP+FP} = 1 - FDR\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;False Discovery Rate (FDR)&lt;/strong&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\frac{FP}{FP+TP} = 1 - PPV\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;See more on &lt;a href=&#34;https://en.wikipedia.org/wiki/Confusion_matrix&#34;&gt;wiki&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gain-and-lift-chart&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gain and Lift Chart&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;kolmogorov-smirnov-chart&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Kolmogorov Smirnov Chart&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;auc-roc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;AUC – ROC&lt;/h2&gt;
&lt;p&gt;ROC graphs are two-dimensional graphs in which &lt;strong&gt;True Positive&lt;/strong&gt; rate is plotted on the Y axis and &lt;strong&gt;False Positive&lt;/strong&gt; rate is plotted on the X axis.&lt;br /&gt;
An ROC graph depicts relative tradeoffs between benefits (true positives) and costs (false positives) (fawcett_introduction_2006).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gini-coefficient&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gini Coefficient&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;concordant-discordant-ratio&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Concordant – Discordant Ratio&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;root-mean-squared-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Root Mean Squared Error&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bibliography&lt;/h2&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How to split data into train and test subsets?</title>
      <link>/post/statistics/split_data_ways/split_data_ways/</link>
      <pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/split_data_ways/split_data_ways/</guid>
      <description>


&lt;p&gt;Here you can find several simple approaches to split data into train and test subset to fit and to test parameters of your model. We want to split our data:&lt;br /&gt;
train percent - 0.7&lt;br /&gt;
test percent - 0.3&lt;/p&gt;
&lt;p&gt;Let us create a data frame filled with generated data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df = data.frame(x=seq(.1,1,.1), y=seq(1,10), z=letters[seq(1,10)])
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      x  y z
## 1  0.1  1 a
## 2  0.2  2 b
## 3  0.3  3 c
## 4  0.4  4 d
## 5  0.5  5 e
## 6  0.6  6 f
## 7  0.7  7 g
## 8  0.8  8 h
## 9  0.9  9 i
## 10 1.0 10 j&lt;/code&gt;&lt;/pre&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;First approach is to create a vector filled with selected IDs of rows and then apply this vector to subset data.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1) # set state of random number generator
tv1 = sample(nrow(df), nrow(df)*0.7)
tv1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3 4 5 7 2 8 9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Select train and test data for column y using our vector
train = df$y[tv1]
test = df$y[-tv1] # the same as test = df$y[!tv1]
train&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3 4 5 7 2 8 9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  1  6 10&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Another approch is to create a vecotr filled with logical true/false for each row of dataset and apply this vector to subset data.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1) # set state of random number generator
tv2 = sample(c(TRUE, FALSE), nrow(df), replace = T, prob = c(0.7,0.3))
tv2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Select train and test data for column y using our vector
train = df$y[tv2]
test = df$y[!tv2]
train&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  1  2  3  5  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4 6 7&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Using &lt;a href=&#34;https://cran.r-project.org/web/packages/caTools/index.html&#34;&gt;&lt;code&gt;caTools&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(caTools)
set.seed(1) 
sample = sample.split(df, SplitRatio = .7)
train = subset(df$y, sample == TRUE)
test  = subset(df$y, sample == FALSE)
train&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  1  2  4  5  7  8 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3 6 9&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Using &lt;a href=&#34;https://cran.r-project.org/web/packages/dplyr/&#34;&gt;&lt;code&gt;dplyr&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
df$id &amp;lt;- 1:nrow(df)
train &amp;lt;- df %&amp;gt;% dplyr::sample_frac(.7)
test  &amp;lt;- dplyr::anti_join(df, train, by = &amp;#39;id&amp;#39;)
train$y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10  2  8  7  4  6  1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test$y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3 5 9&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Resampling methods</title>
      <link>/post/statistics/resampling_methods/resampling_methods/</link>
      <pubDate>Sat, 03 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/resampling_methods/resampling_methods/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#validation-approach&#34;&gt;Validation approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#leave-one-out-cross-validation-loocv&#34;&gt;Leave-one-out cross-validation (LOOCV)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#k-fold-cross-validation&#34;&gt;k-fold cross validation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bootstrapping&#34;&gt;Bootstrapping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bibiography&#34;&gt;Bibiography&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;validation-approach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Validation approach&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Validation&lt;/strong&gt; (hold-out) approach estimates the prediction error of our predictive models. This involves randomly dividing the available set of observations into two parts, a &lt;strong&gt;training set&lt;/strong&gt; and a &lt;strong&gt;testing set&lt;/strong&gt; (validation set). Our statistical model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate (typically assessed using &lt;strong&gt;MSE&lt;/strong&gt; in the case of a quantitative response) provides an estimate of the test error rate.&lt;br /&gt;
The drawback of the method is that the &lt;strong&gt;test error rate&lt;/strong&gt; can vary depending on training set.&lt;/p&gt;
&lt;p&gt;Let’s buld &lt;strong&gt;polinomial models&lt;/strong&gt; for first 10 degrees to predict &lt;strong&gt;mpg&lt;/strong&gt; from &lt;strong&gt;horsepower&lt;/strong&gt; dataset (&lt;code&gt;ISLR::Auto&lt;/code&gt;), where &lt;strong&gt;mpg&lt;/strong&gt; - miles per gallon and &lt;strong&gt;horsepower&lt;/strong&gt; - engine horsepower.&lt;br /&gt;
Let’s buld models for train data using &lt;strong&gt;polynomial linear regression&lt;/strong&gt; using vaious polinomial degrees and estimate MSE for each of these models using test data:&lt;br /&gt;
1. &lt;strong&gt;Linear&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\hat{mpg} = \hat{\beta}_0 + \hat{\beta}_1 \cdot horsepower\)&lt;/span&gt;.&lt;br /&gt;
2. &lt;strong&gt;Squared&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\hat{mpg} = \hat{\beta}_0 + \hat{\beta}_1 \cdot horsepower + \hat{\beta}_2 \cdot horsepower^2\)&lt;/span&gt;.&lt;br /&gt;
3. &lt;strong&gt;Cubic&lt;/strong&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\hat{mpg} = \hat{\beta}_0 + \hat{\beta}_1 \cdot horsepower + \hat{\beta}_2 \cdot horsepower^2 + \hat{\beta}_3 \cdot horsepower^3\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;#39;ISLR&amp;#39;)        # datasets Auto
attach(Auto)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
# split data into train (0.6) and test (0.4)
inTrain &amp;lt;- sample(nrow(Auto), nrow(Auto)*0.6)
train &amp;lt;- Auto[inTrain, ]
test &amp;lt;- Auto[!inTrain, ]

# Buld polynomial models for first ten degrees
models &amp;lt;- lapply(1:10, function(n) {
  fit.lm &amp;lt;- lm(mpg ~ poly(horsepower, n), data=train)
  })

mse.vec &amp;lt;- sapply(1:10, function(n) {
  mean((mpg[-inTrain] - predict(models[[n]], Auto[-inTrain, ]))^2)
})
# create dataframe to store MSE of our models
mse.df &amp;lt;- data.frame(degree = 1:10, mse = NA)
mse.df$mse &amp;lt;- mse.vec
plot(mse.df, type=&amp;quot;o&amp;quot;, main=&amp;quot;MSE of spline regression models for degrees from 1 to 10&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/resampling_methods/resampling_methods_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;### PLOT with data and first three polynomial models
par(mar = c(4, 4, 0.5, 1))
plot(horsepower[inTrain], mpg[inTrain],
     xlab = &amp;#39;horsepower&amp;#39;, ylab = &amp;#39;mpg&amp;#39;, pch = 21,
     col = rgb(0, 0, 1, alpha = 0.4), bg = rgb(0, 0, 1, alpha = 0.4))
# add test data
points(horsepower[-inTrain], mpg[-inTrain],
       pch = 21, col = rgb(1, 0, 0, alpha = 0.4), bg = rgb(1, 0, 0, alpha = 0.4))

colors=c(&amp;#39;black&amp;#39;, &amp;#39;blue&amp;#39;, &amp;#39;red&amp;#39;)
x1 &amp;lt;- data.frame(horsepower=seq(min(horsepower), max(horsepower), length = 200))
for (i in 1:3) {
    y2 &amp;lt;- predict(models[[i]], newdata=x1)
    lines(x1$horsepower, y2, col=colors[i], lwd=c(2,2,2))
}
legend(&amp;#39;topright&amp;#39;, lty=c(1,1,1),
       col = c(&amp;#39;black&amp;#39;, &amp;#39;blue&amp;#39;, &amp;#39;red&amp;#39;, rgb(0, 0, 1, alpha = 0.4), bg = rgb(1, 0, 0, alpha = 0.4)),
       legend = c(&amp;#39;Linear model&amp;#39;, &amp;#39;Squared model&amp;#39;, &amp;#39;Cubic model&amp;#39;, &amp;#39;train data&amp;#39;, &amp;#39;test data&amp;#39;),
       lwd=c(2,2,2,NA,NA), pch=c(NA,NA,NA,16,16))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/resampling_methods/resampling_methods_files/figure-html/unnamed-chunk-2-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# end PLOT&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets calculate MSE for these spline regression with dergees from 1 to 10 for 10 different training data subsets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
mse.df.2 &amp;lt;- data.frame(sample = vector(&amp;quot;integer&amp;quot;, 100), 
                       degree = vector(&amp;quot;integer&amp;quot;, 100), 
                       mse = vector(&amp;quot;double&amp;quot;, 100))
counter &amp;lt;- 1

for(i in 1:10) {
  # random sample
  set.seed(i)
  sample &amp;lt;- sample(c(TRUE, FALSE), nrow(Auto), replace = T, prob = c(0.6,0.4))
  train &amp;lt;- Auto[sample, ]
  test &amp;lt;- Auto[!sample, ]
  
  # modeling
  for(j in 1:10) {
    lm.fit &amp;lt;- lm(mpg ~ poly(horsepower, j), data = train)
    
    # add degree &amp;amp; mse values
    mse.df.2[counter, 2] &amp;lt;- j
    mse.df.2[counter, 3] &amp;lt;- mean((test$mpg - predict(lm.fit, test))^2)
    
    # add sample identifier
    mse.df.2[counter, 1] &amp;lt;- i
    counter &amp;lt;- counter + 1
  }
  next
}

ggplot(mse.df.2, aes(degree, mse, color = factor(sample))) +
  geom_line(show.legend = FALSE) +
  geom_point(show.legend = FALSE) +
  ylim(c(10, 30))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/resampling_methods/resampling_methods_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt; All ten curves indicate that there is not much benefit in including cubic or higher-order polynomial terms in the model.&lt;br /&gt;
However, MSE depends on train data subsets.&lt;br /&gt;
The train data contains subset of the observations which can lead to the overestimation of validation set error rate in compair with the test error rate for the model fit on the entire data set.&lt;br /&gt;
&lt;strong&gt;Cross-validation methods&lt;/strong&gt; are used to overcome this drawback.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;leave-one-out-cross-validation-loocv&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Leave-one-out cross-validation (LOOCV)&lt;/h2&gt;
&lt;p&gt;In the LOOCV method a single observation is used for the validation set, and the remaining &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt; observations make up the training set.&lt;br /&gt;
Since the validation observation was not used in the fitting process, the estimate error provides an approximately unbiased estimate for the test error. The validation is repated by subsetting one different observation each time, calculating MSE for this observation each time. The average MSE is calculated in the end. Note: we will use &lt;code&gt;glm&lt;/code&gt; function for linear regression models and the &lt;code&gt;cv.glm&lt;/code&gt; function from &lt;code&gt;boot&lt;/code&gt; package, which provide necessary tools to perform resamping.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# LOOCV
library(&amp;#39;boot&amp;#39;)        # cross-validation
# step 1: fit model
glm.fit &amp;lt;- glm(mpg ~ horsepower, data = Auto)
# setp 2: perform LOOCV across entire data set
loocv.err &amp;lt;- cv.glm(Auto, glm.fit)

str(loocv.err)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## List of 4
##  $ call : language cv.glm(data = Auto, glmfit = glm.fit)
##  $ K    : num 392
##  $ delta: num [1:2] 24.2 24.2
##  $ seed : int [1:626] 403 392 -1703707781 1994959178 434562476 -1277611857 -1105401243 1020654108 526650482 -1538305299 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;call&lt;/strong&gt;: the original function call.&lt;br /&gt;
&lt;strong&gt;K&lt;/strong&gt;: the number of folds used. The LOOCV repeats validation with each of all of 392 observations.&lt;br /&gt;
&lt;strong&gt;delta&lt;/strong&gt;: prediction error (MSE) estimated by the cross-validation.&lt;br /&gt;
&lt;strong&gt;seed&lt;/strong&gt;: the values of the random seed used for the function call&lt;/p&gt;
&lt;p&gt;Let’s compare MSE estimated by &lt;strong&gt;LOOCV&lt;/strong&gt; methods with MSE derived by &lt;strong&gt;simple validation&lt;/strong&gt; approach (0.6/0.4) what we previously used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dplyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# get MSE for our 10 polynomial models, where degree is 1 (linear models). 
mse &amp;lt;- mse.df.2 %&amp;gt;% filter(degree == 1) %&amp;gt;% pull(mse)
mse                 # all MSE from our 10 linear polynoms&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 28.43298 26.38322 27.35615 27.40516 20.31488 24.31761 23.14485
##  [8] 21.92717 25.64242 23.90127&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(mse)           # average MSE from our 10 linear polynoms&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 24.88257&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loocv.err$delta[1]  # LOOCV MSE&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 24.23151&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we can see, MSE derived by LOOCV is 24.2 and it is more precise but very close to the average MSE derived by avereging of our 10 polynomial linear models we made before (~24.9).&lt;/p&gt;
&lt;p&gt;Now we can apply LOOCV to estimate MSE for all 10 degrees of our polynomial regression models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# vector of LOOCV-errors
n &amp;lt;- 10 # max degree of polynom model
cv.err.loocv &amp;lt;- rep(NA, n)
names(cv.err.loocv) &amp;lt;- 1:n
# repeat by powers of polynomes
for (i in 1:n){
  fit.glm &amp;lt;- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.err.loocv[i] &amp;lt;- cv.glm(Auto, fit.glm)$delta[1]
}
# result
cv.err.loocv&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        1        2        3        4        5        6        7        8 
## 24.23151 19.24821 19.33498 19.42443 19.03321 18.97864 18.83305 18.96115 
##        9       10 
## 19.06863 19.49093&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Result is basically the same for this model as we got before but this result is more precise.&lt;br /&gt;
This LOOCV approach can be used with any kind of predictive modeling.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;k-fold-cross-validation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;k-fold cross validation&lt;/h2&gt;
&lt;p&gt;library(‘GGally’) # matrix diagrams K-times cross-validation is a compromize between sample validation and LOOCV. It is computationally more effective than LOOCV but not so presize.&lt;br /&gt;
We will make 10-time validation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- 10 # max degree (number of polynom models)
cv.err.k.fold &amp;lt;- rep(NA, 10)
names(cv.err.k.fold) &amp;lt;- 1:10
# repeat for power of polynomes
for (i in 1:n){
  fit.glm &amp;lt;- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.err.k.fold[i] &amp;lt;- cv.glm(Auto, fit.glm, K = 10)$delta[1]
}
# result
cv.err.k.fold&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        1        2        3        4        5        6        7        8 
## 24.10371 19.34487 19.33756 19.71388 18.86019 19.02235 18.90377 19.18771 
##        9       10 
## 19.29985 19.08448&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;k-fold cross validation&lt;/strong&gt; computationally more effective than &lt;strong&gt;LOOCV&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bootstrapping&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bootstrapping&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Bootstrapping&lt;/strong&gt; repeatedly draws independent samples from our data set to create bootstrap data sets. This sample is performed with replacement, which means that the same observation can be sampled more than once.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n &amp;lt;- nrow(Auto)
statistic &amp;lt;- function(data, index) {
  # extract model coefficients
  coef(lm(mpg ~ horsepower, data = data, subset = index))
}
statistic(Auto, 1:392)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept)  horsepower 
##  39.9358610  -0.1578447&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
boot(Auto, statistic, 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = Auto, statistic = statistic, R = 1000)
## 
## 
## Bootstrap Statistics :
##       original        bias    std. error
## t1* 39.9358610  0.0269563085 0.859851825
## t2* -0.1578447 -0.0002906457 0.007402954&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bootstrap estimate for SE0 is 0.86, and that the bootstrap estimate for SE1is 0.0076. If we compare these to the standard errors provided by the summary function we see a difference (biased error).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(mpg ~ horsepower, data = Auto))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ horsepower, data = Auto)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -13.5710  -3.2592  -0.3435   2.7630  16.9240 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 39.935861   0.717499   55.66   &amp;lt;2e-16 ***
## horsepower  -0.157845   0.006446  -24.49   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 4.906 on 390 degrees of freedom
## Multiple R-squared:  0.6059, Adjusted R-squared:  0.6049 
## F-statistic: 599.7 on 1 and 390 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we compair errors for polynomial model (degree=2) by bootstrap and non-bootstrap approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;statistic.2 &amp;lt;- function(data, index) {
  coef(lm(mpg ~ poly(horsepower, 2), data = data, subset = index))
}
set.seed(1)
boot(Auto, statistic.2, 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = Auto, statistic = statistic.2, R = 1000)
## 
## 
## Bootstrap Statistics :
##       original      bias    std. error
## t1*   23.44592 0.003943212   0.2255528
## t2* -120.13774 0.117312678   3.7008952
## t3*   44.08953 0.047449584   4.3294215&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(mpg ~ poly(horsepower, 2), data = Auto))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ poly(horsepower, 2), data = Auto)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.7135  -2.5943  -0.0859   2.2868  15.8961 
## 
## Coefficients:
##                       Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)            23.4459     0.2209  106.13   &amp;lt;2e-16 ***
## poly(horsepower, 2)1 -120.1377     4.3739  -27.47   &amp;lt;2e-16 ***
## poly(horsepower, 2)2   44.0895     4.3739   10.08   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 4.374 on 389 degrees of freedom
## Multiple R-squared:  0.6876, Adjusted R-squared:  0.686 
## F-statistic:   428 on 2 and 389 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Better correspondence between the bootstrap estimates and the standard estimates suggest a better model fit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bibiography&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bibiography&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://faculty.marshall.usc.edu/gareth-james/&#34;&gt;An Introduction to Statistical Learning by Gareth James&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://afit-r.github.io&#34;&gt;Air Forse Institute of Technology&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Hopfield Neural Network</title>
      <link>/post/statistics/hopfield_network/hopfield_network/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/hopfield_network/hopfield_network/</guid>
      <description>


&lt;p&gt;Here is an example of python code.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np

# Patterns
x1 = [1, -1, -1, -1, 1, -1, -1, -1, 1]
x2 = [-1, -1, 1, -1, 1, -1, 1, -1, -1]

L = []
L.append(x1)
L.append(x2)

# Function to find weight from list of vectors (lists)
def wts(l):
    # length of the first element in input list
    # Would be great to check that all elements have the same length
    ln = len(l[1])
    # Weight matrix filled with zeros
    W = np.zeros((ln, ln))
    # add x @ x.T for all vectors in input list
    for i in range(len(l)):
        L[i] = np.reshape(np.array(L[i]), (ln, 1))
        W = W + L[i] @ L[i].T
    # fill main diagonal with zeros
    np.fill_diagonal(W, 0)
    return W

W = wts(L)

### TEST
# test vector
t = (-1, -1, 1, -1, 1, -1, -1, 1, -1)

def wvec(vec, W):
    vec = np.reshape(np.array(vec), (len(vec), 1))
    vec = W @ vec
    vec = np.ndarray.round(np.tanh(vec))
    return vec

vec = wvec(t, W)

for i in L:
    if np.array_equal(vec, i):
        print(&amp;quot;Pattern detected&amp;quot;, i)
    else:
        print(&amp;quot;Pattern not detected&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Pattern not detected
## Pattern detected [[-1]
##  [-1]
##  [ 1]
##  [-1]
##  [ 1]
##  [-1]
##  [ 1]
##  [-1]
##  [-1]]&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ANOVA</title>
      <link>/post/statistics/anova/anova/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/anova/anova/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#one-way-anova&#34;&gt;One-way ANOVA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sources&#34;&gt;Sources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;one-way-anova&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;One-way ANOVA&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;variance&lt;/strong&gt; = SS/df, where SS - sum of squares and df - degree of freedom&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(SS = \displaystyle\sum_{i=1}^{n}{(x_i - \mu)^2}\)&lt;/span&gt;, where&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the sample &lt;strong&gt;mean&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;n&lt;/strong&gt; is the sample &lt;strong&gt;size&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(var(x) = \frac{1}{n}{\displaystyle\sum_{i=1}^{n}{(x_i - \mu)^2}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;3 groups of students with scores (1-100):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a = c(82,93,61,74,69,70,53)
b = c(71,62,85,94,78,66,71)
c = c(64,73,87,91,56,78,87)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;SST = SSE + SSC = W + B, where&lt;br /&gt;
SST - Total Sum of Squares&lt;br /&gt;
SSE - Error Sum of Squares - within (W)&lt;br /&gt;
SSC - Sum of Squares Columns (treatmens) - between (B)&lt;/p&gt;
&lt;p&gt;C - columns (treatments)&lt;br /&gt;
N - total number of observations&lt;/p&gt;
&lt;p&gt;Mean squared of columns - MSC = SSC/df_columns, where df_columns = C-1&lt;br /&gt;
Mean squared of error - MSE = SSE/df_error, where df_error = N-C&lt;br /&gt;
Sum of squares (total) - SST, where df_total = N-1 F-statistics - F = MSC/MSE&lt;/p&gt;
&lt;p&gt;Let’s calculate degree of freedom for our example:&lt;br /&gt;
df_columns = 3-1 = 2, MSC = SSC/2&lt;br /&gt;
df_error = 21-3 = 18, MSE = SSE/18&lt;br /&gt;
df_total = 21-1 = 20&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a = c(82,93,61,74,69,70,53)
b = c(71,62,85,94,78,66,71)
c = c(64,73,87,91,56,78,87)
sq = function(x) { sum((x - mean(x))^2) }
sq(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1039.429&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sq(b)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 751.4286&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sq(c)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1021.714&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using R packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data
# Number of calories consumed by month:
may &amp;lt;- c(2166, 1568, 2233, 1882, 2019)
sep &amp;lt;- c(2279, 2075, 2131, 2009, 1793)
dec &amp;lt;- c(2226, 2154, 2583, 2010, 2190)

d &amp;lt;- stack(list(may=may, sep=sep, dec=dec))
d&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    values ind
## 1    2166 may
## 2    1568 may
## 3    2233 may
## 4    1882 may
## 5    2019 may
## 6    2279 sep
## 7    2075 sep
## 8    2131 sep
## 9    2009 sep
## 10   1793 sep
## 11   2226 dec
## 12   2154 dec
## 13   2583 dec
## 14   2010 dec
## 15   2190 dec&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;values&amp;quot; &amp;quot;ind&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;oneway.test(values ~ ind, data=d, var.equal=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  One-way analysis of means
## 
## data:  values and ind
## F = 1.7862, num df = 2, denom df = 12, p-value = 0.2094&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# alternative using aov
res &amp;lt;- aov(values ~ ind, data = d)
res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Call:
##    aov(formula = values ~ ind, data = d)
## 
## Terms:
##                      ind Residuals
## Sum of Squares  174664.1  586719.6
## Deg. of Freedom        2        12
## 
## Residual standard error: 221.1183
## Estimated effects may be unbalanced&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(res)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             Df Sum Sq Mean Sq F value Pr(&amp;gt;F)
## ind          2 174664   87332   1.786  0.209
## Residuals   12 586720   48893&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sources&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sources&lt;/h1&gt;
&lt;p&gt;Example for one-way ANOVA: &lt;a href=&#34;https://www.youtube.com/playlist?list=PLIeGtxpvyG-KA-BLkL391X__r0kU4_hm5&#34;&gt;youtube&lt;/a&gt; by Brandon Foltz&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Heart rate zones</title>
      <link>/post/sport/heart_rate_zones/heart_rate_zones/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/sport/heart_rate_zones/heart_rate_zones/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-heart-zones&#34;&gt;What is heart zones&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#calculator&#34;&gt;Calculator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#does-fat-burning-zone-exists&#34;&gt;Does fat burning zone exists?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bibliography&#34;&gt;Bibliography&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;what-is-heart-zones&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What is heart zones&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;calculator&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calculator&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;does-fat-burning-zone-exists&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Does fat burning zone exists?&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bibliography&lt;/h3&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Polycomb and Trithorax group proteins</title>
      <link>/post/biology/pcg_trx/pcg_trx/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/biology/pcg_trx/pcg_trx/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#polycomb-group-pcg-proteins&#34;&gt;Polycomb group PcG proteins&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#prc1&#34;&gt;PRC1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#prc2&#34;&gt;PRC2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pho-complex&#34;&gt;Pho-complex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mechanisms-of-inactivation-of-gene-expression&#34;&gt;Mechanisms of inactivation of gene expression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trxg-complex-components&#34;&gt;TrxG complex components&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pcgtrxg-response-elements&#34;&gt;PcG/TrxG response elements&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nuclear-architecture&#34;&gt;Nuclear architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#open-questions&#34;&gt;Open questions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#glossary&#34;&gt;Glossary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cited-literature&#34;&gt;Cited literature&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;polycomb-group-pcg-proteins&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Polycomb group PcG proteins&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Polycomb group&lt;/strong&gt; (PcG) proteins generally maintain gene repression, whereas &lt;strong&gt;Trithorax group&lt;/strong&gt; (TrxG) proteins maintain the active gene expression.&lt;/p&gt;
&lt;p&gt;PcG and TrxG proteins were initially isolated in &lt;em&gt;Drosophila&lt;/em&gt; as factors involved in maintaining the expression patterns of HOX genes, which encode transcription factors that are important determinants of patterning during embryonic development.&lt;/p&gt;
&lt;p&gt;In mammals, PRCs are targeted to a subset of CpG islands (CGIs) at the promoters of developmental genes and prevent unscheduled cellular differentiation.&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;drosophila&lt;/em&gt; PcG proteins form &lt;em&gt;Polycomb repressive complex 1&lt;/em&gt; (&lt;strong&gt;PRC1&lt;/strong&gt;), &lt;em&gt;Polycomb repressive complex 2&lt;/em&gt; (&lt;strong&gt;PRC2&lt;/strong&gt;) and &lt;em&gt;Pleiohomeotic&lt;/em&gt; (Pho) &lt;em&gt;repressive complex&lt;/em&gt; (&lt;strong&gt;PhoRC&lt;/strong&gt;). PRC2 and PRC1 are recruited to chromatin by PhoRC which directly binds &lt;em&gt;polycomb response elements&lt;/em&gt; (&lt;strong&gt;PREs&lt;/strong&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prc1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PRC1&lt;/h2&gt;
&lt;p&gt;PRC1 ubiquitinylates histone H2AK119 (H2AK118 in Drosophila) and alter chromatin structure whereas PRC2 trimethylates histone H3K27.&lt;/p&gt;
&lt;p&gt;PRC1 is composed of the core components &lt;strong&gt;Polycomb&lt;/strong&gt; (Pc), &lt;strong&gt;Polyhomeotic&lt;/strong&gt; (Ph), &lt;strong&gt;Posterior sex combs&lt;/strong&gt; (Psc) and &lt;strong&gt;Sex combs extra&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Pc can bind the H3K27me3 that facilitate anchoring the complex to chromatin.&lt;/p&gt;
&lt;p&gt;Psc and Sce form a heterodimer, which enhances the E3 ubiquitin ligase activity of the complex. This activity is provided by the Sce subunit of&lt;/p&gt;
&lt;p&gt;PRC1, which monoubiquitylates H2AK118 (K119 in mammals). This ubiquitylation event is thought to restrict RNA polymerase II (Pol II) elongation, but was also shown to recruit PRC2 members.&lt;/p&gt;
&lt;p&gt;Ph subunit is able to bind itself that promotes spreading of PRC1 complex.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prc2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PRC2&lt;/h2&gt;
&lt;p&gt;PRC2 core complex is composed of &lt;strong&gt;Enhancer of zeste&lt;/strong&gt; E(z), &lt;strong&gt;Suppressor of zeste 12&lt;/strong&gt; Su(z)12, &lt;strong&gt;Extra sex combs&lt;/strong&gt; (Esc) and &lt;strong&gt;p55&lt;/strong&gt; (Nurf55 or Caf1).&lt;/p&gt;
&lt;p&gt;E(z) methylate H3K27.&lt;/p&gt;
&lt;p&gt;Esc bind H3K27me3 and facilitates multimerization of complex.&lt;/p&gt;
&lt;p&gt;PRC1-mediated events are also thought to compact chromatin to limit the access of activating factors and the Psc subunit in particular has been linked to this function.&lt;/p&gt;
&lt;p&gt;p55 is present in a number of chromatin remodeling complexes and interacts with Su(z)12, H3 and H4. The loss of loss of p55 appears to have little consequence on PRC2 activity.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pho-complex&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pho-complex&lt;/h2&gt;
&lt;p&gt;The first complex, referred to as Pho repressive complex (PhoRC), is composed of Pho and Sfmbt. A second Pho-containing complex has also been described (Pho-INO80) that, in addition to Pho, contains the INO80 nucleosome remodeling complex (Klymenko, 2006). Pho binds DNA in a sequence-specific manner and help to recruit PcG complexes to their response elements (Grossniklaus and Paro, 2014).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mechanisms-of-inactivation-of-gene-expression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mechanisms of inactivation of gene expression&lt;/h2&gt;
&lt;p&gt;After the initial recruitment of PRC2 and PRC1 by Pho protein, Enhancer of zeste &lt;strong&gt;E(z)&lt;/strong&gt;, a member of PRC2, methylate H3 histone (H3K27me3) at both PREs and along the gene body. This modification is then recognized by Pc, a member of PRC1, which, in turn, ubiquitylates H2A119 via Sex combs extra (&lt;strong&gt;Sce&lt;/strong&gt;), another PRC1 member, and stabilizes PRC2. The accumulation of PRC1 and PRC2 within gene bodies results in the compaction of local nucleosomes and the further silencing of the inactive genes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trxg-complex-components&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TrxG complex components&lt;/h2&gt;
&lt;p&gt;TrxG include the COMPASS, COMPASS-like, TAC1 and ASH1 complexes, and SET domain HMTs.&lt;/p&gt;
&lt;p&gt;Common subunits for COMPASS and COMPASS-like complexes include Ash2, Dpy30 (Dpy-30L1), Hcf1 (Hcf), Rbbp5 and Wds.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pcgtrxg-response-elements&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PcG/TrxG response elements&lt;/h2&gt;
&lt;p&gt;In Drosophila, the genomic nucleation sites of PcG- and TrxG-mediated epigenetic memory have been referred to as PcG/TrxG response elements (PRE/TREs).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;nuclear-architecture&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nuclear architecture&lt;/h2&gt;
&lt;p&gt;Nuclear architecture by PRC1 complex is disscussed in the recent review &lt;span class=&#34;citation&#34;&gt;(Illingworth 2019)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;open-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Open questions&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Although some PRE/TRE-like elements have been identified, it is unclear if all of the properties of the Drosophila PRE/TRE (e.g. epigenetic memory) are maintained in the mammalian system.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;How PcG and TrxG proteins are recruited to these elements?&lt;br /&gt;
We currently lack a clear understanding of the hierarchical recruitment of PcG and TrxG proteins to PRE/TREs, and elucidating these recruitment mechanisms is thus an area of active research.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What determines the active (TRE) versus repressed (PRE) state?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What factors d which PRC1 targets will physically interact?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What is the impact of stoichiometry of PRC1 subunits during development?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This page summarizes several recent reviews &lt;span class=&#34;citation&#34;&gt;(Geisler and Paro 2015)&lt;/span&gt; to aggregate information in this actively studied area of research.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;glossary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Glossary&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;PRC1&lt;/strong&gt; - Polycomb Repressive Complex 1&lt;br /&gt;
&lt;strong&gt;PRC2&lt;/strong&gt; - Polycomb Repressive Complex 2&lt;br /&gt;
&lt;strong&gt;Pho&lt;/strong&gt; - Pleiohomeotic&lt;br /&gt;
&lt;strong&gt;PhoRC&lt;/strong&gt; - Pleiohomeotic Repressive Complex&lt;br /&gt;
&lt;strong&gt;PREs&lt;/strong&gt; - Polycomb Response Elements&lt;br /&gt;
&lt;strong&gt;Psc&lt;/strong&gt; - Posterior Sex Combs&lt;br /&gt;
&lt;strong&gt;E(z)&lt;/strong&gt; - Enhancer of zeste&lt;br /&gt;
&lt;strong&gt;Su(z)12&lt;/strong&gt; - Suppressor of zeste 12&lt;br /&gt;
&lt;strong&gt;Esc&lt;/strong&gt; - Extra sex combs&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cited-literature&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;Cited literature&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-geisler_trithorax_2015&#34;&gt;
&lt;p&gt;Geisler, Sarah J., and Renato Paro. 2015. “Trithorax and Polycomb Group-Dependent Regulation: A Tale of Opposing Activities.” &lt;em&gt;Development&lt;/em&gt; 142 (17): 2876–87. doi:&lt;a href=&#34;https://doi.org/10.1242/dev.120030&#34;&gt;10.1242/dev.120030&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-illingworth_chromatin_2019&#34;&gt;
&lt;p&gt;Illingworth, Robert S. 2019. “Chromatin Folding and Nuclear Architecture: PRC1 Function in 3d.” &lt;em&gt;Current Opinion in Genetics &amp;amp; Development&lt;/em&gt; 55 (July): 82–90. doi:&lt;a href=&#34;https://doi.org/10.1016/j.gde.2019.06.006&#34;&gt;10.1016/j.gde.2019.06.006&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R tips and coding tricks</title>
      <link>/post/programming/r_tips_tricks/r_tips_tricks/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/programming/r_tips_tricks/r_tips_tricks/</guid>
      <description>


&lt;p&gt;Here you can find some simple interesting concepts for coding in R.&lt;/p&gt;
&lt;div id=&#34;how-to-select-all-numeric-columns-in-a-data-frame&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to select all numeric columns in a data frame?&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df = data.frame(x = c(1:3),
                y = c(&amp;#39;A&amp;#39;,&amp;#39;B&amp;#39;,&amp;#39;C&amp;#39;),
                z = c(0.1, 0.2, 0.3))
df&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   x y   z
## 1 1 A 0.1
## 2 2 B 0.2
## 3 3 C 0.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Select all numeric columns
df[sapply(df,is.numeric)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   x   z
## 1 1 0.1
## 2 2 0.2
## 3 3 0.3&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simple Markov process</title>
      <link>/post/statistics/markov_process/simple_markov_process/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/markov_process/simple_markov_process/</guid>
      <description>


&lt;p&gt;Here, we will consider a simple example of Markov process with implementation in R.&lt;br /&gt;
The following example is taken from &lt;a href=&#34;http://www.bodowinter.com&#34;&gt;Bodo Winter website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;Markov process&lt;/strong&gt; is characterized by (1) &lt;strong&gt;a finite set of states&lt;/strong&gt; and (2) &lt;strong&gt;fixed transition probabilities between the states&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let’s consider an example. Assume you have a classroom, with students who could be either in the state &lt;strong&gt;alert&lt;/strong&gt; or in the state &lt;strong&gt;bored&lt;/strong&gt;. And then, at any given time point, there’s a certain probability of an alert student becoming bored (say 0.2), and there’s a probability of a bored student becoming alert (say 0.25).&lt;/p&gt;
&lt;p&gt;Let’s say there are 20 alert and 80 bored students in a particular class. This is your initial condition at time point &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Given the transition probabilities above, what’s the number of alert and bored students at the next point in time, &lt;span class=&#34;math inline&#34;&gt;\(t+1\)&lt;/span&gt;?&lt;br /&gt;
Multiply 20 by 0.2 (=4) and these will be the alert students that turn bored.&lt;br /&gt;
And then multiply 80 by 0.25 (=20) and these will be the bored students that turn alert.&lt;br /&gt;
So, at &lt;span class=&#34;math inline&#34;&gt;\(t+1\)&lt;/span&gt;, there’s going to be 20-4+20 alert students. And there’s going to be 80+4-20 bored students. Before, 80% of the students were bored and now, only 64% of the students are bored. Conversely, 36% are alert.&lt;/p&gt;
&lt;p&gt;A handy way of representing this Markov process is by defining a transition probability matrix:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;A&lt;/th&gt;
&lt;th&gt;B&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;A&lt;span class=&#34;math inline&#34;&gt;\(_{t+1}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;0.8&lt;/td&gt;
&lt;td&gt;0.25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;B&lt;span class=&#34;math inline&#34;&gt;\(_{t+1}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;0.75&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;What this matrix says is: A proportion of 0.8 of the people who are in state A (alert) will also be at state A at time point &lt;span class=&#34;math inline&#34;&gt;\(t+1\)&lt;/span&gt;. And, a proportion of 0.25 of the people who are in state B (bored) will switch to alert at t+1. This is what the first row says. The next row is simply one minus the probabilities of the first row, because probabilities (or proportions) have to add up to 1. Now think about multiplying this matrix with the initial proportions of alert and bored students that we had above. 0.8 are bored and 0.2 are alert. In linear algebra this would look the following way:&lt;/p&gt;
&lt;div&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{bmatrix}
 0.8 &amp;amp; 0.25 \\
 0.2 &amp;amp; 0.75
\end{bmatrix}\times\begin{bmatrix}
 0.2 \\
 0.8
\end{bmatrix} = \begin{bmatrix}
 0.8\times0.2 + 0.25\times0.8 \\
 0.2\times0.2 + 0.75\times0.8
\end{bmatrix} = \begin{bmatrix}
0.36 \\
0.64
\end{bmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The results of these calculations are exactly the proportions that we saw above: 36% alert student and 64% bored students.&lt;/p&gt;
&lt;p&gt;Now, you might ask yourself: What happens if this process continues? What happens at &lt;span class=&#34;math inline&#34;&gt;\(t+2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(t+3\)&lt;/span&gt; etc.? Will it be the case that at one point there are no bored students any more? Let’s simulate this in R and find out! Let’s call this &lt;strong&gt;tpm&lt;/strong&gt; for &lt;strong&gt;transition probability matrix&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tpm = matrix(c(0.8,0.25, 0.2,0.75), nrow=2, byrow=TRUE)
colnames(tpm) = c(&amp;#39;A&amp;#39;,&amp;#39;B&amp;#39;)
rownames(tpm) = c(&amp;#39;At+1&amp;#39;, &amp;#39;Bt+1&amp;#39;)
tpm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        A    B
## At+1 0.8 0.25
## Bt+1 0.2 0.75&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again this matrix shows that 0.8 students who were in state A at time point t will still be in state A at &lt;span class=&#34;math inline&#34;&gt;\(t+1\)&lt;/span&gt;. And 0.25 students who were in state B at time point t will be in state A at &lt;span class=&#34;math inline&#34;&gt;\(t+1\)&lt;/span&gt;. The second row has a similar interpretation for alert and bored students becoming bored at &lt;span class=&#34;math inline&#34;&gt;\(t+1\)&lt;/span&gt;. Remember that Markov processes assume fixed transition probabilities. This means that in the simulation that we’ll be doing, we leave the transition probability matrix unchanged. However, we will define a vector of the actual proportions – and these are allowed to change. In time, we expect more and more students to become alert, because the transition probability from B to A (which, to remind you, was 0.25) is higher than from A to B (which was 0.2).&lt;/p&gt;
&lt;p&gt;Let’s start our simulation by setting the initial condition as 0.1 students are alert and 0.9 students are bored and define a matrix called &lt;strong&gt;sm&lt;/strong&gt; (short for &lt;strong&gt;student matrix&lt;/strong&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sm = as.matrix(c(0.1, 0.9))
rownames(sm)= c(&amp;#39;A&amp;#39;, &amp;#39;B&amp;#39;)
sm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [,1]
## A  0.1
## B  0.9&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s repeat by looping:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in 1:10){
    sm = tpm %*% sm
    }&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we’re looping 10 times and on each iteration, we multiply the matrix &lt;strong&gt;tpm&lt;/strong&gt; with the student matrix &lt;strong&gt;sm&lt;/strong&gt;. We take this result and store it in &lt;strong&gt;sm&lt;/strong&gt;. This means that at the next iteration, our fixed &lt;strong&gt;transition probability matrix&lt;/strong&gt; will be multiplied by a different student matrix, allowing for the proportions to slowly change over time.&lt;br /&gt;
R operator ’%*%’ is used for matrix multiplication&lt;/p&gt;
&lt;p&gt;Outcome of our ten loop iterations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]
## At+1 0.5544017
## Bt+1 0.4455983&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, after 10 iterations of the Markov process, we now have about 55% alert students and 45% bored ones. What is interesting to me is that even though 80% of the people who are alert at one time point remain alert at the next time point, the process only converged on 55% alert and 45% bored after 10 iterations.&lt;/p&gt;
&lt;p&gt;Let’s reset our initial condition to (0.1 alert and 0.9 bored students) and run a thousand iterations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in 1:1000){
    sm = tpm %*% sm
    }
sm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           [,1]
## At+1 0.5555556
## Bt+1 0.4444444&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A 1000 iterations, and we seem to be zoning in onto ~55% and ~44%. This phenomenon is called &lt;strong&gt;Markov convergence&lt;/strong&gt;. You could run even more iterations, and your outcome would get closer and closer to 0.5555 (to infinity). So, the model converges on an equilibrium. However, this is not a fixed equilibrium. It’s not the case that the Markov process comes to a hold or that nobody changes states between alertness and boredness any more. The equilibrium that we’re dealing with here is a statistical equilibrium, where the proportions of alert and bored students remain the same. but there still is constant change (at each time step, 0.2 alert students become bored and 0.25 bored students become alert). Markov models always converge to a statistical equilibrium if the conditions (1) and (2) above are met, and if you can get from any state within your Markov model to any other state (in the case of just two states, that clearly is the case). What’s so cool about this is that it is, at first sight, fairly counterintuitive.&lt;/p&gt;
&lt;p&gt;At least when I thought about the transition probabilities for the first time, I somehow expected all students to become alert but as we saw, that’s not the case. Moreover, this process is not sensitive to initial conditions. That means that when you start with any proportion of alert or bored students (even extreme ones such as 0.0001 alert students), the process will reach the statistical equilibrium – albeit sometimes a little faster or slower. You can play around with different values for the &lt;strong&gt;sm&lt;/strong&gt; object to explore this property of Markov convergence. Another interesting thing is that the process is impervious to intervention: Say, you introduced something that made more students alert – the Markov model would quickly get back to equilibrium. So Markov processes are essentially ahistorical processes: history doesn’t matter. Even with extreme initial conditions or extreme interventions, the process quickly converges to the equilibrium defined by the transition probabilities. The only way to persistently change the system is to change the transition probabilities. Finally, what I find so cool about Markov processes is their computational simplicity.&lt;/p&gt;
&lt;div id=&#34;sources&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sources&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.bodowinter.com&#34;&gt;Bodo Winter website&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Spline model</title>
      <link>/post/statistics/splines/splines/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/statistics/splines/splines/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#generate-dataset-from-a-given-function&#34;&gt;Generate dataset from a given function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#split-data-for-train-and-test&#34;&gt;Split data for train and test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#diagram-of-the-given-function-and-generated-datasets&#34;&gt;Diagram of the given function and generated datasets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#build-a-model-using-splines&#34;&gt;Build a model using splines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#diagram-of-mse-for-train-and-test-data&#34;&gt;Diagram of MSE for train and test data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#build-optimal-model-and-plot-for-the-model&#34;&gt;Build optimal model and plot for the model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bibliograpy&#34;&gt;Bibliograpy&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;In this example we will generate data from a given function and then build a model using splines and estimate quality of the model.&lt;/p&gt;
&lt;div id=&#34;generate-dataset-from-a-given-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generate dataset from a given function&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# parameters to generate a dataset
n.all &amp;lt;- 100             # number of observations
train.percent &amp;lt;- 0.85    # portion of the data for training
res.sd &amp;lt;- 1              # standard deviation of noise
x.min &amp;lt;- 5               # min limit of the data
x.max &amp;lt;- 105             # max limit of the data

# generate x
set.seed(1)       # to get reproducible results by randomizer
x &amp;lt;- runif(x.min, x.max, n = n.all)

# noise from normal destibution
set.seed(1)
res &amp;lt;- rnorm(mean = 0, sd = res.sd, n = n.all)

# generate y using a given function
y.func &amp;lt;- function(x) {4 - 2e-02*x + 5.5e-03*x^2 - 4.9e-05*x^3}

# add noise
y &amp;lt;- y.func(x) + res&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;split-data-for-train-and-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Split data for train and test&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# split dataset for training and test
set.seed(1)
# generate vector of chosen x for train data
inTrain &amp;lt;- sample(seq_along(x), size = train.percent*n.all)

# train data set
x.train &amp;lt;- x[inTrain]
y.train &amp;lt;- y[inTrain]

# test data set
x.test &amp;lt;- x[-inTrain]
y.test &amp;lt;- y[-inTrain]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;diagram-of-the-given-function-and-generated-datasets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diagram of the given function and generated datasets&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# lines of generated data for plot
x.line &amp;lt;- seq(x.min, x.max, length = n.all)
y.line &amp;lt;- y.func(x.line)

# PLOT
# generate plot by train data
par(mar = c(4, 4, 1, 1)) # reduce margins (optional)
plot(x.train, y.train,
     main = &amp;#39;Generated data and original function&amp;#39;,
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = &amp;#39;X&amp;#39;, ylab = &amp;#39;Y&amp;#39;, 
     xlim = c(x.min, x.max),
     ylim = c(min(y), max(y)), 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# add points of test data
points(x.test, y.test, col = &amp;#39;red&amp;#39;, bg = &amp;#39;red&amp;#39;, pch = 21)

# add the given function
lines(x.line, y.line, lwd = 2, lty = 2)

# add legend
legend(&amp;#39;topleft&amp;#39;, legend = c(&amp;#39;train&amp;#39;, &amp;#39;test&amp;#39;, &amp;#39;f(X)&amp;#39;),
       pch = c(16, 16, NA), 
       col = c(grey(0.2), &amp;#39;red&amp;#39;, &amp;#39;black&amp;#39;),  
       lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/splines/splines_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build-a-model-using-splines&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build a model using splines&lt;/h2&gt;
&lt;p&gt;We will compair sevaral models with degree of freedoms (df) from 2 to 40, where 2 correspond to a linear model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;max.df &amp;lt;- 40                       # max degree of freedom (df)
# 
tbl &amp;lt;- data.frame(df = 2:max.df)   # data frame for writing errors
tbl$MSE.train &amp;lt;- 0                 # column 1: errors of train data
tbl$MSE.test &amp;lt;- 0                  # сcolumn 2: errors of test data

# generate models using for cycle
for (i in 2:max.df) {
    mod &amp;lt;- smooth.spline(x = x.train, y = y.train, df = i)
    
    # predicted values for train and test data using built model
    y.model.train &amp;lt;- predict(mod, data.frame(x = x.train))$y[, 1]
    y.model.test &amp;lt;- predict(mod, data.frame(x = x.test))$y[, 1]
    
    # MSE errors for train and test data
    MSE &amp;lt;- c(sum((y.train - y.model.train)^2) / length(x.train),
             sum((y.test - y.model.test)^2) / length(x.test))
    
    # write errors to the previously created data frame
    tbl[tbl$df == i, c(&amp;#39;MSE.train&amp;#39;, &amp;#39;MSE.test&amp;#39;)] &amp;lt;- MSE
}

# view first rows of the table
head(tbl, 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   df MSE.train MSE.test
## 1  2 3.7188566 2.885166
## 2  3 1.4463925 1.635813
## 3  4 0.8938817 1.239533
## 4  5 0.7668250 1.038918&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;diagram-of-mse-for-train-and-test-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diagram of MSE for train and test data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot MSE from our table
plot(x = tbl$df, y = tbl$MSE.test,
     main = &amp;quot;Changes of MSE from degrees of freedom&amp;quot;,
     type = &amp;#39;l&amp;#39;, col = &amp;#39;red&amp;#39;, lwd = 2,
     xlab = &amp;#39;spline degree of freedom&amp;#39;, ylab = &amp;#39;MSE&amp;#39;,
     ylim = c(min(tbl$MSE.train, tbl$MSE.test), 
              max(tbl$MSE.train, tbl$MSE.test)),
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# add 
points(x = tbl$df, y = tbl$MSE.test,
       pch = 21, col = &amp;#39;red&amp;#39;, bg = &amp;#39;red&amp;#39;)
lines(x = tbl$df, y = tbl$MSE.train, col = grey(0.3), lwd = 2)
# minimal MSE
abline(h = res.sd, lty = 2, col = grey(0.4), lwd = 2)

# add legend
legend(&amp;#39;topright&amp;#39;, legend = c(&amp;#39;train&amp;#39;, &amp;#39;test&amp;#39;),
       pch = c(NA, 16), 
       col = c(grey(0.2), &amp;#39;red&amp;#39;),  
       lty = c(1, 1), lwd = c(2, 2), cex = 1.2)

# df of minimal MSE for test data
min.MSE.test &amp;lt;- min(tbl$MSE.test)
df.min.MSE.test &amp;lt;- tbl[tbl$MSE.test == min.MSE.test, &amp;#39;df&amp;#39;]

# optimal df for precise model and maximal simplicity
df.my.MSE.test &amp;lt;- 6
my.MSE.test &amp;lt;- tbl[tbl$df == df.my.MSE.test, &amp;#39;MSE.test&amp;#39;]

# show the optimal solution
abline(v = df.my.MSE.test, 
       lty = 2, lwd = 2)
points(x = df.my.MSE.test, y = my.MSE.test, 
       pch = 15, col = &amp;#39;blue&amp;#39;)
mtext(df.my.MSE.test, 
      side = 1, line = -1, at = df.my.MSE.test, col = &amp;#39;blue&amp;#39;, cex = 1.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/splines/splines_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build-optimal-model-and-plot-for-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build optimal model and plot for the model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.MSE.test &amp;lt;- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test)

# predict data for 250 x&amp;#39;s to get smoothed curve
x.model.plot &amp;lt;- seq(x.min, x.max, length = 250)
y.model.plot &amp;lt;- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1]

# plot train data
par(mar = c(4, 4, 1, 1))
plot(x.train, y.train,
     main = &amp;quot;Initial data and the best fit model&amp;quot;,
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = &amp;#39;X&amp;#39;, ylab = &amp;#39;Y&amp;#39;, 
     xlim = c(x.min, x.max),
     ylim = c(min(y), max(y)), 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# add test data
points(x.test, y.test, col = &amp;#39;red&amp;#39;, bg = &amp;#39;red&amp;#39;, pch = 21)

# function
lines(x.line, y.line,lwd = 2, lty = 2)

# add model
lines(x.model.plot, y.model.plot, lwd = 2, col = &amp;#39;blue&amp;#39;)

# legend
legend(&amp;#39;topleft&amp;#39;, legend = c(&amp;#39;train&amp;#39;, &amp;#39;test&amp;#39;, &amp;#39;f(X)&amp;#39;, &amp;#39;model&amp;#39;),
       pch = c(16, 16, NA, NA), 
       col = c(grey(0.2), &amp;#39;red&amp;#39;, &amp;#39;black&amp;#39;, &amp;#39;blue&amp;#39;),  
       lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/statistics/splines/splines_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliograpy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bibliograpy&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://faculty.marshall.usc.edu/gareth-james/&#34;&gt;An Introduction to Statistical Learning by Gareth James&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Name of the paper</title>
      <link>/publication/template_paper/</link>
      <pubDate>Sun, 07 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/publication/template_paper/</guid>
      <description>


&lt;p&gt;Summary of the paper and comments&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
