---
title: "Linear Regression (Math)"
author: "Mark Goldberg"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
draft: false
math: true
tags: ["Math", "Statistics", "Regression"]
categories: ["Statistics"]
#bibliography: [bib.bib]
output:
  blogdown::html_page:
    toc: true
summary: "Pure math for linear regression model."
---
## Simple linear regression
$\hat y = \beta_0 + \beta_1 x$

$\hat y$ - expectd value of $y$ given $x$, the same as $E(Y|x)$  
$\beta_0$ - intercept  
$\beta_1$ - slope   

$\hat \beta_0 = \bar y - \hat\beta_1 \bar x$  

$\hat\beta_1 = \frac{\displaystyle\sum_{i=1}^{n} (x_i - \bar x)(y_i-\bar y)}{\displaystyle\sum_{i=1}^{n} (x_i - \bar x)^2}$


## How to find coefficients β0 and β1?
There are several ways to extimate coefficients of linear regression. Here we discuss the least squares approach. Other aproaches include **maximul likelihood estimation**.  
To estimate $\beta_1$ and $\beta_0$ we should find **minimum** of **sum of squared residuals** ($SSR$)   
To find this minimum we should calculate derivatives of $SSR$ with respect to $\beta_0$ and $\beta_1$ and set them to 0:  

$\displaystyle\min_{\beta_0,\beta1} : SSR \implies \frac{\partial SSR}{\partial \beta_0} = \frac{\partial SSR}{\partial \beta_1} = 0$

Here is the solution:  
$SSR = \displaystyle\sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1x_i))^2 = \displaystyle\sum_{i=1}^{n} (y_i^2 - 2y_i \beta_0 - 2y_i \beta_1 x_i + \beta_0^2 + 2\beta_0\beta_1x_i + \beta_1^2x_i^2)$

$\frac{\partial SSR}{\partial\beta_0} = \displaystyle\sum_{i=1}^{n}(-2y_i + 2\beta_0 + 2\beta_1x_i)$

$\displaystyle\sum_{i=1}^{n} (-y_i + \hat\beta_0 + \hat\beta_1 x_i) = 0$

$\bar y = \frac{1}{n} \displaystyle\sum_{i=1}^{n} y_i$ and $\bar x = \frac{1}{n} \displaystyle\sum_{i=1}^{n} x_i \implies -n\bar y + n \hat\beta_0 + \hat\beta_1 n \hat x = 0$

$\hat \beta_0 = \bar y - \hat\beta_1 \bar x$

$\frac{\partial SSR}{\partial \beta_1} = \displaystyle\sum_{i=1}^{n} (-2x_i y_i + 2\beta_0 x_i + 2\beta_1 x_i^2)$

$-\displaystyle\sum_{i=1}^{n} x_i y_i + \hat\beta_0 \displaystyle\sum_{i=1}^{n} x_i + \hat\beta_1\displaystyle\sum_{i=1}^{n} x_i^2 = 0$

$-\displaystyle\sum_{i=1}^{n} x_i y_i + (\bar y - \hat\beta_1 \bar x) \displaystyle\sum_{i=1}^{n} x_i + \hat\beta_1\displaystyle\sum_{i=1}^{n} x_i^2 = 0$

$\hat\beta_1 = \frac{\displaystyle\sum_{i=1}^{n} x_i(y_i-\bar y)}{\displaystyle\sum_{i=1}^{n} x_i (x_i - \bar x)} = \frac{\displaystyle\sum_{i=1}^{n} (x_i - \bar x)(y_i-\bar y)}{\displaystyle\sum_{i=1}^{n} (x_i - \bar x)^2} = \frac{Cov(x,y)}{Var(x)} = r_{xy} \frac{s_y}{s_x}$

$\hat x$ and $\hat y$ - estimated $x$ and $y$  
$\bar x$ and $\bar y$ - averages of $x_i$ and $y_i$  
$r_{xy}$ - sample correlation coefficient between $x$ and $y$  
$s_x$ and $s_y$ - uncorrected sample standard deviations of $x$ and $y$  
$Var$ and $Cov$ - sample variance and sample covariance.  

## Matrix form for multiple regression
We can write $\hat y = \beta_0 + \beta_1 x$ in a matrix form:

$Y = \begin{bmatrix}y_1\\y_2\\\vdots\\y_n\end{bmatrix}$ $\b = \begin{bmatrix}\beta_1\\\beta_2\\\vdots\\\beta_n\end{bmatrix}$
$X = \begin{bmatrix}1 & x_{1,1} & x_{1,2} \dots & x_{1,k}\\1 & x_{2,1} & x_{2,2} \dots & x_{2,k}\\\vdots & \vdots & \vdots & \vdots\\1 & x_{n,1} & x_{n,2} \dots & x_{n,k}\end{bmatrix}$

$Y = Xb$

$X'Y = X'Xb$

$(X'X)^-1X'Xb = (X'X)^-1X'Y$

$b = (X'X)^-1X'Y$

$(X'X)^-1X'X = I$ - identity matrix
