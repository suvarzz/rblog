---
title: "Resampling methods"
author: "Mark Goldberg"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
draft: true
math: true
tags: ["R", "Statistics", "Resampling", "Cross-validation", "Bootstrap"]
categories: ["Statistics"]
#bibliography: [bib.bib]
output:
  blogdown::html_page:
    toc: true
summary: "From the following examples we will learn wow to estimate model precision by cross-validation, validation sample and leave-one-out cross-validation (LOOCV)."
---

## Validation approach
**Validation** (hold-out) approach estimates the prediction error of our predictive models. This involves randomly dividing the available set of observations into two parts, a **training set** and a **testing set** (validation set). Our statistical model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate (typically assessed using **MSE** in the case of a quantitative response) provides an estimate of the test error rate.  
The drawback of the method is that the **test error rate** can vary depending on training set.  

Let's buld **polinomial models** for first 10 degrees to predict **mpg** from **horsepower** dataset (`ISLR::Auto`), where **mpg** - miles per gallon and **horsepower** - engine horsepower.  
Let's buld models for train data using **polynomial linear regression** using vaious polinomial degrees and estimate MSE for each of these models using test data:  
1. **Linear**: $\hat{mpg} = \hat{\beta}_0 + \hat{\beta}_1 \cdot horsepower$.  
2. **Squared**: $\hat{mpg} = \hat{\beta}_0 + \hat{\beta}_1 \cdot horsepower + \hat{\beta}_2 \cdot horsepower^2$.  
3. **Cubic**: $\hat{mpg} = \hat{\beta}_0 + \hat{\beta}_1 \cdot horsepower + \hat{\beta}_2 \cdot horsepower^2 + \hat{\beta}_3 \cdot horsepower^3$.  

```{r, echo=T, results="hide", message=F}
library('ISLR')        # datasets Auto
attach(Auto)
```

```{r}
set.seed(1)
# split data into train (0.6) and test (0.4)
inTrain <- sample(nrow(Auto), nrow(Auto)*0.6)
train <- Auto[inTrain, ]
test <- Auto[!inTrain, ]

# Buld polynomial models for first ten degrees
models <- lapply(1:10, function(n) {
  fit.lm <- lm(mpg ~ poly(horsepower, n), data=train)
  })

mse.vec <- sapply(1:10, function(n) {
  mean((mpg[-inTrain] - predict(models[[n]], Auto[-inTrain, ]))^2)
})
# create dataframe to store MSE of our models
mse.df <- data.frame(degree = 1:10, mse = NA)
mse.df$mse <- mse.vec
plot(mse.df, type="o", main="MSE of spline regression models for degrees from 1 to 10")

### PLOT with data and first three polynomial models
par(mar = c(4, 4, 0.5, 1))
plot(horsepower[inTrain], mpg[inTrain],
     xlab = 'horsepower', ylab = 'mpg', pch = 21,
     col = rgb(0, 0, 1, alpha = 0.4), bg = rgb(0, 0, 1, alpha = 0.4))
# add test data
points(horsepower[-inTrain], mpg[-inTrain],
       pch = 21, col = rgb(1, 0, 0, alpha = 0.4), bg = rgb(1, 0, 0, alpha = 0.4))

colors=c('black', 'blue', 'red')
x1 <- data.frame(horsepower=seq(min(horsepower), max(horsepower), length = 200))
for (i in 1:3) {
    y2 <- predict(models[[i]], newdata=x1)
    lines(x1$horsepower, y2, col=colors[i], lwd=c(2,2,2))
}
legend('topright', lty=c(1,1,1),
       col = c('black', 'blue', 'red', rgb(0, 0, 1, alpha = 0.4), bg = rgb(1, 0, 0, alpha = 0.4)),
       legend = c('Linear model', 'Squared model', 'Cubic model', 'train data', 'test data'),
       lwd=c(2,2,2,NA,NA), pch=c(NA,NA,NA,16,16))
# end PLOT
```
Lets calculate MSE for these spline regression with dergees from 1 to 10 for 10 different training data subsets:
```{r, message=F}
library(ggplot2)
mse.df.2 <- data.frame(sample = vector("integer", 100), 
                       degree = vector("integer", 100), 
                       mse = vector("double", 100))
counter <- 1

for(i in 1:10) {
  # random sample
  set.seed(i)
  sample <- sample(c(TRUE, FALSE), nrow(Auto), replace = T, prob = c(0.6,0.4))
  train <- Auto[sample, ]
  test <- Auto[!sample, ]
  
  # modeling
  for(j in 1:10) {
    lm.fit <- lm(mpg ~ poly(horsepower, j), data = train)
    
    # add degree & mse values
    mse.df.2[counter, 2] <- j
    mse.df.2[counter, 3] <- mean((test$mpg - predict(lm.fit, test))^2)
    
    # add sample identifier
    mse.df.2[counter, 1] <- i
    counter <- counter + 1
  }
  next
}

ggplot(mse.df.2, aes(degree, mse, color = factor(sample))) +
  geom_line(show.legend = FALSE) +
  geom_point(show.legend = FALSE) +
  ylim(c(10, 30))
```
All ten curves indicate that there is not much benefit in including cubic or higher-order polynomial terms in the model.  
However, MSE depends on train data subsets.  
The train data contains subset of the observations which can lead to the overestimation of validation set error rate in compair with the test error rate for the model fit on the entire data set.  
**Cross-validation methods** are used to overcome this drawback.  

## Leave-one-out cross-validation (LOOCV)
In the LOOCV method a single observation is used for the validation set, and the remaining $n - 1$ observations make up the training set.  
Since the validation observation was not used in the fitting process, the estimate error provides an approximately unbiased estimate for the test error. The validation is repated by subsetting one different observation each time, calculating MSE for this observation each time. The average MSE is calculated in the end. 
Note: we will use `glm` function for linear regression models and the `cv.glm` function from `boot` package, which provide necessary tools to perform resamping.  
```{r message=F}
# LOOCV
library('boot')        # cross-validation
# step 1: fit model
glm.fit <- glm(mpg ~ horsepower, data = Auto)
# setp 2: perform LOOCV across entire data set
loocv.err <- cv.glm(Auto, glm.fit)

str(loocv.err)
```
**call**: the original function call.  
**K**: the number of folds used. The LOOCV repeats validation with each of all of 392 observations.  
**delta**: prediction error (MSE) estimated by the cross-validation.  
**seed**: the values of the random seed used for the function call

Let's compare MSE estimated by **LOOCV** methods with MSE derived by **simple validation** approach (0.6/0.4) what we previously used.  
```{r}
library(dplyr)
# get MSE for our 10 polynomial models, where degree is 1 (linear models). 
mse <- mse.df.2 %>% filter(degree == 1) %>% pull(mse)
mse                 # all MSE from our 10 linear polynoms
mean(mse)           # average MSE from our 10 linear polynoms
loocv.err$delta[1]  # LOOCV MSE
```
As we can see, MSE derived by LOOCV is 24.2 and it is more precise but very close to the average MSE derived by avereging of our 10 polynomial linear models we made before (~24.9).  

Now we can apply LOOCV to estimate MSE for all 10 degrees of our polynomial regression models.  
```{r}
# vector of LOOCV-errors
n <- 10 # max degree of polynom model
cv.err.loocv <- rep(NA, n)
names(cv.err.loocv) <- 1:n
# repeat by powers of polynomes
for (i in 1:n){
  fit.glm <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.err.loocv[i] <- cv.glm(Auto, fit.glm)$delta[1]
}
# result
cv.err.loocv
```
Result is basically the same for this model as we got before but this result is more precise.  
This LOOCV approach can be used with any kind of predictive modeling.  

## k-fold cross validation
library('GGally')      # matrix diagrams
K-times cross-validation is a compromize between sample validation and LOOCV. It is computationally more effective than LOOCV but not so presize.  
We will make 10-time validation.     

```{r}
n <- 10 # max degree (number of polynom models)
cv.err.k.fold <- rep(NA, 10)
names(cv.err.k.fold) <- 1:10
# repeat for power of polynomes
for (i in 1:n){
  fit.glm <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.err.k.fold[i] <- cv.glm(Auto, fit.glm, K = 10)$delta[1]
}
# result
cv.err.k.fold
```
**k-fold cross validation** computationally more effective than **LOOCV**.

## Bootstrapping
**Bootstrapping** repeatedly draws independent samples from our data set to create bootstrap data sets. This sample is performed with replacement, which means that the same observation can be sampled more than once.  

```{r}
n <- nrow(Auto)
statistic <- function(data, index) {
  # extract model coefficients
  coef(lm(mpg ~ horsepower, data = data, subset = index))
}
statistic(Auto, 1:392)
set.seed(1)
boot(Auto, statistic, 1000)
```
Bootstrap estimate for SE0 is 0.86, and that the bootstrap estimate for SE1is 0.0076. If we compare these to the standard errors provided by the summary function we see a difference (biased error).  
```{r}
summary(lm(mpg ~ horsepower, data = Auto))
```
Now we compair errors for polynomial model (degree=2) by bootstrap and non-bootstrap approach.
```{r}
statistic.2 <- function(data, index) {
  coef(lm(mpg ~ poly(horsepower, 2), data = data, subset = index))
}
set.seed(1)
boot(Auto, statistic.2, 1000)
summary(lm(mpg ~ poly(horsepower, 2), data = Auto))
```
Better correspondence between the bootstrap estimates and the standard estimates suggest a better model fit.  

## Bibiography
[An Introduction to Statistical Learning by Gareth James](http://faculty.marshall.usc.edu/gareth-james/)  
[Air Forse Institute of Technology](https://afit-r.github.io)  