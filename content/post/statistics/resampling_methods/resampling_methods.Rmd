---
title: "Resampling methods"
author: "Mark Goldberg"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
draft: true
math: true
tags: ["R", "Statistics", "Resampling", "Cross-validation", "Bootstrap"]
categories: ["Statistics"]
#bibliography: [bib.bib]
output:
  blogdown::html_page:
    toc: true
summary: "From the following examples we will learn wow to estimate model precision by cross-validation, validation sample and leave-one-out cross-validation (LOOCV)."
---

## Validation approach
**Validation** (hold-out) approach estimates the prediction error of our predictive models. This involves randomly dividing the available set of observations into two parts, a **training set** and a **testing set** (validation set). Our statistical model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate (typically assessed using **MSE** in the case of a quantitative response) provides an estimate of the test error rate.  
The drawback of the method is that the **test error rate** can vary depending on training set.  

Let's buld **polinomial models** for first 10 degrees to predict **mpg** from **horsepower** dataset (`ISLR::Auto`), where **mpg** - miles per gallon and **horsepower** - engine horsepower.  
Let's buld models for train data using **polynomial linear regression** using vaious polinomial degrees and estimate MSE for each of these models using test data:  
1. **Linear**: $\hat{mpg} = \hat{\beta}_0 + \hat{\beta}_1 \cdot horsepower$.  
2. **Squared**: $\hat{mpg} = \hat{\beta}_0 + \hat{\beta}_1 \cdot horsepower + \hat{\beta}_2 \cdot horsepower^2$.  
3. **Cubic**: $\hat{mpg} = \hat{\beta}_0 + \hat{\beta}_1 \cdot horsepower + \hat{\beta}_2 \cdot horsepower^2 + \hat{\beta}_3 \cdot horsepower^3$.  

```{r, echo=T, results="hide", message=F}
library('ISLR')        # datasets Auto
attach(Auto)
```

```{r}
set.seed(1)
# split data into train (0.6) and test (0.4)
inTrain <- sample(nrow(Auto), nrow(Auto)*0.6)
train <- Auto[inTrain, ]
test <- Auto[!inTrain, ]

# Buld polynomial models for first ten degrees
models <- lapply(1:10, function(n) {
  fit.lm <- lm(mpg ~ poly(horsepower, n), data=train)
  })

mse.vec <- sapply(1:10, function(n) {
  mean((mpg[-inTrain] - predict(models[[n]], Auto[-inTrain, ]))^2)
})
# create dataframe to store MSE of our models
mse.df <- data.frame(degree = 1:10, mse = NA)
mse.df$mse <- mse.vec
plot(mse.df, type="o", main="MSE of spline regression models for degrees from 1 to 10")

### PLOT with data and first three polynomial models
par(mar = c(4, 4, 0.5, 1))
plot(horsepower[inTrain], mpg[inTrain],
     xlab = 'horsepower', ylab = 'mpg', pch = 21,
     col = rgb(0, 0, 1, alpha = 0.4), bg = rgb(0, 0, 1, alpha = 0.4))
# add test data
points(horsepower[-inTrain], mpg[-inTrain],
       pch = 21, col = rgb(1, 0, 0, alpha = 0.4), bg = rgb(1, 0, 0, alpha = 0.4))

colors=c('black', 'blue', 'red')
x1 <- data.frame(horsepower=seq(min(horsepower), max(horsepower), length = 200))
for (i in 1:3) {
    y2 <- predict(models[[i]], newdata=x1)
    lines(x1$horsepower, y2, col=colors[i], lwd=c(2,2,2))
}
legend('topright', lty=c(1,1,1),
       col = c('black', 'blue', 'red', rgb(0, 0, 1, alpha = 0.4), bg = rgb(1, 0, 0, alpha = 0.4)),
       legend = c('Linear model', 'Squared model', 'Cubic model', 'train data', 'test data'),
       lwd=c(2,2,2,NA,NA), pch=c(NA,NA,NA,16,16))
# end PLOT
```

### Leave-one-out cross-validation (LOOCV)
```{r, echo=T, results="hide", message=F}
library('GGally')      # matrix diagrams
library('boot')        # cross-validation
```
```{r}
# fit model for train data
fit.glm <- glm(mpg ~ horsepower, data = Auto)
# LOOCV-error
cv.err <- cv.glm(Auto, fit.glm)
cv.err$delta[1]
```  

Estimate the precision of polynomial models by changing power.

```{r}
# vector of LOOCV-errors
cv.err.loocv <- rep(0, 5)
names(cv.err.loocv) <- 1:5
# repeat by powers of polynomes
for (i in 1:5){
  fit.glm <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.err.loocv[i] <- cv.glm(Auto, fit.glm)$delta[1]
}
# result
cv.err.loocv
```

## k-fold cross validation

K-times cross-validation is a compromize between sample validation and LOOCV. It is computationally more effective than LOOCV but not so presize.  
We will make 10-time validation.     

```{r}
cv.err.k.fold <- rep(0, 5)
names(cv.err.k.fold) <- 1:5
# repeat for power of polynomes
for (i in 1:5){
  fit.glm <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.err.k.fold[i] <- cv.glm(Auto, fit.glm,
                             K = 10)$delta[1]
}
# result
cv.err.k.fold
```

Compare with previous result:   

```{r}
mse.df
```

Results show that error of liniear model is higher that was shown by MSE and errors are lower for polynomial regression models n=2,3.  

## Bootstrapping   
One of the great advantages of the bootstrap approach is that it can be applied in almost all situations. No complicated mathematical calculations are required. Performing a bootstrap analysis in R entails only two steps. First, we must create a function that computes the statistic of interest. Second, we use the `boot()` function, which is part of the boot library, to perform the bootstrap by repeatedly sampling observations from the data set with replacement.  
Lets apply bootstrap for calculation of residual errors.   

```{r, eval=F}
boot.fn <- function(data, index){
  coef(lm(mpg ~ horsepower, data = data, subset = index))
}
boot.fn(Auto, 1:n)

set.seed(1)
boot.fn(Auto, sample(n, n, replace = T))
# aply boot to calculate standard errors for 1000 samplings with repeats)
boot(Auto, boot.fn, 1000)
# compere with 'МНК'
#summary(lm(mpg ~ horsepower))$coef
# оценки отличаются из-за того, что МНК -- параметрический метод с допущениями
# вычислим оценки параметров квадратичной модели регрессии
boot.fn.2 <- function(data, index){
  coef(lm(mpg ~ poly(horsepower, 2), data = data, subset = index))
}
# apply bootstap with 1000 samplings
set.seed(1)
boot(Auto, boot.fn, 1000)
```

Regression models errors calculated by MHK are similar byb errors calculated using bootstap

### Bibiography
[An Introduction to Statistical Learning by Gareth James](http://faculty.marshall.usc.edu/gareth-james/)  
[Air Forse Institute of Technology](https://afit-r.github.io)  