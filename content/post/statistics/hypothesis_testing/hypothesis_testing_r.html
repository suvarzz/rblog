---
title: "Hypothesis testing"
author: "Mark Boltengagen"
date: '2019-08-01'
draft: true
math: true
tags: ["R", "Statistics", "Hypothesis testing"]
categories: ["R", "Statistics"]
---



<p>Null hypothesis (H<sub>0</sub>):<br />
1. H<sub>0</sub>: m = μ<br />
2. H<sub>0</sub>: m <span class="math inline">\(\leq\)</span> μ<br />
3. H<sub>0</sub>: m <span class="math inline">\(\geq\)</span> μ</p>
<p>Alternative hypotheses (H<sub>a</sub>): 1. H<sub>a</sub>:m ≠ μ (different)<br />
2. H<sub>a</sub>:m &gt; μ (greater)<br />
3. H<sub>a</sub>:m &lt; μ (less)</p>
<p>Note: Hypothesis 1. are called <strong>two-tailed tests</strong> and hypotheses 2. &amp; 3. are called <strong>one-tailed tests</strong>.</p>
<p>The p-value is the probability that the observed data could happen, under the condition that the null hypothesis is true.</p>
<p>Note: p-value is not the probability that the null hypothesis is true.<br />
Note: Absence of evidence ⧧ evidence of absence.</p>
<p>Cutoffs for hypothesis testing *p &lt; 0.05, **p &lt; 0.01, ***p &lt; 0.001. If p value is less than significance level alpha (0.05), the hull hypothesies is rejected.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>not rejected (‘negative’)</th>
<th>rejected (‘positive’)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>H<sub>0</sub> true</td>
<td>True negative (specificity)</td>
<td>False Positive (Type I error)</td>
</tr>
<tr class="even">
<td>H<sub>0</sub> false</td>
<td>False Negative (Type II error)</td>
<td>True positive (sensitivity)</td>
</tr>
</tbody>
</table>
<p>Type II errors are usually more dangerous.</p>
<div id="t-test" class="section level1">
<h1>t-test</h1>
<div id="t-test-and-normal-distribution" class="section level2">
<h2>t-test and normal distribution</h2>
<p>t-distribution assumes that the observations are <strong>independent</strong> and that they follow a <strong>normal distribution</strong>. If the data are <strong>dependent</strong>, then p-values will likely be totally wrong (e.g., for positive correlation, too optimistic). Type II errors?<br />
It is good to test if observations are normally distributed. Otherwise we assume that data is normally distributed.<br />
Independence of observations is usually not testable, but can be reasonably assumed if the data collection process was random without replacement.</p>
<p>FIXME: I do not understand this. Deviation data from normalyty will lead to type-I errors. I data is deviated from normal distribution, use <strong>Wilcoxon test</strong> or <strong>permutation tests</strong>.</p>
</div>
<div id="one-sample-t-test" class="section level2">
<h2>One-sample t-test</h2>
<p>One-sample t-test is used to compare the mean of one sample to a known standard (or theoretical/hypothetical) mean (μ).<br />
t-statistics: <span class="math inline">\(t = \frac{m - \mu}{s/\sqrt{n}}\)</span>, where<br />
<strong>m</strong> is the sample <strong>mean</strong><br />
<strong>n</strong> is the sample <strong>size</strong><br />
<strong>s</strong> is the sample <strong>standard deviation</strong> with n−1 degrees of freedom<br />
<strong>μ</strong> is the <strong>theoretical value</strong><br />
Q: And what should I do with this t-statistics?<br />
Q: What is the difference between t-test and ANOVA?<br />
Q: What is the smallest sample size which can be tested by t-test?<br />
Q: Show diagrams explaining why p-value of one-sided is smaller than two-sided tests.</p>
<p>R example:<br />
We want to test if N is different from given mean μ=0:</p>
<pre class="r"><code>N = c(-0.01, 0.65, -0.17, 1.77, 0.76, -0.16, 0.88, 1.09, 0.96, 0.25)
t.test(N, mu = 0, alternative = &quot;less&quot;)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  N
## t = 3.0483, df = 9, p-value = 0.9931
## alternative hypothesis: true mean is less than 0
## 95 percent confidence interval:
##      -Inf 0.964019
## sample estimates:
## mean of x 
##     0.602</code></pre>
<pre class="r"><code>t.test(N, mu = 0, alternative = &quot;two.sided&quot;)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  N
## t = 3.0483, df = 9, p-value = 0.01383
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  0.1552496 1.0487504
## sample estimates:
## mean of x 
##     0.602</code></pre>
<pre class="r"><code>t.test(N, mu = 0, alternative = &quot;greater&quot;)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  N
## t = 3.0483, df = 9, p-value = 0.006916
## alternative hypothesis: true mean is greater than 0
## 95 percent confidence interval:
##  0.239981      Inf
## sample estimates:
## mean of x 
##     0.602</code></pre>
<p>FIXME: why it accepts all alternatives at the same time (less and greater?)</p>
</div>
<div id="two-samples-t-test" class="section level2">
<h2>Two samples t-test</h2>
<p>Do two different samples have the same mean?<br />
H<sub>0</sub>:<br />
1. H<sub>0</sub>: m<sub>1</sub> - m<sub>2</sub> = 0<br />
2. H<sub>0</sub>: m<sub>1</sub> - m<sub>2</sub> <span class="math inline">\(\leq\)</span> 0<br />
3. H<sub>0</sub>: m<sub>1</sub> - m<sub>2</sub> <span class="math inline">\(\geq\)</span> 0</p>
<p>H<sub>a</sub>:<br />
1. H<sub>a</sub>: m<sub>1</sub> - m<sub>2</sub> ≠ 0 (different)<br />
2. H<sub>a</sub>: m<sub>1</sub> - m<sub>2</sub> &gt; 0 (greater)<br />
3. H<sub>a</sub>: m<sub>1</sub> - m<sub>2</sub> &lt; 0 (less)</p>
<p>The paired sample t-test has four main assumptions:</p>
<ol style="list-style-type: decimal">
<li>The dependent variable must be <strong>continuous</strong> (interval/ratio).<br />
</li>
<li>The observations are <strong>independent</strong> of one another.<br />
</li>
<li>The dependent variable should be approximately <strong>normally distributed</strong>.<br />
</li>
<li>The dependent variable should not contain any <strong>outliers</strong>.</li>
</ol>
<p>Continuous data can take on any value within a range (income, height, weight, etc.). The opposite of continuous data is discrete data, which can only take on a few values (Low, Medium, High, etc.). Occasionally, discrete data can be used to approximate a continuous scale, such as with <strong>Likert-type scales</strong>.</p>
<p>t-statistics: <span class="math inline">\(t=\frac{y - x}{SE}\)</span>, where y and x are the samples means. SE is the standard error for the difference. If H<sub>0</sub> is correct, test statistic follows a t-distribution with n+m-2 degrees of freedom (n, m the number of observations in each sample).</p>
<p>To apply t-test samples must be tested if they have equal variance:<br />
equal variance (homoscedastic). Type 3 means two samples, unequal variance (heteroscedastic).</p>
<pre class="r"><code>### t-test
a = c(175, 168, 168, 190, 156, 181, 182, 175, 174, 179)
b = c(185, 169, 173, 173, 188, 186, 175, 174, 179, 180)

# test homogeneity of variances using Fisher’s F-test
var.test(a,b)</code></pre>
<pre><code>## 
##  F test to compare two variances
## 
## data:  a and b
## F = 2.1028, num df = 9, denom df = 9, p-value = 0.2834
## alternative hypothesis: true ratio of variances is not equal to 1
## 95 percent confidence interval:
##  0.5223017 8.4657950
## sample estimates:
## ratio of variances 
##           2.102784</code></pre>
<pre class="r"><code># variance is homogene (can use var.equal=T in t.test)

# t-test
t.test(a,b, 
       var.equal=TRUE,   # variance is homogene (tested by var.test(a,b)) 
       paired=FALSE)     # samples are independent</code></pre>
<pre><code>## 
##  Two Sample t-test
## 
## data:  a and b
## t = -0.94737, df = 18, p-value = 0.356
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -10.93994   4.13994
## sample estimates:
## mean of x mean of y 
##     174.8     178.2</code></pre>
</div>
</div>
<div id="summary-of-r-functions-for-t-tests" class="section level1">
<h1>Summary of R functions for t-tests</h1>
<p>One-sample t-test</p>
<pre class="r"><code>t.test(x, mu = 0, alternative = c(&quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot;), paired = FALSE, var.equal = FALSE, conf.level = 0.95)</code></pre>
<div id="what-is-that" class="section level2">
<h2>What is that?</h2>
<p>one-way ANOVA or 2-way ANOVA with <strong>Bonferroni multiple comparison</strong> or <strong>Dunnett’s post-test</strong></p>
</div>
</div>
<div id="non-parametric-tests" class="section level1">
<h1>Non-parametric tests</h1>
<div id="mann-whitney-u-rank-sum-test" class="section level2">
<h2>Mann-Whitney U Rank Sum Test</h2>
<ol style="list-style-type: decimal">
<li>The dependent variable is ordinal or continuous.<br />
</li>
<li>The data consist of a randomly selected sample of independent observations from two independent groups.<br />
</li>
<li>The dependent variables for the two independent groups share a similar shape.</li>
</ol>
</div>
<div id="wilcoxon-test" class="section level2">
<h2>Wilcoxon test</h2>
<p>The <strong>Wilcoxon</strong> is a <strong>non-parametric test</strong> which works on normal and non-normal data. However, we usually prefer not to use it if we can assume that the data is normally distributed. The non-parametric test comes with less statistical power, this is a price that one has to pay for more flexible assumptions.</p>
</div>
</div>
<div id="tests-for-categorical-variables" class="section level1">
<h1>Tests for categorical variables</h1>
<p><strong>Categorical variable</strong> can take fixed number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property.</p>
<div id="chi-squared-tests" class="section level2">
<h2>Chi-squared tests</h2>
<p>The chi-squared test is most suited to large datasets. As a general rule, the chi-squared test is appropriate if at least 80% of the cells have an expected frequency of 5 or greater. In addition, none of the cells should have an expected frequency less than 1. If the expected values are very small, categories may be combined (if it makes sense to do so) to create fewer larger categories. Alternatively, Fisher’s exact test can be used.</p>
<pre class="r"><code>data = rbind(c(83,35), c(92,43))
data</code></pre>
<pre><code>##      [,1] [,2]
## [1,]   83   35
## [2,]   92   43</code></pre>
<pre class="r"><code>chisq.test(data, correct=F)</code></pre>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  data
## X-squared = 0.14172, df = 1, p-value = 0.7066</code></pre>
<p>chisq.test(testor,correct=F) ## Fisher’s Exact test R Example:</p>
<table>
<thead>
<tr class="header">
<th>Group</th>
<th>TumourShrinkage-No</th>
<th>TumourShrinkage-Yes</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1 Treatment</td>
<td>8</td>
<td>3</td>
<td>11</td>
</tr>
<tr class="even">
<td>2 Placebo</td>
<td>9</td>
<td>4</td>
<td>13</td>
</tr>
<tr class="odd">
<td>3 Total</td>
<td>17</td>
<td>7</td>
<td>24</td>
</tr>
</tbody>
</table>
<p>The <strong>null hypothesis</strong> is that there is <strong>no association</strong> between treatment and tumour shrinkage.<br />
The <strong>alternative hypothesis</strong> is that there is <strong>some association</strong> between treatment group and tumour shrinkage.</p>
<pre class="r"><code>data = rbind(c(8,3), c(9,4))
data</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    8    3
## [2,]    9    4</code></pre>
<pre class="r"><code>fisher.test(data)</code></pre>
<pre><code>## 
##  Fisher&#39;s Exact Test for Count Data
## 
## data:  data
## p-value = 1
## alternative hypothesis: true odds ratio is not equal to 1
## 95 percent confidence interval:
##   0.1456912 10.6433317
## sample estimates:
## odds ratio 
##   1.176844</code></pre>
<p>The output Fisher’s exact test tells us that the probability of observing such an extreme combination of frequencies is high, our p-value is 1.000 which is clearly greater than 0.05. In this case, there is <strong>no evidence of an association</strong> between treatment group and tumour shrinkage.</p>
</div>
</div>
<div id="multiple-testing" class="section level1">
<h1>Multiple testing</h1>
<p>When performing a large number of tests, the type I error is inflated: for α=0.05 and performing n tests, the probability of no false positive result is: 0.095 x 0.95 x … (n-times) &lt;&lt;&lt; 0.095<br />
The larger the number of tests performed, the higher the probability of a false rejection!<br />
Many data analysis approaches in genomics rely on itemby-item (i.e. multiple) testing:<br />
Microarray or RNA-Seq expression profiles of “normal” vs “perturbed” samples: gene-by-gene<br />
ChIP-chip: locus-by-locus<br />
RNAi and chemical compound screens<br />
Genome-wide association studies: marker-by-marker<br />
QTL analysis: marker-by-marker and trait-by-trait</p>
<p><strong>False positive rate</strong> (FPR) - the proportion of false positives among all resulst.</p>
<p><strong>False discovery rate</strong> (FDR) - the proportion of false positives among all significant results.</p>
<p>Example: 20,000 genes, 100 hits, 10 of them wrong.<br />
FPR: 0.05%<br />
FDR: 10%</p>
<div id="the-bonferroni-correction" class="section level2">
<h2>The Bonferroni correction</h2>
<p>The Bonferroni correction sets the significance cut-off at α/n.</p>
</div>
</div>
<div id="sources" class="section level1">
<h1>Sources</h1>
<p><a href="http://www.sthda.com/english/wiki/one-sample-t-test-in-r">One-Sample T-test in R</a></p>
</div>
