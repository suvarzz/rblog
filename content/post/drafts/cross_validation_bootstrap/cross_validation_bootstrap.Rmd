---
title: "Cross-validation and bootstrap"
author: "Mark Goldberg"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
draft: true
math: true
tags: ["R", "Statistics", "Cross-validation", "Bootstrap"]
categories: ["Statistics"]
#bibliography: [bib.bib]
output:
  blogdown::html_page:
    toc: true
summary: "From the following examples we will learn wow to estimate model precision by cross-validation, validation sample and leave-one-out cross-validation (LOOCV)."
---

From the following examples we will learn wow to estimate model precision by cross-validation, validation sample and leave-one-out cross-validation (LOOCV).

*Model*: linear regression, kNN
*Dataset*: `Auto {ISLR}`

We want to build models and compare this models:  
mpg ~ f(horsepower) `Auto` dataset, where  
**mpg** - miles per gallon  
**horsepower** - engine horsepower  

```{r, echo=T, results="hide", message=F}
library('ISLR')        # datasets Auto
attach(Auto)
```
```{r}
plot(horsepower, mpg,
     xlab = 'horsepower', ylab = 'mpg', pch = 21,
     col = rgb(0, 0, 1, alpha = 0.4),
     bg = rgb(0, 0, 1, alpha = 0.4))
```

### Validation sample 
Split data to train and test sets and build model using train data.  

Make random vector for data subsetting:  
```{r}
n <- nrow(Auto)         # number of observation
train.percent <- 0.5    # portion of train data

# split data into train and test
set.seed(1)
inTrain <- sample(n, n*train.percent)
```

Plot data
```{r}
# plot train data
par(mar = c(4, 4, 0.5, 1))
plot(horsepower[inTrain], mpg[inTrain],
     xlab = 'horsepower', ylab = 'mpg', pch = 21,
     col = rgb(0, 0, 1, alpha = 0.4), bg = rgb(0, 0, 1, alpha = 0.4))
# add test data
points(horsepower[-inTrain], mpg[-inTrain],
       pch = 21, col = rgb(1, 0, 0, alpha = 0.4), bg = rgb(1, 0, 0, alpha = 0.4))
legend('topright', pch = c(16, 16), col = c('blue', 'red'), legend = c('test', 'train'))
```

Let's buld models for train data using **polynomial linear regression** using vaious polinomial degrees and estimate MSE for each of these models using test data:  
1. **Linear**: $\hat{mpg} = \hat{\beta}_0 + \hat{\beta}_1 \cdot horsepower$.  
2. **Squared**: $\hat{mpg} = \hat{\beta}_0 + \hat{\beta}_1 \cdot horsepower + \hat{\beta}_2 \cdot horsepower^2$.  
3. **Cubic**: $\hat{mpg} = \hat{\beta}_0 + \hat{\beta}_1 \cdot horsepower + \hat{\beta}_2 \cdot horsepower^2 + \hat{\beta}_3 \cdot horsepower^3$.  

``` {r}
models <- lapply(1:3, function(n) {
  fit.lm <- lm(mpg ~ poly(horsepower, n), data=Auto, subset=inTrain)
  })

# keep MSE for each model
err.test <- sapply(1:3, function(n) {
  mean((mpg[-inTrain] - predict(models[[n]], Auto[-inTrain, ]))^2)
})
names(err.test) <- c('lm1', 'lm2', 'lm3')
```

Q: Do our modeles fit the formulas shown before?

Plot models
```{r}

par(mar = c(4, 4, 0.5, 1))
plot(horsepower[inTrain], mpg[inTrain],
     xlab = 'horsepower', ylab = 'mpg', pch = 21,
     col = rgb(0, 0, 1, alpha = 0.4), bg = rgb(0, 0, 1, alpha = 0.4))
# add test data
points(horsepower[-inTrain], mpg[-inTrain],
       pch = 21, col = rgb(1, 0, 0, alpha = 0.4), bg = rgb(1, 0, 0, alpha = 0.4))

colors=c('black', 'blue', 'red')
x1 <- data.frame(horsepower=seq(min(horsepower), max(horsepower), length = 200))
for (i in 1:3) {
    y2 <- predict(models[[i]], newdata=x1)
    lines(x1$horsepower, y2, col=colors[i], lwd=c(2,2,2))
}
legend('topright', lty=c(1,1,1),
       col = c('black', 'blue', 'red'),
       legend = c('x', 'x^2', 'x^3'), lwd=c(2,2,2))
```

### Leave-one-out cross-validation (LOOCV)
```{r, echo=T, results="hide", message=F}
library('GGally')      # matrix diagrams
library('boot')        # cross-validation
```
```{r}
# fit model for train data
fit.glm <- glm(mpg ~ horsepower, data = Auto)
# LOOCV-error
cv.err <- cv.glm(Auto, fit.glm)
cv.err$delta[1]
```  

Estimate the precision of polynomial models by changing power.

```{r}
# vector of LOOCV-errors
cv.err.loocv <- rep(0, 5)
names(cv.err.loocv) <- 1:5
# repeat by powers of polynomes
for (i in 1:5){
  fit.glm <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.err.loocv[i] <- cv.glm(Auto, fit.glm)$delta[1]
}
# result
cv.err.loocv
```


### k-cross-validation

K-times cross-validation is a compromize between sample validation and LOOCV. It is computationally more effective than LOOCV but not so presize.  
We will make 10-time validation.     

```{r}
cv.err.k.fold <- rep(0, 5)
names(cv.err.k.fold) <- 1:5
# repeat for power of polynomes
for (i in 1:5){
  fit.glm <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.err.k.fold[i] <- cv.glm(Auto, fit.glm,
                             K = 10)$delta[1]
}
# result
cv.err.k.fold
```

Compare with previous result:   

```{r}
err.test
```

Опираясь на результаты расчётов с кросс-валидацией, можно заключить, что на самом деле ошибка вне выборки у линейной модели выше, чем показывала MSE на тестовой выборке. А модели со степенями 2 и 3 на самом деле точнее, чем показывала MSE без перекрёстной проверки.   

## Bootstrap   

### Presition estimation of model parameter  

Пример с инвестиционным портфелем из двух активов: `Portfolio {ISLR}`. В наборе данных две переменных: 
* `X` -- income from X,   
* `Y` -- income from Y.   
We have $X$ and $Y$, the portion of $X$ is $\alpha$. Min of income dispersion: 

$$
\mathrm{Var}(\alpha X + (1 - \alpha) Y) \rightarrow \mathrm{min}
$$

-- parameter:  
$$
\alpha = \frac{\sigma_Y^2 - \sigma_{XY}}{\sigma_X^2 + \sigma_Y^2 - 2\sigma_{XY}}
$$
Данных для оценки $\hat{\sigma_X^2}$, $\hat{\sigma_Y^2}$ и $\hat{\sigma_{XY}}$ немного (100 наблюдений), поэтому применим бутстреп.   

```{r}
head(Portfolio)
str(Portfolio)
# функция для вычисления искомого параметра
alpha.fn <- function(data, index){
  X = data$X[index]
  Y = data$Y[index]
  (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2*cov(X, Y))
}
# рассчитать alpha по всем 100 наблюдениям
alpha.fn(Portfolio, 1:100)
# создать бутстреп-выборку и повторно вычислить alpha
set.seed(1)
alpha.fn(Portfolio, sample(100, 100, replace = T))
# теперь -- многократное повторение предыдущей операции
boot(Portfolio, alpha.fn, R = 1000)
```

Бутстреп повторяет расчёт параметра много раз, делая повторные выборки из наших 100 наблюдений. В итоге этим методом можно вычислить стандартную ошибку параметра, не опираясь на допущения о законе распределении параметра. В нашем случае $\alpha = 0.576$ со стандартной ошибкой $s_{\hat{\alpha}} = 0.089$.   

### Точность оценки параметра регрессии   

При построении модели регрессии проблемы в остатках приводят к неверной оценке ошибок параметров. Обойти эту проблему можно, применив для расчёта этих ошибок бутстреп.   

```{r}
# Оценивание точности линейной регрессионной модели ----------------------------
# оценить стандартные ошибки параметров модели 
#  mpg = beta_0 + beta_1 * horsepower с помощью бутстрепа,
#  сравнить с оценками ошибок по МНК
# функция для расчёта коэффициентов ПЛР по выборке из данных
boot.fn <- function(data, index){
  coef(lm(mpg ~ horsepower, data = data, subset = index))
}
boot.fn(Auto, 1:n)
# пример применения функции к бутстреп-выборке
set.seed(1)
boot.fn(Auto, sample(n, n, replace = T))
# применяем функцию boot для вычисления стандартных ошибок параметров
#  (1000 выборок с повторами)
boot(Auto, boot.fn, 1000)
# сравним с МНК
attach(Auto)
summary(lm(mpg ~ horsepower))$coef
detach(Auto)
# оценки отличаются из-за того, что МНК -- параметрический метод с допущениями
# вычислим оценки параметров квадратичной модели регрессии
boot.fn.2 <- function(data, index){
  coef(lm(mpg ~ poly(horsepower, 2), data = data, subset = index))
}
# применим функцию к 1000 бутсреп-выборкам
set.seed(1)
boot(Auto, boot.fn, 1000)
```

В модели регрессии, для которой проводился расчёт, похоже, не нарушаются требования к остаткам, и оценки стандартных ошибок параметров, рассчитанные по МНК, очень близки к ошибкам этих же параметров, полученных бутстрепом.   


*Literature*   

1. *James G., Witten D., Hastie T. and Tibshirani R.*  An Introduction to Statistical Learning with Applications in R. URL: [http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf)  